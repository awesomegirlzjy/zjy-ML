# 12 支持向量机

> 西瓜书里6.1节讲到的“什么是支持向量机”，更好理解些。

到目前为止，你已经见过一系列不同的学习算法。在监督学习中，许多学习算法的性能都非常类似，因此，重要的不是你该选择使用学习算法**A**还是学习算法**B**，而更重要的是，应用这些算法时所使用的数据量以及其他技巧，如：特征的选择、正则化参数的选择等等。

还有一个广泛应用于工业界和学术界的更加强大的<font color="red">监督学习算法</font>——**支持向量机**(**Support Vector Machine, SVM**)。与逻辑回归和神经网络相比，支持向量机在学习复杂的非线性方程时提供了一种更为清晰、强大的方式。

依然是先从优化目标开始。

## 12.1 优化目标

> 支持向量机对模型进行优化的目标是什么？相当于是在问“代价函数”是什么。优化目标就是使代价函数尽可能小。

在逻辑回归中，我们已经熟悉了下面这个假设函数$h_{\theta}(x)$，以及右边的Sigmoid激活函数。

![image-20211002171228060](E:\md笔记\PyTorch\images\image-20211002171228060.png)

令$z=\theta^Tx$。则：

![image-20211002170814973](E:\md笔记\PyTorch\images\image-20211002170814973.png)

所以，**支持向量机的假设函数**表示为：

![image-20211003101300826](E:\md笔记\PyTorch\images\image-20211003101300826.png)

-- --

由2.4节，我们知道逻辑回归的代价函数 $Cost\left( {h_\theta}\left( x \right),y \right)$： 

$Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$ .

- $y=1$ 时，$Cost\left( {h_\theta}\left( x \right),y \right)=- log\left( {h_\theta}\left( x \right) \right)=-log\frac{1}{1+e^{-z}}$,  该代价函数关于 $z$ 的图像如下所示：

  <img src="E:\md笔记\PyTorch\images\image-20211002173212328.png" alt="image-20211002173212328" style="zoom: 67%;" />

  为了构建支持向量机，我们在此基础上画出新的代价函数——下图粉色折线（两段直线组成的）所示，它和逻辑回归的效果很相似，但支持向量机在计算上会更有优势，并且使得之后的优化问题变得简单。

<img src="E:\md笔记\PyTorch\images\image-20211002174155139.png" alt="image-20211002174155139" style="zoom:67%;" />

- $y=0$ 时，$Cost\left( {h_\theta}\left( x \right),y \right)=- log\left(1- {h_\theta}\left( x \right) \right)=-log(1- \frac{1}{1+e^{-z}})$,  该代价函数关于 $z$ 的图像如下所示：

  <img src="E:\md笔记\PyTorch\images\image-20211002173945189.png" alt="image-20211002173945189" style="zoom:67%;" />

  类似的，为了构建支持向量机，我们在此基础上画出新的代价函数——下图粉色折线（两段直线组成的）所示：

  <img src="E:\md笔记\PyTorch\images\image-20211002174211175.png" alt="image-20211002174211175" style="zoom:67%;" />

逻辑回归中使用的总的代价函数 $J({\theta})$（加入正则化项的）：

$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}+\frac{\lambda }{2m}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$

对于支持向量机的 $J({\theta})$：

$J\left( \theta \right)=\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}{cost}_1({\theta}^Tx^{(i)})-\left( 1-{{y}^{(i)}} \right){cost}_0({\theta}^Tx^{(i)})]}+\frac{\lambda }{2}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$

> 去了 $\frac{1}{m}$ 这一项，它是个常数，所以不会影响到求 $J({\theta})$ 的最下值。

令$A=\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}{cost}_1({\theta}^Tx^{(i)})-\left( 1-{{y}^{(i)}} \right){cost}_0({\theta}^Tx^{(i)})]}$, $B=\frac{1}{2}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$ .

则支持向量机的 $J({\theta})=A+{\lambda}B$. 通过设定不同的正则化参数 ${\lambda}$，来使得 $A$ 项尽可能小。但是，事实上，对于支持向量机，我们不使用这个式子，而是使用 $J({\theta})=CA+B$ （可以令$C=\frac{1}{\lambda}$， 但不是只能这样）, 这个式子的效果和前面那个式子是一致的。对于前一个式子：如果把 ${\lambda}$ 设为很大的值，这意味着 $B$ 有很高的权重；对于后一个式子：如果把 $C$ 设为很小的值，这意味着 $B$ 将有很高的权重。总而言之，这只是一种不同的控制权衡（权衡 $A$ 和 $B$）的方式，或者说是一种不同的参数设置方法，以此来决定我们是更关心第一项的优化，还是更关心第二项的优化。所以，**支持向量机的 $J({\theta})$ 表示**为：

$J\left( \theta \right)=C\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}{cost}_1({\theta}^Tx^{(i)})-\left( 1-{{y}^{(i)}} \right){cost}_0({\theta}^Tx^{(i)})]}+\frac{1}{2}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$

## 12.2 大间隔分类器

> 支持向量机就是“大间隔分类器”。

由上面讲到的这两幅图可知：

![image-20211003104643909](E:\md笔记\PyTorch\images\image-20211003104643909.png)

如果真实值 $y=1$，那么我们就希望 ${\theta}^Tx ≥ 1$；

如果真实值 $y=0$，那么我们就希望 ${\theta}^Tx ≤ -1$。

假设在 $y= 1$ 的情况下，我们找到了参数 $C$ 的一个很大的取值，使得 $A$ 几乎为0，那么此时 $J({\theta})=B=\frac{1}{2}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$。

从而，我们的优化目标变成了最小化 $\frac{1}{2}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$ . 约束条件为：

- 若 $y^{(i)}=1$, 则 ${\theta}^Tx^{(i)} ≥ 1$；
- 若 $y^{(i)}=0$, 则 ${\theta}^Tx^{(i)} ≤ -1$。

-- --

下面是一个具体的例子。

> 之后会用到的向量计算相关的知识：
>
> ![image-20211003114054288](E:\md笔记\PyTorch\images\image-20211003114054288.png)

我们先忽略截距，令 ${\theta}_0=0$，并假设特征数 $n=2$，此时**优化目标**为：

$min\frac{1}{2}({\theta}_1^2+{\theta}_2^2) = min\frac{1}{2}(\sqrt{{\theta_1^2+{\theta}_2^2}})^2 = min\frac{1}{2}(||{\theta}||)^2$, 其中向量 ${\theta}=({\theta_1, {\theta}_2})^T$.

现在我们考虑 ${\theta}^Tx^{(i)}$ 的计算结果有什么实际意义？

将向量 ${\theta}$ 和向量 ${x}$ 表示在坐标系上：

![image-20211003123241579](E:\md笔记\PyTorch\images\image-20211003123241579.png)

将训练样本投影到参数向量 ${{\theta }}$，然后看一下投影线段的长度，将它称为 $p^{(i)}$（第 $i$个训练样本在参数向量 ${{\theta }}$ 上的投影）。根据向量计算的内容，我们知道 $θ^Tx^{(i)}$ 就等于 $p$ 乘以向量 $θ$ 的**长度/范数**。即 ${\theta}^Tx^{(i)} = p^{(i)}||{\theta}|| = \theta_1\cdot{x_1^{(i)}}+\theta_2\cdot{x_2^{(i)}}$。从而，约束条件可以替换为如下这种表达：

- 若 $y^{(i)}=1$, 则 $p^{(i)}||{\theta}|| ≥ 1$；
- 若 $y^{(i)}=0$, 则 $p^{(i)}||{\theta}|| ≤ -1$。

-- --

下面是一个二分类实例。

![image-20211003123213236](E:\md笔记\PyTorch\images\image-20211003123213236.png)

如果选择上图这样的决策边界（绿色线）：

当数据集如下时，如果分类边界（绿色线）为下图左侧所示：这个决策边界并不是一个好的选择，因为它距离样本的间距很小。支持向量机是如何抛弃这种选择的？首先，参数向量与决策边界正交（==参数向量是决策边界的法向量==.），所以参数向量如图蓝色线所示。把其中几个样本投影到参数向量 ${\theta}$，得到 $p^{(1)}$, $p^{(2)}$，我们会发现，这些投影长度都很小。与此同时，对于  $y^{(i)}=1$，我们希望 $p^{(i)}||{\theta}|| ≥ 1$，所以如果 $p^{(i)}$ 很小的话，就意味着需要 $||{\theta}||$ 很大；对于  $y^{(i)}=0$，我们希望 $p^{(i)}||{\theta}|| ≤ -1$，所以如果 $p^{(i)}$（投影长度哈，是正的，只是最后结果 $p^{(i)}||{\theta}||$ 要加负号） 很小的话，同样意味着需要 $||{\theta}||$ 很大。而前面我们说了，优化目标是 $min\frac{1}{2}(||{\theta}||)^2$，即应尽量使 $||{\theta}||$ 小。综上，这一情况达不到我们想要的目标。

![image-20211003123520039](E:\md笔记\PyTorch\images\image-20211003123520039.png)

而如果选择了上图这样的决策边界：样本投影到参数向量 ${\theta}$ 后得到的投影长度比之前大。类似地，对于  $y^{(i)}=1$，我们希望 $p^{(i)}||{\theta}|| ≥ 1$，所以如果 $p^{(i)}$ 大的话，就意味着 $||{\theta}||$ 小一些；对于  $y^{(i)}=0$，我们希望 $p^{(i)}||{\theta}|| ≤ -1$，所以如果 $p^{(i)}$（投影长度哈，是正的，只是最后结果 $p^{(i)}||{\theta}||$ 要加负号） 大的话，同样意味着 $||{\theta}||$ 小一些。而前面我们说了，优化目标是 $min\frac{1}{2}(||{\theta}||)^2$，即应尽量使 $||{\theta}||$ 小。这一情况刚好满足我们的目标。

综上，支持向量机最终会选择第二个决策边界，达到了优化目标。（以上的推导的前提都假设了 ${\theta}_0=0$, 使决策边界过原点）

## 10.3 核函数

> 用支持向量机构造非线性分类器，主要技巧是一个称之为**核（kernel）**的东西。

### 10.3.1 引入

回顾我们之前讨论过可以使用高阶多项式模型来解决无法用直线进行分隔的分类问题：

![image-20211003125844905](E:\md笔记\PyTorch\images\image-20211003125844905.png)

为了获得上图所示的判定边界，我们的模型可能是：

${{\theta }_{0}}+{{\theta }_{1}}{{x}_{1}}+{{\theta }_{2}}{{x}_{2}}+{{\theta }_{3}}{{x}_{1}}{{x}_{2}}+{{\theta }_{4}}x_{1}^{2}+{{\theta }_{5}}x_{2}^{2}+\cdots $ 

现在我们想用一些列的新的特征 $f$ 来替换旧的特征 $x$ ，即：

${{f}_{1}}={{x}_{1}},{{f}_{2}}={{x}_{2}},{{f}_{3}}={{x}_{1}}{{x}_{2}},{{f}_{4}}=x_{1}^{2},{{f}_{5}}=x_{2}^{2},\cdots$ 

得到：

$h_θ(x)={{\theta }_{1}}f_1+{{\theta }_{2}}f_2+...+{{\theta }_{n}}f_n$

思考：除了对原有的特征进行组合以外，有没有更好的方法来构造特征新特征 $f_1,f_2,f_3$？

解答：利用**核函数**计算新的特征。

### 10.3.2 计算新特征

以特征 $x_1$, $x_2$（先不考虑 $x_0$）为横纵坐标，然后任取三个**标记**(**landmarks**)点 $l^{(1)},l^{(2)},l^{(3)}$。给定一个实例训练样本 $x$，我们用标记点 $l^{(1)},l^{(2)},l^{(3)}$ 与 $x$ 之间的相似度来选取新的特征$f_1,f_2,f_3$。例如：

${{f}_{1}}=similarity(x,{{l}^{(1)}})=exp(-\frac{{{\left|| x-{{l}^{(1)}} \right||}^{2}}}{2{{\sigma }^{2}}})$

其中：$|| {{x-{{l}^{(1)}}||}^{2}}=\sum_{j=1}^n(x_j-l_j^{(1)})^2$，为实例 $x$ 中所有特征与地标 $l^{(1)}$ 之间的距离的和。`exp(a)`表示以 $e$ 为底、以 $a$ 为指数。

相似度函数$similarity(x,{{l}^{(1)}})$就是**核函数**，具体而言，这里是一个**高斯核函数**(**Gaussian Kernel**)。 **注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。**我们可以有不同的相似度度量函数，这里给出的是高斯核函数，之后也将看到其他的核函数。通常，我们将 $similarity(x,{{l}^{(i)}})$ 写成 $K(x,{{l}^{(i)}})$.

下面我们看下核函数到底做了什么，以及标记点的作用是什么。

假设1：<font color="blue"> $x$ 与其中一个标记点的距离近似于0</font>，则 $|| {{x-{{l}^{(1)}}||}^{2}}$ 接近为0，$exp(-\frac{{{\left|| x-{{l}^{(1)}} \right||}^{2}}}{2{{\sigma }^{2}}})$ 接近为1， 即新特征 $f$ 近似于 1。

假设2: <font color="blue"> $x$ 与标记点 $l$ 之间的距离都很远</font>，则 $f$ 近似于 $e^{-(一个大数)}=0$。

举例：假设我们的训练样本含有两个特征[$x_{1}$ $x{_2}$]，令 $l^{(1)}=[3, 5]^T$, ，给定地标$l^{(1)}$与不同的 $\sigma^2$ （高斯函数的参数）值，下图展示了 $x_1, x_2, f_1$ 之间的关系图**（相似度函数/核函数的曲线）**以及对应的等高线图：

![image-20211005101456868](E:\md笔记\PyTorch\images\image-20211005101456868.png)

由图可知，在 $\sigma^2$ 的不同取值下，核函数图像的大致变化趋势是类似的，只是 $\sigma^2$ 越小，那个凸起的宽度就会越窄，等高线图也就收缩了；当我们从 $l_1=(3,5)$ 开始向周围移动，那么特征 $f_1$ 下降到0的速度会变的很快。与此相反，当增大了 $\sigma^2$ ，当我们从 $l_1=(3,5)$ 移走的时候，特征变量的值减小的速度会变得比较慢。

### 10.3.3 计算决策边界

讲完了特征的定义，我们来看看能得到什么样的预测函数：给定一个训练样本 $x$ , 我们准备计算出三个特征变量 $f_1, f_2, f_3$ . 

已知当 $h_θ(x)=θ_0+θ_1f_1+θ_2f_2+θ_1f_3>0$，则预测 $y=1$。其中，令 ${\theta}_0=-0.5, {\theta}_1=1, {\theta}_2=1, {\theta}_3=0$ 。

> 注意区分假设函数和代价函数，性质是不一样的。

![image-20211005105953625](E:\md笔记\PyTorch\images\image-20211005105953625.png)

设训练样本处于上图洋红色点的位置，因为训练样本离 $l^{(1)}$ 更近，离 $l^{(2)}$ 和 $l^{(3)}$ 较远，因此 $f_1$ 接近1，而 $f_2$,$f_3$ 接近0。因此$h_θ(x)=θ_0+θ_1f_1+θ_2f_2+θ_1f_3>0$，因此预测 $y=1$。同理可以求出，对于蓝绿色的点，因为其离三个地标都较远，所以 $f_1, f_2, f_3$ 都接近于0，从而 $h_{\theta}(x)=-0.5<0$， 预测 $y=0$。

最终将得到一个决策边界，使得在一个范围内计算出来的 $h_{\theta}(x)>0$，预测 $y=1$；在另一个范围内计算出来的 $h_{\theta}(x)<0$，预测 $y=0$。在预测时，我们采用的特征不是训练样本本身的特征，而是通过核函数计算出的新特征 $f_1,f_2,f_3$。

以上，就是通过定义核函数和标记点来训练出复杂的非线性决策边界的方法。

### 10.3.4 如何选择标记点

结论：直接将训练样本作为标记点。则：

![image-20211005112719945](E:\md笔记\PyTorch\images\image-20211005112719945.png)

其中比较重要的一个值：

![image-20211005112817075](E:\md笔记\PyTorch\images\image-20211005112817075.png)

$f^{(i)}$ 是 $m+1$ 维的特征向量；$f_i^{(i)}$ 衡量的是 $x^{(i)}$ 与其自身的相似度。

现在，支持向量机的假设函数与训练目标变成如下形式：

![image-20211005114901067](E:\md笔记\PyTorch\images\image-20211005114901067.png)

下面我们将核函数运用到支持向量机中：

- 修改支持向量机的假设为：

  给定$x$，计算新特征$f$，当$θ^Tf>=0$ 时，预测 $y=1$；反之预测 $y=0$。

- 修改支持向量机的代价函数为：

  $min_{\theta} C\sum\limits_{i=1}^{m}{[{{y}^{(i)}}cos {{t}_{1}}}( {{\theta }^{T}}{{f}^{(i)}})+(1-{{y}^{(i)}})cos {{t}_{0}}( {{\theta }^{T}}{{f}^{(i)}})]+\frac{1}{2}\sum\limits_{j=1}^{n=m}{\theta _{j}^{2}}$ 

  > 在具体实施过程中，我们通常会为了简化计算而对最后的正则化项进行些微调整。首先将其变为向量运算：$\sum{_{j=1}^{n=m}}\theta _{j}^{2}={{\theta}^{T}}\theta$。其次，在计算 ${{\theta}^{T}}\theta$ 时，我们用 $θ^TMθ$ 代替 $θ^Tθ$，其中 $M$ 是根据我们选择的核函数而确定的一个矩阵。

理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 $M$ 来简化计算的方法不适用于逻辑回归，因此计算将非常耗费时间。

-- --

下面是支持向量机的两个参数 $C$ 和 ${\sigma}^2$ 的影响：

> $C$ 的作用与 $1/\lambda$ 相似，$\lambda$ 是逻辑回归中使用的正则化参数。

- $C$ 较大时，相当于 $\lambda$ 较小，可能会导致过拟合，高方差；

- $C$ 较小时，相当于 $\lambda$ 较大，可能会导致欠拟合，高偏差；

- ${\sigma}^2$ 较大时，可能会导致低方差，高偏差；

- ${\sigma}^2$ 较小时，可能会导致低偏差，高方差。

> ![image-20211005121024581](E:\md笔记\PyTorch\images\image-20211005121024581.png)
>
> 如果我们只有一个特征 $x_1$, 在其左侧有一个标记点 $l_1$. 如果 ${\sigma}^2$ 较大，则表示相似度函数/核函数的曲线相对平缓，这就使得随着输入 $x$，变得缓慢的模型，就导致高偏差、低方差。

如果你看了本周的编程作业，你就能亲自实现这些想法，并亲眼看到这些效果。这就是利用核函数的支持向量机算法，希望这些关于偏差和方差的讨论，能给你一些对于算法结果预期的直观印象。

### 10.3.5 其他几种核函数

## 10.4 使用SVM

目前为止，我们已经讨论了 **SVM** 比较抽象的层面。本节讲解**支持向量机算法**，在应用 **SVM** 时，需要用到它。支持向量机算法提出了一个特别优化的问题。常用的实现该算法的库：**liblinear **和 **libsvm**。使用库时，需要明确以下几点：

- 参数 $C$ 的选择
- 内核参数的选择或你想要使用的相似函数/核函数

> 介绍一些相似函数/核函数：
>
> - **线性核函数(linear kernel)**：一个不需要任何内核参数的相似函数。它在预测时只是使用了：${\theta}^Tx ≥ 0$，则 $y=1$；反之 $y=0$. 当训练集特征非常多而样本非常少的时候，就不需要拟合一个很复杂的非线性的决策边界，此时就可以选择线性核函数；否则，会容易出现过拟合。
> - **高斯内核函数(Gaussian kernel)**：${{f}_{1}}=exp(-\frac{{{\left|| x-{{l}^{(1)}} \right||}^{2}}}{2{{\sigma }^{2}}})$，其中 $l^{(i)}=x^{(i)}$, 需要选择 ${\sigma}^2$ 的值。当特征少、训练集样本多时，适合选择高斯核函数。
> - 多项式核函数（**Polynomial Kerne**l）
> - 字符串核函数（**String kernel**）
> - 卡方核函数（ **chi-square kernel**）
> - 直方图交集核函数（**histogram intersection kernel**）
> - 等等.......
>
> 【注意】不是所有你可能提出来的相似函数，都是有效的核函数，只有满足Mercer's定理（莫塞尔定理），才能被支持向量机的优化软件正确处理。



**【多类分类问题】**

假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有 $k$ 个类，则我们需要 $k$ 个模型，以及 $k$ 个参数向量 ${{\theta }}$。我们同样也可以训练 $k$ 个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。



**【几个使用标准】**

$n$ 为特征数，$m$ 为训练样本数。

(1)如果相较于 $m$ 而言，$n$ 要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。

(2)如果 $n$ 较小，而且 $m$ 大小中等，例如 $n$ 在 1-1000 之间，而 $m$ 在10-10000之间，使用高斯核函数的支持向量机。

(3)如果 $n$ 较小，而 $m$ 较大，例如 $n$ 在1-1000之间，而 $m$ 大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。

值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。

