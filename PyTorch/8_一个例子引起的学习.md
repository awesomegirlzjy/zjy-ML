> 源自：https://pytorch.org/tutorials/beginner/pytorch_with_examples.html

我们将$y = sin(x)$与三阶多项式的拟合问题作为我们的运行示例。该网络将具有四个参数，并且将通过使网络输出与真实输出之间的欧几里德距离最小化来进行梯度下降训练，以适应随机数据。

# 用numpy实现

`numpy`是一个用于科学计算的通用框架，它并不存在计算图、深度学习或者梯度这些概念。但我们仍然可以使用它来将一个三阶多项式拟合为正弦函数，只需手动实现前向传播和后向传播。

```python
import math
import numpy as np

# input:
x = np.linspace(-math.pi, math.pi, 2000)
# output:
y = np.sin(x)
# weights:
a = np.random.randn()
b = np.random.randn()
c = np.random.randn()
d = np.random.randn()

learning_rate = 1e-6
for t in range(2000):
    # 正向传播(计算预测的y：y = a + b x + c x^2 + d x^3)
    y_pred = a + b * x + c * x ** 2 + d * x ** 3
    # 计算损失值
    loss = np.square(y_pred - y).sum()
    if t % 100 == 99:
        print(t, loss)

    # 反向传播(计算a,b,c,d相对于loss的梯度)
    grad_y_pred = 2.0 * (y_pred - y)
    grad_a = grad_y_pred.sum()
    grad_b = (grad_y_pred * x).sum()
    grad_c = (grad_y_pred * x ** 2).sum()
    grad_d = (grad_y_pred * x ** 3).sum()

    # 更新权重
    a -= learning_rate * grad_a
    b -= learning_rate * grad_b
    c -= learning_rate * grad_c
    d -= learning_rate * grad_d

print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')
```

![image-20210908003149755](E:\md笔记\PyTorch\images\image-20210908003149755.png)

# 用Tensor实现

PyTorch中的Tensor基本上等同于numpy：它们都对深度学习、计算图或梯度一无所知，仅仅是一个用于科学计算的n维数组。

Tensor与numpy最大的不同：Tensor可以应用在GPU上从而加快运算；要想使其运行在GPU上，只需将Tensor转换为cuda数据类型。
```python
import math
import torch

dtype = torch.float
device = torch.device('cpu')
# device = torch.device("cuda:0")  # 如果使用GPU，则取消注释即可

# input:
x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)
# output:
y = torch.sin(x)
# weights:
a = torch.randn((), device=device, dtype=dtype)
b = torch.randn((), device=device, dtype=dtype)
c = torch.randn((), device=device, dtype=dtype)
d = torch.randn((), device=device, dtype=dtype)

learning_rate = 1e-6
for t in range(2000):
    #正向传播(计算预测的y：y = a + b x + c x^2 + d x^3)
    y_pred = a + b * x + c * x ** 2 + d * x ** 3
    # compute loss:
    loss = (y_pred - y).pow(2).sum().item()
    if t % 100 == 99:
        print(t, loss)

    # 反向传播(计算a,b,c,d相对于loss的梯度)
    grad_y_pred = 2.0 * (y_pred - y)
    grad_a = grad_y_pred.sum()
    grad_b = (grad_y_pred * x).sum()
    grad_c = (grad_y_pred * x ** 2).sum()
    grad_d = (grad_y_pred * x ** 3).sum()

    # update weights:
    a -= learning_rate * grad_a
    b -= learning_rate * grad_b
    c -= learning_rate * grad_c
    d -= learning_rate * grad_d

print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')
```
# 用Autograd实现

我们可以使用**自动差分（automatic differentiation）**在神经网络中自动计算反向传递。`autograd`包实现了此功能。使用autograd时，网络的**正向传递**将定义一个计算图；计算图中的每个叶子结点都是Tensor，每条边代表从输入张量生成输出张量的函数。在进行**反向传播**时，将自动计算梯度。

下面，我们就无须手动实现反向传播了（直接使用“轮子”；正向传播仍然是用的是Tensor）：
```python
import math
import torch

dtype = torch.float
device = torch.device('cpu')
# device = torch.device("cuda:0")  # 如果使用GPU，则取消注释即可

# input:
x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)
# output:
y = torch.sin(x)
# weights:
a = torch.randn((), device=device, dtype=dtype, requires_grad=True)
b = torch.randn((), device=device, dtype=dtype, requires_grad=True)
c = torch.randn((), device=device, dtype=dtype, requires_grad=True)
d = torch.randn((), device=device, dtype=dtype, requires_grad=True)

learning_rate = 1e-6
for t in range(2000):
    #正向传播(计算预测的y：y = a + b x + c x^2 + d x^3)
    y_pred = a + b * x + c * x ** 2 + d * x ** 3
    # compute loss:
    loss = (y_pred - y).pow(2).sum()
    if t % 100 == 99:
        print(t, loss)

    # 反向传播(所有设置了requires_grad=True的Tensor都将自动计算梯度)
    loss.backward()

    # update weights:
    with torch.no_grad():
        a -= learning_rate * a.grad
        b -= learning_rate * b.grad
        c -= learning_rate * c.grad
        d -= learning_rate * d.grad
        # 手动将梯度更新为0
        a.grad = None
        b.grad = None
        c.grad = None
        d.grad = None

print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')
```
## 自定义autograd函数
每个原始的autograd运算符实际上都是在Tensor上运行的两个函数：
1）forward()：从输入张量计算输出张量。
2）backward()：接收输出张量相对于某个标量值的梯度，并计算输入张量相对于相同标量值的梯度。

定义一个继承自`torch.autograd.Function`的类，并实现forward()、backward()。然后，我们可以通过构造实例并像调用函数一样调用新的autograd运算符，并传递包含输入数据的张量。

下面，规定我们的模型为$y = a + bP_3(c+dx)$，自定义autograd函数，并利用它来计算$P_3(x) =  \dfrac{1}{2}(5x^3-3x)$的前向传播和反向传播。

```python
import math
import torch

class LegendrePolynomial3(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return 0.5 * (5 * input ** 3 - 3 * input)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        return grad_output * 1.5 * (5 * input ** 2 - 1)

dtype = torch.float
device = torch.device('cpu')
# device = torch.device("cuda:0")  # 如果使用GPU，则取消注释即可

# input:
x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)
# output:
y = torch.sin(x)
# weights: 需要将这些权重初始化为与正确的结果相差不远的值，以确保收敛。
a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)
b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)
c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)
d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)

learning_rate = 1e-6
for t in range(2000):
    # 实例化P_3
    P3 = LegendrePolynomial3.apply
    # 正向传播
    y_pred = a + b * P3(c + d * x)
    # compute loss:
    loss = (y_pred - y).pow(2).sum()
    if t % 100 == 99:
        print(t, loss)

    # 反向传播(所有设置了requires_grad=True的Tensor都将自动计算梯度)
    loss.backward()

    # update weights:
    with torch.no_grad():
        a -= learning_rate * a.grad
        b -= learning_rate * b.grad
        c -= learning_rate * c.grad
        d -= learning_rate * d.grad
        # 手动将梯度更新为0
        a.grad = None
        b.grad = None
        c.grad = None
        d.grad = None

print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')
```

## PyTorch: nn
PyTorch的autograd使得定义计算图、计算梯度变得很容易，但是原始的autograd对于定义一个复杂一些的神经网络并不够好。而`nn`包可以帮助我们解决这一问题。

`nn`包中定义了一系列的Modules以及损失函数(loss functions)。

下面我们使用`nn`包来实现一个模型网络。
```python
import math
import torch

# input:
x = torch.linspace(-math.pi, math.pi, 2000)
# output:
y = torch.sin(x)

'''
p的形状为(3,)，x.unsqueeze(-1)的形状为(2000, 3)，广播语义将用于获取一个形状为(2000, 3)的张量
'''
p = torch.tensor([1, 2, 3])
xx = x.unsqueeze(-1).pow(p)


'''
使用nn包将我们的模型定义为一系列序列；
nn.Sequential是一个包含其他许多Modules的Module。
'''
model = torch.nn.Sequential(
    torch.nn.Linear(3, 1),  #  The Linear Module computes output from input using a linear function, and holds internal Tensors for its weight and bias.
    torch.nn.Flatten(0, 1)  # The Flatten layer flatens the output of the linear layer to a 1D tensor, to match the shape of `y`.
)

loss_fn = torch.nn.MSELoss(reduction='sum')

learning_rate = 1e-6
for t in range(2000):
    # 正向传播
    y_pred = model(xx)
    # 计算损失值
    loss = loss_fn(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    model.zero_grad()

    # 反向传播(所有设置了requires_grad=True的Tensor都将自动计算梯度)
    loss.backward()

    # 使用梯度下降来更新权重:
    with torch.no_grad():
        for param in model.parameters():
            param -= learning_rate * param.grad

# 访问"模型"的第一层
linear_layer = model[0]
print(f"Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:,0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3")
```
## optim
`optim`包抽象了优化算法的理念，并提供了常用优化算法的实现。有了它，我们就不用手动更新权重了，而是使用`optim`包定义一个优化器，让优化器自动帮我们更新权重。

```python
import math
import torch

# input:
x = torch.linspace(-math.pi, math.pi, 2000)
# output:
y = torch.sin(x)

'''
p的形状为(3,)，x.unsqueeze(-1)的形状为(2000, 3)，广播语义将用于获取一个形状为(2000, 3)的张量
'''
p = torch.tensor([1, 2, 3])
xx = x.unsqueeze(-1).pow(p)


'''
使用nn包将我们的模型定义为一系列序列；
nn.Sequential是一个包含其他许多Modules的Module。
'''
model = torch.nn.Sequential(
    torch.nn.Linear(3, 1),  #  The Linear Module computes output from input using a linear function, and holds internal Tensors for its weight and bias.
    torch.nn.Flatten(0, 1)  # The Flatten layer flatens the output of the linear layer to a 1D tensor, to match the shape of `y`.
)

loss_fn = torch.nn.MSELoss(reduction='sum')
learning_rate = 1e-3
optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)


for t in range(2000):
    # 正向传播
    y_pred = model(xx)
    # 计算损失值
    loss = loss_fn(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # 反向传播前先将梯度清零（否则梯度会累加）
    optimizer.zero_grad()

    # 反向传播(所有设置了requires_grad=True的Tensor都将自动计算梯度)
    loss.backward()

    # 优化——更新权重
    optimizer.step()

# 访问"模型"的第一层
linear_layer = model[0]
print(f"Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:,0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3")
```

## 自定义nn模型
通过继承`nn.Module`自定义一个模型类，并实现`forward()`。

```python
import math
import torch


class Polynomial3(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.a = torch.nn.Parameter(torch.randn(()))
        self.b = torch.nn.Parameter(torch.randn(()))
        self.c = torch.nn.Parameter(torch.randn(()))
        self.d = torch.nn.Parameter(torch.randn(()))
    def forward(self, x):
        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3
    def string(self):
        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'

# input:
x = torch.linspace(-math.pi, math.pi, 2000)
# output:
y = torch.sin(x)

model = Polynomial3()

criterion  = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)

for t in range(2000):
    # 正向传播
    y_pred = model(x)
    # 计算损失值
    loss = criterion(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # 反向传播前先将梯度清零（否则梯度会累加）
    optimizer.zero_grad()

    # 反向传播(所有设置了requires_grad=True的Tensor都将自动计算梯度)
    loss.backward()

    # 优化——更新权重
    optimizer.step()


print(f"Result: {model.string()}")
```

## Control Flow + Weight Sharing
为了展示PyTorch动态图的强大功能，我们将实现一个非常奇怪的模型：一个三阶多项式，在每个前向传递中选择3到5之间的一个随机数，并使用该数量的阶次，多次重复使用相同的权重进行计算第四和第五阶。
```python
import math
import random
import torch


class DynamicNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.a = torch.nn.Parameter(torch.randn(()))
        self.b = torch.nn.Parameter(torch.randn(()))
        self.c = torch.nn.Parameter(torch.randn(()))
        self.d = torch.nn.Parameter(torch.randn(()))
        self.e = torch.nn.Parameter(torch.randn(()))
    def forward(self, x):
        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3
        for exp in range(4, random.randint(4, 6)):  # 取随机数，得到四阶或五阶的多项式
            y = y + self.e * x ** exp
        return y
    def string(self):
        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'

# input:
x = torch.linspace(-math.pi, math.pi, 2000)
# output:
y = torch.sin(x)

model = DynamicNet()

criterion  = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)

for t in range(30000):
    # 正向传播
    y_pred = model(x)
    # 计算损失值
    loss = criterion(y_pred, y)
    if t % 2000 == 1999:
        print(t, loss.item())

    # 反向传播前先将梯度清零（默认情况下，梯度会累加）
    optimizer.zero_grad()

    # 反向传播(所有设置了requires_grad=True的Tensor都将自动计算梯度)
    loss.backward()

    # 优化——更新权重
    optimizer.step()


print(f"Result: {model.string()}")
```