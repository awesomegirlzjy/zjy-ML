> å­¦ä¹ èµ„æºæ¥è‡ª==ã€Šæ·±åº¦å­¦ä¹ ä¹‹PyTorchå®æˆ˜è®¡ç®—æœºè§†è§‰ã€‹(å”è¿›æ°‘)== . ä¸€æ­¥æ­¥è®©ä½ å¼„æ‡‚ä½¿ç”¨PyTorchå¦‚ä½•æ­å»ºä¸€ä¸ªç¥ç»ç½‘ç»œã€‚

> æœºå™¨å­¦ä¹ çš„å·¥ä½œæµç¨‹ï¼š
> 1. å¤„ç†æ•°æ® working with data
> 2. åˆ›å»ºæ¨¡å‹ creating models
> 3. ä¼˜åŒ–æ¨¡å‹å‚æ•° optimizing model parameters
> 4. ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ saving the trained models

# æ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€
## æ¬ æ‹Ÿåˆã€è¿‡æ‹Ÿåˆ
æ¬ æ‹Ÿåˆï¼šå¯¹å·²æœ‰æ•°æ®çš„åŒ¹é…æ€§å¾ˆå·®ï¼Œå¯¹æ•°æ®ä¸­çš„å™ªå£°ä¸æ•æ„Ÿã€‚
è¿‡æ‹Ÿåˆï¼šå¯¹æ•°æ®çš„åŒ¹é…æ€§å¤ªå¥½ï¼Œå¯¹æ•°æ®ä¸­çš„å™ªå£°éå¸¸æ•æ„Ÿã€‚

## æ­£å‘ä¼ æ’­ã€åå‘ä¼ æ’­
æ­£å‘ä¼ æ’­æ²¿ç€ä»è¾“â¼Šå±‚åˆ°è¾“å‡ºå±‚çš„é¡ºåºï¼Œä¾æ¬¡è®¡ç®—å¹¶å­˜å‚¨ç¥ç»â½¹ç»œçš„ä¸­é—´å˜é‡ã€‚
åå‘ä¼ æ’­æ²¿ç€ä»è¾“å‡ºå±‚åˆ°è¾“â¼Šå±‚çš„é¡ºåºï¼Œä¾æ¬¡è®¡ç®—å¹¶å­˜å‚¨ç¥ç»â½¹ç»œçš„ä¸­é—´å˜é‡å’Œå‚æ•°çš„æ¢¯åº¦ã€‚

> - åœ¨é€šè¿‡å¤šæ¬¡åå‘ä¼ æ’­åï¼Œå°±å¯ä»¥å¾—åˆ°æ¨¡å‹çš„æœ€ä¼˜å‚æ•°ç»„åˆã€‚æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„å‚æ•°è¿›è¡Œåå‘ä¼ æ’­çš„è¿‡ç¨‹å…¶å®å°±æ˜¯ä¸€ä¸ªå¤åˆå‡½æ•°æ±‚å¯¼çš„è¿‡ç¨‹ã€‚
> - åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç›¸äº’ä¾èµ–ã€‚
## æŸå¤±å’Œä¼˜åŒ–
==æŸå¤±==
- åº¦é‡é¢„æµ‹å€¼å’Œæ•°æ®çœŸå®å€¼ä¹‹é—´çš„å·®è·ï¼Œç”¨æ¥è¡¡é‡è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¥½åã€‚
- æ¨¡å‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®è·è¶Šå¤§ï¼ŒæŸå¤±å€¼å°±ä¼šè¶Šé«˜ï¼Œè¿™æ—¶æˆ‘ä»¬å°±éœ€è¦é€šè¿‡ä¸æ–­åœ°å¯¹æ¨¡å‹ä¸­çš„å‚æ•°è¿›è¡Œä¼˜åŒ–æ¥å‡å°‘æŸå¤±ã€‚
- è¿›è¡ŒæŸå¤±å€¼è®¡ç®—çš„å‡½æ•°å«åš*æŸå¤±å‡½æ•°*ï¼Œå¦‚ï¼šå‡æ–¹è¯¯å·®å‡½æ•°ï¼ˆMean Square Error, MSEï¼‰ã€å‡æ–¹æ ¹è¯¯å·®å‡½æ•°ï¼ˆRoot Mean Square Error, RMSEï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®å‡½æ•°ï¼ˆMean Absolute Error, MAEï¼‰ã€‚

==ä¼˜åŒ–==  

- å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–çš„æœ€ç»ˆç›®çš„å°½å¯èƒ½çš„åœ¨ä¸è¿‡æ‹Ÿåˆçš„æƒ…å†µä¸‹**é™ä½æŸå¤±å€¼**ã€‚
- **åå‘ä¼ æ’­**æ˜¯æ¨¡å‹å‚æ•°ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œåœ¨å®é™…çš„ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜é¢ä¸´åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç›¸å…³å‚æ•°çš„åˆå§‹åŒ–ã€å‚æ•°ä»¥ä½•ç§å½¢å¼è¿›è¡Œå¾®è°ƒã€å¦‚ä½•é€‰å–åˆé€‚çš„**å­¦ä¹ é€Ÿç‡**ï¼ˆå­¦ä¹ é€Ÿç‡ç”¨äºæ§åˆ¶æ¢¯åº¦æ›´æ–°çš„å¿«æ…¢ã€‚å­¦ä¹ é€Ÿç‡è¶Šå¿«ï¼Œå‚æ•°çš„æ›´æ–°è·¨æ­¥å°±ä¼šè¶Šå¤§ï¼Œæ˜“å‡ºç°å±€éƒ¨æœ€ä¼˜å’ŒæŠ–åŠ¨ï¼‰ç­‰é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥æŠŠä¼˜åŒ–å‡½æ•°çœ‹åšä¸Šè¿°é—®é¢˜çš„è§£å†³æ–¹æ¡ˆçš„é›†åˆã€‚
- å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¼˜åŒ–çš„å‡½æ•°å«åš*ä¼˜åŒ–å‡½æ•°*ã€‚
- **æ¢¯åº¦**ï¼šæ¢¯åº¦æ˜¯å°†å¤šå…ƒå‡½æ•°çš„å„ä¸ªå‚æ•°æ±‚å¾—çš„åå¯¼æ•°ä»¥å‘é‡çš„å½¢å¼å±•ç°å‡ºæ¥ï¼Œä¹Ÿå«åšå¤šå…ƒå‡½æ•°çš„æ¢¯åº¦ã€‚åå¯¼æ•°å°±æ˜¯åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªå‚æ•°æ±‚å¾—çš„åå¯¼æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨æ¨¡å‹ä¼˜åŒ–çš„è¿‡ç¨‹ä¸­ä½¿ç”¨çš„å‚æ•°å¾®è°ƒå€¼å…¶å®å°±æ˜¯å‡½æ•°è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ï¼Œè¿™ä¸ªè¿‡ç¨‹åˆå«åšå‚æ•°çš„æ¢¯åº¦æ›´æ–°ã€‚ã€å¯¹äºåªæœ‰å•ä¸ªå‚æ•°çš„å‡½æ•°ï¼Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„å¯¼æ•°æ¥å®Œæˆå‚æ•°çš„æ›´æ–°ï¼Œå¦‚æœåœ¨ä¸€ä¸ªå‡½æ•°ä¸­éœ€è¦å¤„ç†çš„æ˜¯å¤šä¸ªå‚æ•°çš„é—®é¢˜ï¼Œå°±é€‰æ‹©ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ¥å®Œæˆå‚æ•°çš„æ›´æ–°ã€‚ **===>**  æ¢¯åº¦å°±æ˜¯å¯¼æ•°å‘€ï¼Œæ˜¯å¤šä¸ªåå¯¼æ•°ç»„åˆæˆçš„ä¸€ä¸ªå‘é‡ã€‘
- å®é™…æ“ä½œä¸­æœ€å¸¸ç”¨åˆ°çš„æ˜¯ä¸€é˜¶ä¼˜åŒ–å‡½æ•°ï¼Œå¦‚æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descent, GDï¼‰ã€æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆBatch Gradient Descent, BGDï¼‰ã€éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descent, SGDï¼‰ã€Momentumã€Adagradã€è‡ªé€‚åº”æ—¶åˆ»ä¼°è®¡æ–¹æ³•ï¼ˆAdaptive Moment Estimation, Adamï¼‰ï¼Œç­‰ç­‰ã€‚ä¸€é˜¶ä¼˜åŒ–å‡½æ•°åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ±‚è§£çš„æ˜¯å‚æ•°çš„ä¸€é˜¶å¯¼æ•°ï¼Œè¿™äº›ä¸€é˜¶å¯¼æ•°çš„å€¼å°±æ˜¯æ¨¡å‹ä¸­å‚æ•°çš„å¾®è°ƒå€¼ã€‚
## æ¿€æ´»å‡½æ•°
**åç½®ï¼ˆBiasï¼‰**ï¼šåç½®å¯ä»¥è®©æˆ‘ä»¬æ­å»ºçš„ç¥ç»ç½‘ç»œæ¨¡å‹åç¦»åŸç‚¹ï¼ˆæ²¡æœ‰åç½®çš„å‡½æ•°å¿…å®šä¼šç»è¿‡åŸç‚¹ï¼‰ã€‚

- æ— åç½®ï¼šf(x) = W * X
- æœ‰åç½®bï¼šf(x) = W * X + b

**æ¿€æ´»å‡½æ•°**çš„å¼•å…¥ç»™æˆ‘ä»¬æ­å»ºçš„æ¨¡å‹å¸¦æ¥äº†éçº¿æ€§å› ç´ ï¼Œéçº¿æ€§çš„æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„é—®é¢˜ï¼Œæ‰€ä»¥é€šè¿‡é€‰å–ä¸åŒçš„æ¿€æ´»å‡½æ•°ä¾¿å¯ä»¥å¾—åˆ°å¤æ‚å¤šå˜çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼šSigmoidã€tanhã€**ReLU**ï¼ˆRectified Linear Unitï¼Œä¿®æ­£çº¿æ€§å•å…ƒï¼‰ã€‚

# ğŸ”‘æ­å»ºä¸€ä¸ªç®€æ˜“ç¥ç»ç½‘ç»œ
### ä»£ç 
```python
# Step1ã€å¯¼åŒ…
import torch


# Step2ã€å®šä¹‰å˜é‡
batch_n = 100        # ä¸€ä¸ªæ‰¹æ¬¡ä¸­è¾“å…¥æ•°æ®çš„æ•°é‡ï¼ˆæ¯ä¸ªæ•°æ®åŒ…å«çš„æ•°æ®ç‰¹å¾æœ‰input_dataä¸ªï¼‰
hidden_layer = 100   # ç»è¿‡éšè—å±‚åä¿ç•™çš„æ•°æ®ç‰¹å¾çš„ä¸ªæ•°ï¼ˆæ­¤ç¨‹åºä»…è€ƒè™‘ä¸€å±‚éšè—å±‚ï¼Œæ‰€ä»¥ä»…å®šä¹‰ä¸€ä¸ªéšè—å±‚çš„å‚æ•°ï¼‰
input_data = 1000    # æ¯ä¸ªæ•°æ®åŒ…å«çš„æ•°æ®ç‰¹å¾çš„ä¸ªæ•°
output_data = 10     # æœ‰output_dataç§åˆ†ç±»ç»“æœ


# Step3ã€å®šä¹‰ç»´åº¦ & åˆå§‹åŒ–æƒé‡
x = torch.randn(batch_n, input_data)   # è¾“å…¥å±‚ç»´åº¦(100, 1000)
y = torch.randn(batch_n, output_data)  # è¾“å‡ºå±‚ç»´åº¦(100, 10)

w1 = torch.randn(input_data, hidden_layer)   # ä»è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡å‚æ•°ç»´åº¦(1000, 100)
w2 = torch.randn(hidden_layer, output_data)  # ä»éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡å‚æ•°ç»´åº¦(100, 10)


# Step4ã€å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ
epoch_n = 20          # è®­ç»ƒæ¬¡æ•°/åå‘ä¼ æ’­çš„æ¬¡æ•°
learning_rate = 1e-6  # æ¢¯åº¦ä¸‹é™ä½¿ç”¨çš„å­¦ä¹ é€Ÿç‡

for epoch in range(epoch_n): 	 # ã€ä¸‹æœ‰è¯¦ç»†ä»‹ç»ã€‘
    # 4.1 æ­£å‘ä¼ æ’­
    h1 = x.mm(w1)         # çŸ©é˜µä¹˜ç§¯ï¼šx(100, 1000) * w1(1000, 100) ==> h1(100, 100)
    h1 = h1.clamp(min=0)  # å¯¹h1è¿›è¡Œè£å‰ªï¼Œå°†å°äº0çš„å€¼å…¨éƒ¨é‡æ–°èµ‹å€¼ä¸º0(è¿™å°±åƒåŠ ä¸Šäº†ä¸€ä¸ªReLUæ¿€æ´»å‡½æ•°çš„åŠŸèƒ½)
    y_pred = h1.mm(w2)    # æ­£å‘ä¼ æ’­å¾—åˆ°çš„é¢„æµ‹ç»“æœï¼šh1(100, 100) * w2(100, 10) ==> y_pred(100, 10)

    loss = (y_pred - y).pow(2).sum()  # è®¡ç®—é¢„æµ‹ç»“æœä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®ï¼ˆä½¿ç”¨çš„æ˜¯å‡æ–¹è¯¯å·®å‡½æ•°ï¼‰
    print("Epoch: {}, Loss:{:.4f}".format(epoch, loss))

    # 4.2 åå‘ä¼ æ’­ï¼ˆè®¡ç®—é“¾å¼æ±‚å¯¼çš„ç»“æœï¼‰
    grad_y_pred = 2 * (y_pred - y)
    grad_w2 = h1.t().mm(grad_y_pred)  # w2å¯¹åº”çš„æ¢¯åº¦

    grad_h = grad_y_pred.clone()
    grad_h = grad_h.mm(w2.t())
    grad_h.clamp_(min=0)
    grad_w1 = x.t().mm(grad_h)  	  # w1å¯¹åº”çš„æ¢¯åº¦

    # 4.3 æ ¹æ®å­¦ä¹ é€Ÿç‡å¯¹w1ã€w2çš„æƒé‡å‚æ•°è¿›è¡Œæ›´æ–°
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
```
### éƒ¨åˆ†ä»£ç è§£é‡Š 
ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´è¿‡ç¨‹ï¼š
1. å…ˆè¾“å…¥100ä¸ªå…·æœ‰1000ä¸ªç‰¹å¾çš„æ•°æ®ï¼ˆè¾“å…¥å±‚ï¼‰ï¼›
2. ç»è¿‡éšè—å±‚åå˜æˆ100ä¸ªå…·æœ‰100ä¸ªç‰¹å¾çš„æ•°æ®ï¼ˆéšè—å±‚ï¼‰ï¼›
> å‰ä¸¤æ­¥å¯¹åº”`h1 = x.mm(w1)`
3. å†ç»è¿‡è¾“å‡ºå±‚åè¾“å‡º100ä¸ªå…·æœ‰10ä¸ªåˆ†ç±»ç»“æœå€¼çš„æ•°æ®ï¼ˆè¾“å‡ºå±‚ï¼‰ï¼›
> ç¬¬ä¸‰æ­¥å¯¹åº”`y_pred = h1.mm(w2)`
4. å¾—åˆ°è¾“å‡ºç»“æœä¹‹åè®¡ç®—**æŸå¤±å€¼**å¹¶è¿›è¡Œ**åå‘ä¼ æ’­**ã€‚

æœ¬ç¨‹åºä¸­å®šä¹‰çš„è¾“å…¥å±‚`x`ã€è¾“å‡ºå±‚`y`ã€`y_pred `ï¼Œå…¶å®å°±åˆ†åˆ«å¯¹åº”çœŸå®çš„æ•°æ®é›†é‡Œçš„`æ•°æ®`ã€`æ•°æ®æ‰€å±åˆ†ç±»`ã€`ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹(å°±æ˜¯åšäº†ä¸¤æ¬¡çŸ©é˜µçš„ä¹˜ç§¯)é¢„æµ‹å‡ºæ¥çš„æ•°æ®çš„åˆ†ç±»`ã€‚åº”è¯¥ç€é‡äºæ€»ä½“æ­¥éª¤ã€‚

### æ‰“å°ç»“æœ
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2021042220044381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
### ç»“è®º
éšç€è®­ç»ƒã€ä¼˜åŒ–çš„æ¬¡æ•°ä¸æ–­å¢åŠ ï¼ŒæŸå¤±å€¼ï¼ˆçœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®å€¼ï¼‰åœ¨ä¸æ–­å‡å°‘ã€‚

# è‡ªåŠ¨æ¢¯åº¦(torch.autogradã€Variable)
### ç†è®ºåŸºç¡€
1. `torch.autograd`åŒ…ï¼šPyTorchæä¾›çš„`torch.autograd`åŒ…ï¼Œå®ç°äº†æ¨¡å‹å‚æ•°è‡ªåŠ¨è®¡ç®—åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­éœ€è¦ç”¨åˆ°çš„æ¢¯åº¦å€¼ï¼ˆæˆ–è€…è¯´å®ƒå®Œæˆäº†ç¥ç»ç½‘ç»œåå‘ä¼ æ’­ä¸­çš„**é“¾å¼æ±‚å¯¼**ï¼‰ã€‚
2. **è‡ªåŠ¨æ¢¯åº¦**çš„å®ç°è¿‡ç¨‹ï¼šå…ˆé€šè¿‡è¾“å…¥çš„Tensoræ•°æ®ç±»å‹çš„å˜é‡åœ¨ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ç”Ÿæˆä¸€å¼ **è®¡ç®—å›¾**ï¼Œç„¶åæ ¹æ®è¿™ä¸ªè®¡ç®—å›¾å’Œè¾“å‡ºç»“æœå‡†ç¡®è®¡ç®—å‡ºæ¯ä¸ªå‚æ•°éœ€è¦æ›´æ–°çš„**æ¢¯åº¦**ï¼Œå¹¶é€šè¿‡å®Œæˆ**åå‘ä¼ æ’­**æ¥å®Œæˆå¯¹å‚æ•°çš„æ¢¯åº¦æ›´æ–°ã€‚
3. ä½¿ç”¨`torch.autograd`åŒ…ä¸­çš„**Variableç±»**å¯¹Tensorå®ä¾‹å¯¹è±¡è¿›è¡Œå°è£…ï¼Œåœ¨å°è£…åï¼Œè®¡ç®—å›¾ä¸­çš„å„ä¸ªç»“ç‚¹å°±æ˜¯ä¸€ä¸ªVariableå¯¹è±¡ï¼Œè¿™æ ·æ‰èƒ½åº”ç”¨è‡ªåŠ¨æ¢¯åº¦çš„åŠŸèƒ½ã€‚
4.  å¦‚æœå·²ç»æŒ‰ç…§å¦‚ä¸Šæ–¹å¼å®Œæˆäº†ç›¸å…³çš„æ“ä½œï¼Œåˆ™åœ¨é€‰ä¸­äº†è®¡ç®—å›¾ä¸­çš„æŸä¸ªèŠ‚ç‚¹æ—¶ï¼Œè¿™ä¸ªèŠ‚ç‚¹å¿…å®šä¼šæ˜¯ä¸€ä¸ª`Variable`å¯¹è±¡ã€‚è®¾`X`ä¸ºä¸€ä¸ªèŠ‚ç‚¹ï¼Œé‚£ä¹ˆ`X.data`ä»£è¡¨Tensoræ•°æ®ç±»å‹çš„å˜é‡ï¼Œ`X.grad`ä¹Ÿæ˜¯ä¸€ä¸ªVariableå¯¹è±¡ï¼Œä¸è¿‡å®ƒè¡¨ç¤ºçš„æ˜¯`X`çš„æ¢¯åº¦ï¼Œåœ¨æƒ³è®¿é—®æ¢¯åº¦å€¼æ—¶éœ€è¦ä½¿ç”¨`X.grad.data`ã€‚
### ä»£ç 
```python
# Step1ã€å¯¼åŒ…
import torch
from torch.autograd import Variable


# Step2ã€å®šä¹‰å˜é‡
batch_n = 100
hidden_layer = 100 
input_data = 1000 
output_data = 10 


# Step3ã€å®šä¹‰ç»´åº¦ & åˆå§‹åŒ–æƒé‡ ã€ä¸‹æœ‰è¯¦ç»†ä»‹ç»â€”â€”Variableã€‘
x = Variable(torch.randn(batch_n, input_data), requires_grad=False)  # è¾“å…¥å±‚ç»´åº¦
y = Variable(torch.randn(batch_n, output_data), requires_grad=False)  # è¾“å‡ºå±‚ç»´åº¦
w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad=True)  # ä»è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡å‚æ•°ç»´åº¦
w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad=True)  # ä»éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡å‚æ•°ç»´åº¦


# Step4ã€å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ
epoch_n = 20   
learning_rate = 1e-6

for epoch in range(epoch_n):
	# 4.1 æ­£å‘ä¼ æ’­
    y_pred = x.mm(w1).clamp(min=0).mm(w2)
    loss = (y_pred-y).pow(2).sum()
    print("Epoch:{}, Loss:{:.4f}".format(epoch, loss.data.item()))  # åªåŒ…å«ä¸€ä¸ªæ•´å‹çš„Tensorå‹å˜é‡è°ƒç”¨item()å‡½æ•°å¯ä»¥ç›´æ¥æŠŠTensorè½¬æ¢æˆä¸€ä¸ªæ•´å‹å˜é‡

	# 4.2 åå‘ä¼ æ’­
    loss.backward()   # ã€ä¸‹æœ‰è¯¦ç»†ä»‹ç»â€”â€”backward()ã€‘

	# 4.3 æ ¹æ®å­¦ä¹ é€Ÿç‡å¯¹w1ã€w2çš„æƒé‡å‚æ•°è¿›è¡Œæ›´æ–°
    w1.data -= learning_rate*w1.grad.data  # w1.grad.dataæ˜¯æ±‚å¾—çš„æ¢¯åº¦å€¼
    w2.data -= learning_rate*w2.grad.data

    # 4.4 å°†æœ¬æ¬¡è®¡ç®—å¾—åˆ°çš„å„ä¸ªå‚æ•°èŠ‚ç‚¹çš„æ¢¯åº¦å€¼é€šè¿‡grad.data.zero_()å…¨éƒ¨ç½®é›¶ï¼Œå¦åˆ™ï¼Œè®¡ç®—çš„æ¢¯åº¦å€¼ä¼šè¢«ä¸€ç›´ç´¯åŠ ï¼Œå½±å“åç»­çš„è®¡ç®—ã€‚
    w1.grad.data.zero_()
    w2.grad.data.zero_()
```
### éƒ¨åˆ†ä»£ç è§£é‡Š
Step3ä¸­ï¼Œä½¿ç”¨`torch.autograd`åŒ…ä¸­çš„Variableç±»å¯¹Tensorå®ä¾‹å¯¹è±¡è¿›è¡Œå°è£…ã€‚å‚æ•°`requires_grad=False`è¡¨ç¤ºï¼šè¯¥å˜é‡åœ¨è¿›è¡Œè‡ªåŠ¨æ¢¯åº¦è®¡ç®—çš„è¿‡ç¨‹ä¸­ä¸ä¼šä¿ç•™æ¢¯åº¦å€¼ã€‚æŠŠ`x`, `y`è®¾ç½®ä¸º`False`ï¼Œæ˜¯å› ä¸ºè¿™ä¸¤ä¸ªå˜é‡å¹¶ä¸æ˜¯æˆ‘ä»¬çš„æ¨¡å‹éœ€è¦ä¼˜åŒ–çš„å‚æ•°ã€‚

Step4.2ä¸­ï¼Œ`backward()`å‡½æ•°çš„åŠŸèƒ½æ˜¯è®©æ¨¡å‹æ ¹æ®è®¡ç®—å›¾è‡ªåŠ¨è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ¢¯åº¦å€¼å¹¶æ ¹æ®éœ€æ±‚è¿›è¡Œä¿ç•™ã€‚æœ‰äº†è¿™ä¸€æ­¥ï¼Œæƒé‡å‚æ•°`w1.data`ã€`w2.data`å°±å¯ä»¥ç›´æ¥ä½¿ç”¨åœ¨è‡ªåŠ¨æ¢¯åº¦è¿‡ç¨‹ä¸­æ±‚å¾—çš„æ¢¯åº¦å€¼`w1.grad.data`ã€`w2.grad.data`ï¼Œå¹¶ç»“åˆ**å­¦ä¹ é€Ÿç‡**æ¥å¯¹ç°æœ‰çš„å‚æ•°è¿›è¡Œæ›´æ–°ã€ä¼˜åŒ–ã€‚
### æ‰“å°ç»“æœ
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20210422203036311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
### ç»“è®º
æŸå¤±å€¼ä¹Ÿåœ¨é€æ¸å‡å°ï¼Œè€Œä¸”æ¯”ä¸Šä¸ªç¨‹åºçš„ç»“æœæ›´å¥½ã€‚

# è‡ªå®šä¹‰ä¼ æ’­å‡½æ•°(torch.nn.Module)
### ç†è®ºåŸºç¡€
é™¤äº†å¯ä»¥é‡‡ç”¨è‡ªåŠ¨æ¢¯åº¦æ–¹æ³•ï¼Œè¿˜å¯ä»¥é€šè¿‡æ„å»ºä¸€ä¸ªç»§æ‰¿äº†`torch.nn.Module`çš„æ–°ç±»ï¼Œæ¥å®Œæˆå¯¹å‰å‘ä¼ æ’­å‡½æ•°å’Œåå‘ä¼ æ’­å‡½æ•°çš„è‡ªå®šä¹‰é‡å†™ã€‚é‡å†™`forward()`å³è¡¨ç¤ºé‡å†™å‰å‘ä¼ æ’­å‡½æ•°ï¼Œé‡å†™`backward()`å³è¡¨ç¤ºé‡å†™åå‘ä¼ æ’­å‡½æ•°ã€‚
### ä»£ç 
```python
# Step1ã€å¯¼åŒ…
import torch
from torch.autograd import Variable


# Step2ã€å®šä¹‰å˜é‡
batch_n = 100
hidden_layer = 100
input_data = 1000
output_data = 10


# Step3ã€é‡å†™ä¼ æ’­å‡½æ•°
class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, input, w1, w2):  # å®ç°æ¨¡å‹çš„å‰å‘ä¼ æ’­ä¸­çš„çŸ©é˜µè¿ç®—
        x = torch.mm(input, w1)
        x = torch.clamp(x, min=0)
        x = torch.mm(x, w2)
        return x

    def backward(self):  # å®ç°è‡ªåŠ¨æ¢¯åº¦è®¡ç®—ï¼ˆæ²¡æœ‰ç‰¹åˆ«éœ€æ±‚ï¼Œä¸€èˆ¬ä¸ç”¨è¿›è¡Œè°ƒæ•´ï¼‰
        pass


# Step4ã€å®šä¹‰ç»´åº¦ & åˆå§‹åŒ–æƒé‡ 
x = Variable(torch.randn(batch_n, input_data), requires_grad=False)  # è¾“å…¥å±‚ç»´åº¦
y = Variable(torch.randn(batch_n, output_data), requires_grad=False)  # è¾“å‡ºå±‚ç»´åº¦
w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad=True)  # ä»è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡å‚æ•°ç»´åº¦
w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad=True)  # ä»éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡å‚æ•°ç»´åº¦


# Step5ã€å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ
epoch_n = 20  
learning_rate = 1e-6 
model = Model()  # å®ä¾‹åŒ–ç±»å¯¹è±¡

for epoch in range(epoch_n):
	# Step5.1 æ­£å‘ä¼ æ’­
    y_pred = model(x, w1, w2)   # forward()ä¼šè‡ªåŠ¨è¢«æ‰§è¡Œ
    loss = (y_pred - y).pow(2).sum()
    print("Epoch: {}, Loss: {:.4f}".format(epoch, loss.data.item()))
    
    # Step5.2 åå‘ä¼ æ’­
    loss.backward()
	
	 # Step5.3 æƒé‡å‚æ•°æ›´æ–°
    w1.data -= learning_rate * w1.grad.data
    w2.data -= learning_rate * w2.grad.data
	
	# Step5.2 é‡ç½®æ¢¯åº¦å€¼ä¸º0
    w1.grad.data.zero_()
    w2.grad.data.zero_()
```
### æ‰“å°ç»“æœ
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20210422205912454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)

# æ¨¡å‹æ­å»º(torch.nn.Sequential)
### ç†è®ºåŸºç¡€
- PyTorchä¸­ä¸ä»…æœ‰èƒ½å®ç°è‡ªåŠ¨æ¢¯åº¦è®¡ç®—çš„ç±»ï¼Œå®ƒè¿˜å®šä¹‰äº†ç¥ç»ç½‘ç»œä¸­çº¿æ€§å˜æ¢ã€æ¿€æ´»å‡½æ•°ã€å·ç§¯å±‚ã€å…¨è¿æ¥å±‚ã€æ± åŒ–å±‚ç­‰å¸¸ç”¨ç¥ç»ç½‘ç»œç»“æ„çš„å®ç°ã€‚
- åœ¨å®Œæˆæ¨¡å‹çš„æ­å»ºä¹‹åï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨PyTorchæä¾›çš„ç±»å‹ä¸°å¯Œçš„**ä¼˜åŒ–å‡½æ•°**æ¥å®Œæˆå¯¹æ¨¡å‹å‚æ•°çš„ä¼˜åŒ–ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰å¾ˆå¤šé˜²æ­¢æ¨¡å‹åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿè¿‡æ‹Ÿåˆçš„ç±»ã€‚
- PyTorchä¸­çš„`torch.nn`åŒ…æä¾›äº†å¾ˆå¤šä¸å®ç°ç¥ç»ç½‘ç»œä¸­çš„å…·ä½“åŠŸèƒ½ç›¸å…³çš„ç±»ã€‚æ¯”å¦‚ï¼Œç¥ç»ç½‘ç»œä¸­çš„å·ç§¯å±‚ã€æ± åŒ–å±‚ã€å…¨è¿æ¥å±‚è¿™ç±»å±‚æ¬¡æ„é€ çš„æ–¹æ³•ï¼Œä»¥åŠé˜²æ­¢è¿‡æ‹Ÿåˆçš„å‚æ•°å½’ä¸€åŒ–æ–¹æ³•ã€Dropoutæ–¹æ³•ï¼Œè¿˜æœ‰æ¿€æ´»å‡½æ•°éƒ¨åˆ†çš„çº¿æ€§æ¿€æ´»å‡½æ•°ã€éçº¿æ€§æ¿€æ´»å‡½æ•°ç›¸å…³çš„æ–¹æ³•ï¼Œç­‰ç­‰ã€‚
### ä»£ç 
```python
import torch
from torch.autograd import Variable

batch_n = 100
hidden_layer = 100
input_data = 1000
output_data = 10

x = Variable(torch.randn(batch_n, input_data), requires_grad=False) 
y = Variable(torch.randn(batch_n, output_data), requires_grad=False) 
# ã€ä¸ºä»€ä¹ˆæ— éœ€å®šä¹‰w1ã€w2äº†ï¼Ÿä¸‹æœ‰è§£é‡Šã€‘


# æ­å»ºæ¨¡å‹ã€ä¸‹æœ‰è¯¦ç»†ä»‹ç»ã€‘
models = torch.nn.Sequential(
    torch.nn.Linear(input_data, hidden_layer),  # ä»è¾“å…¥å±‚åˆ°éšè—å±‚çš„çº¿æ€§å˜æ¢
    torch.nn.ReLU(),    						# æ¿€æ´»å‡½æ•°
    torch.nn.Linear(hidden_layer, output_data)  # ä»éšè—å±‚åˆ°è¾“å‡ºå±‚çš„çº¿æ€§å˜æ¢
)


# å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€å¯¹å‚æ•°è¿›è¡Œä¼˜åŒ–
epoch_n = 10000
learning_rate = 1e-4

loss_fn = torch.nn.MSELoss()    # ã€ä¸‹æœ‰è¯¦ç»†ä»‹ç»â€”â€”torch.nn.MSELoss()ã€‘

for epoch in range(epoch_n):
    y_pred = models(x)
    loss = loss_fn(y_pred, y)
    if epoch%1000 == 0:  # æ¯å®Œæˆ1000æ¬¡è®­ç»ƒï¼Œæ‰æ‰“å°ä¸€æ¬¡
        print("Epoch: {}, Loss: {:.4f}".format(epoch, loss.data.item()))
        
    models.zero_grad()

    loss.backward()

	# å‚æ•°æ¢¯åº¦æ›´æ–°ï¼šå¯¹models.parameters()è¿›è¡Œéå†ä»¥è®¿é—®æ¨¡å‹ä¸­çš„å…¨éƒ¨å‚æ•°ï¼Œç„¶åå¯¹å…¶æ›´æ–°
    for param in models.parameters():   
        param.data -= param.grad.data * learning_rate
```
### éƒ¨åˆ†ä»£ç è§£é‡Š
1. `torch.nn.Sequential`æ‹¬å·å†…çš„å†…å®¹å°±æ˜¯æˆ‘ä»¬æ­å»ºçš„ç¥ç»ç½‘ç»œæ¨¡å‹çš„å…·ä½“ç»“æ„ï¼Œè¿™é‡Œé¦–å…ˆé€šè¿‡`torch.nn.Linear(input_data, hidden_layer)`å®Œæˆä»è¾“å…¥å±‚åˆ°éšè—å±‚çš„çº¿æ€§å˜æ¢ï¼Œç„¶åç»è¿‡æ¿€æ´»å‡½æ•°å’Œ`torch.nn.Linear(hidden_layer, output_data)`å®Œæˆä»éšè—å±‚åˆ°è¾“å‡ºå±‚çš„çº¿æ€§å˜æ¢ã€‚

- **`torch.nn.Sequential`**
`torch.nn.Sequential`ç±»æ˜¯`torch.nn`ä¸­çš„ä¸€ç§åºåˆ—å®¹å™¨ï¼Œé€šè¿‡åœ¨å®¹å™¨ä¸­åµŒå¥—å„ç§å®ç°ç¥ç»ç½‘ç»œä¸­å…·ä½“åŠŸèƒ½ç›¸å…³çš„ç±»ï¼Œæ¥å®Œæˆå¯¹ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ­å»ºï¼Œæœ€ä¸»è¦çš„æ˜¯ï¼Œå‚æ•°ä¼šæŒ‰ç…§æˆ‘ä»¬å®šä¹‰å¥½çš„åºåˆ—è‡ªåŠ¨ä¼ é€’ä¸‹å»ã€‚æˆ‘ä»¬å¯ä»¥å°†åµŒå¥—åœ¨å®¹å™¨ä¸­çš„å„ä¸ªéƒ¨åˆ†çœ‹åšå„ç§ä¸åŒçš„æ¨¡å—ï¼Œè¿™äº›æ¨¡å—å¯ä»¥è‡ªç”±ç»„åˆã€‚æ¨¡å—çš„åŠ å…¥ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯åœ¨ä»¥ä¸Šä»£ç ä¸­ä½¿ç”¨çš„ç›´æ¥åµŒå¥—ï¼Œå¦ä¸€ç§æ˜¯ä»¥`orderdict`æœ‰åºå­—å…¸çš„æ–¹å¼è¿›è¡Œä¼ å…¥ã€‚åŒºåˆ«æ˜¯ï¼Œä½¿ç”¨åè€…æ­å»ºçš„æ¨¡å‹çš„æ¯ä¸ªæ¨¡å—éƒ½æœ‰æˆ‘ä»¬è‡ªå®šä¹‰çš„åå­—ï¼Œè€Œå‰è€…é»˜è®¤ä½¿ç”¨ä»é›¶å¼€å§‹çš„æ•°å­—åºåˆ—ä½œä¸ºæ¯ä¸ªæ¨¡å—çš„åå­—ã€‚

- **`torch.nn.Linear`**
`torch.nn.Linear`ç±»ç”¨äºå®šä¹‰æ¨¡å‹çš„çº¿æ€§å±‚ï¼Œå³å®Œæˆå‰é¢æåˆ°çš„ä¸åŒçš„å±‚ä¹‹é—´çš„çº¿æ€§å˜æ¢ã€‚`torch.nn.Linear`ç±»æ¥æ”¶çš„å‚æ•°æœ‰ä¸‰ä¸ªï¼Œåˆ†åˆ«æ˜¯è¾“å…¥ç‰¹å¾æ•°ã€è¾“å‡ºç‰¹å¾æ•°å’Œæ˜¯å¦ä½¿ç”¨åç½®ï¼ˆé»˜è®¤ä¸ºTrueï¼‰ã€‚åœ¨å®é™…ä½¿ç”¨çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€å°†è¾“å…¥çš„ç‰¹å¾æ•°å’Œè¾“å‡ºçš„ç‰¹å¾æ•°ä¼ é€’ç»™`torch.nn.Linear`ç±»ï¼Œå°±ä¼š**è‡ªåŠ¨ç”Ÿæˆå¯¹åº”ç»´åº¦çš„æƒé‡å‚æ•°å’Œåç½®**ã€‚= => æ ¹æ®æˆ‘ä»¬æ­å»ºæ¨¡å‹çš„è¾“å…¥ã€è¾“å‡ºå’Œå±‚æ¬¡ç»“æ„çš„éœ€æ±‚ï¼Œå®ƒçš„è¾“å…¥æ˜¯åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸­åŒ…å«100ä¸ªç‰¹å¾æ•°ä¸º1000çš„æ•°æ®ï¼Œæœ€åå¾—åˆ°100ä¸ªç‰¹å¾æ•°ä¸º10çš„æ•°æ®ï¼Œä¸­é—´éœ€è¦ç»è¿‡ä¸¤æ¬¡çº¿æ€§å˜æ¢ï¼š1ï¼‰x(100, 1000) * w1(1000, 100) ==> h1(100, 100)ï¼Œ2ï¼‰h1(100, 100) * w2(100, 10) ==> y_pred(100, 10)ã€‚æ‰€ä»¥è¦ä½¿ç”¨ä¸¤ä¸ªçº¿æ€§å±‚ã€‚è¿™ä»£æ›¿äº†ä¹‹å‰ä½¿ç”¨çŸ©é˜µä¹˜æ³•æ–¹å¼çš„å®ç°ã€‚

- **`torch.nn.ReLU`**
`torch.nn.ReLU`ç±»å±äºéçº¿æ€§æ¿€æ´»åˆ†ç±»ï¼Œåœ¨å®šä¹‰æ—¶é»˜è®¤ä¸éœ€è¦ä¼ å…¥å‚æ•°ã€‚

2. è®¡ç®—æŸå¤±å‡½æ•°çš„ä»£ç å‘ç”Ÿäº†å˜åŒ–ï¼Œç°åœ¨ä½¿ç”¨çš„æ˜¯`torch.nn`åŒ…ä¸­å·²ç»å®šä¹‰å¥½çš„å‡æ–¹è¯¯å·®å‡½æ•°ç±»`torch.nn.MSELoss`æ¥è®¡ç®—æŸå¤±å€¼ã€‚
- **`torch.nn.MSELoss`**
`torch.nn.MSELoss`ç±»ä½¿ç”¨**å‡æ–¹è¯¯å·®å‡½æ•°**æ¥è®¡ç®—æŸå¤±å€¼ã€‚åœ¨å®šä¹‰ç±»çš„å¯¹è±¡æ—¶ä¸ç”¨ä¼ å…¥ä»»ä½•å‚æ•°ï¼Œä½†åœ¨ä½¿ç”¨å®ä¾‹æ—¶éœ€è¦è¾“å…¥ä¸¤ä¸ªç»´åº¦ä¸€æ ·çš„å‚æ•°æ–¹å¯è¿›è¡Œè®¡ç®—ã€‚

- **`torch.nn.L1Loss`**
`torch.nn.L1Loss`ç±»ä½¿ç”¨**å¹³å‡ç»å¯¹è¯¯å·®å‡½æ•°**æ¥è®¡ç®—æŸå¤±å€¼ã€‚

- **`torch.nn.CrossEntropyLoss`**
`torch.nn.CrossEntropyLoss`ç±»ä½¿ç”¨**äº¤å‰ç†µ**æ¥è®¡ç®—æŸå¤±å€¼ã€‚

3. ç”±1ã€2çŸ¥ï¼Œæ­¤ç¨‹åºå°±å·²ç»ä¸éœ€è¦è‡ªå·±åšçŸ©é˜µä¹˜æ³•æ¥è¿›è¡Œçº¿æ€§å˜æ¢ï¼ˆå®é™…ä¸Šå°±æ˜¯ç”¨çŸ©é˜µä¹˜æ³•åšé¢„æµ‹ï¼‰äº†ï¼Œå› ä¸ºè¿™äº›è½¬æ¢å·¥ä½œå·²ç»äº¤ç»™æ­å»ºå¥½çš„æ¨¡å‹æ¥å®Œæˆäº†ã€‚
### è¿è¡Œç»“æœ
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20210424115957198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
### ç»“è®º
å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨è¯¥æ¨¡å‹å·²ç»å¯ä»¥ä½¿è¯¯å·®å¾ˆå°äº†ã€‚
# å‚æ•°ä¼˜åŒ–(torch.optim)
### ç†è®ºåŸºç¡€
- åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä»£ç ä¸­çš„ç¥ç»ç½‘ç»œæƒé‡çš„å‚æ•°ä¼˜åŒ–å’Œæ›´æ–°è¿˜æ²¡æœ‰å®ç°è‡ªåŠ¨åŒ–ï¼Œå¹¶ä¸”ç›®å‰ä½¿ç”¨çš„ä¼˜åŒ–æ–¹æ³•éƒ½æœ‰å›ºå®šçš„å­¦ä¹ é€Ÿç‡ï¼Œæ‰€ä»¥ä¼˜åŒ–å‡½æ•°ç›¸å¯¹ç®€å•ã€‚
- `torch.optim`åŒ…ä¸­æä¾›äº†éå¸¸å¤šçš„å®ç°**å‚æ•°è‡ªåŠ¨ä¼˜åŒ–**çš„ç±»ï¼Œæ¯”å¦‚SGDã€AdaGradã€RMSPropã€Adamç­‰ã€‚

### ä»£ç 
```python
import torch
from torch.autograd import Variable
batch_n = 100
hidden_layer = 1000
input_data = 1000
output_data = 10

x = Variable(torch.randn(batch_n, input_data), requires_grad=False)
y = Variable(torch.randn(batch_n, output_data), requires_grad=False)

models = torch.nn.Sequential(
    torch.nn.Linear(input_data, hidden_layer),
    torch.nn.ReLU(),
    torch.nn.Linear(hidden_layer, output_data)
)


epoch_n = 100
learning_rate = 1e-4
loss_fn = torch.nn.MSELoss()

optimzer = torch.optim.Adam(models.parameters(), lr = learning_rate)  # ã€åæœ‰è¯¦ç»†ä»‹ç»ã€‘

for epoch in range(epoch_n):
    y_pred = models(x)
    loss = loss_fn(y_pred, y)
    if epoch % 5 == 0:
        print("Epoc {}, Loss: {:.4f}".format(epoch, loss.data.item()))

    optimzer.zero_grad()  # å¯¹æ¨¡å‹å‚æ•°æ¢¯åº¦å½’é›¶

    loss.backward()  	  # åå‘ä¼ æ’­

    optimzer.step()  	  # ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦å€¼å¯¹å„ä¸ªç»“ç‚¹çš„å‚æ•°è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼ˆè¿™æ˜¯æœ¬èŠ‚è¦å­¦ä¹ çš„æ–°è½®å­ï¼Œåˆå‡è½»äº†å‰é¢éƒ½æ˜¯ä¸“é—¨å†™ä»£ç æ¥æ›´æ–°çš„å·¥ä½œï¼‰
```
### éƒ¨åˆ†ä»£ç è§£é‡Š
ä½¿ç”¨ä¼˜åŒ–å‡½æ•°å¯¹å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼š`torch.optim.Adam(éœ€è¦ä¼˜åŒ–çš„å‚æ•°åˆ—è¡¨ï¼Œå­¦ä¹ é€Ÿç‡çš„åˆå§‹å€¼[é»˜è®¤ä¸º0.001]ï¼ŒåŠ¨é‡)`ã€‚Adamä¼˜åŒ–å‡½æ•°çš„ä¸€ä¸ªå¼ºå¤§ä¹‹å¤„ï¼šå¯¹æ¢¯åº¦æ›´æ–°ä½¿ç”¨åˆ°çš„å­¦ä¹ é€Ÿç‡è¿›è¡Œè‡ªé€‚åº”è°ƒèŠ‚ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦ä¼˜åŒ–çš„æ˜¯æ¨¡å‹ä¸­çš„å…¨éƒ¨å‚æ•°ï¼Œæ‰€ä»¥ä¼ é€’ç»™`torch.optim.Adam`ç±»çš„å‚æ•°æ˜¯`models.parameters`ã€‚
### è¿è¡Œç»“æœ
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2021042412071426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
# å®æˆ˜æ‰‹å†™æ•°å­—è¯†åˆ«
### ç†è®ºåŸºç¡€
æ¯æ¡æ•°æ®çš„ç‰¹å¾éƒ½æ˜¯28*28ä¸ª[0,1]ä¹‹é—´çš„å®æ•°ï¼Œè¡¨ç¤ºç°åº¦å›¾åƒï¼›æ ‡ç­¾æ˜¯{0,1,...,9}è¿™10ä¸ªæ•°å­—ä¸­çš„ä¸€ä¸ªã€‚

å…·ä½“è¿‡ç¨‹ï¼š
1) ä½¿ç”¨è®­ç»ƒæ•°æ®å¯¹æ­å»ºå¥½çš„ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œè®­ç»ƒå¹¶å®Œæˆå‚æ•°ä¼˜åŒ–ã€‚
2) ä½¿ç”¨ä¼˜åŒ–å¥½çš„æ¨¡å‹å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¯¹æ¯”é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„æŸå¤±å€¼ï¼ŒåŒæ—¶è®¡ç®—å‡ºç»“æœé¢„æµ‹çš„å‡†ç¡®ç‡ã€‚

### ä»£ç 1
```python
import torch
import torchvision
import matplotlib.pyplot as plt
from torchvision import datasets, transforms  # ç”¨äºä¸‹è½½æ•°æ®é›†ã€æ•°æ®å˜æ¢
from torchvision.transforms import ToTensor


# Step1. æ•°æ®é›†ä¸‹è½½
transform = transforms.Compose([
    transforms.ToTensor(), 
    transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])
])  # ã€åæœ‰è¯¦ç»†ä»‹ç»ã€‘

data_train = datasets.MNIST(
    root="./dataMNIST/",  # æŒ‡å®šæ•°æ®é›†ä¸‹è½½ä¹‹åçš„å­˜æ”¾è·¯å¾„
    transform=transform,  # å¯¼å…¥æ•°æ®é›†æ—¶éœ€è¦å¯¹æ•°æ®è¿›è¡Œå“ªç§å˜æ¢æ“ä½œ ã€åæœ‰è¯¦ç»†ä»‹ç»ã€‘
    train=True, 		  # æŒ‡æ˜æ•°æ®é›†ä¸‹è½½å®Œæˆåéœ€è¦è½½å…¥å“ªéƒ¨åˆ†æ•°æ®(True-->è®­ç»ƒé›†, False-->æµ‹è¯•é›†)
    download=True
)

data_test = datasets.MNIST(
    root='./dataMNIST/',
    train=False,
    transform=transform
)


# Step2. æ•°æ®è£…è½½ ã€åæœ‰è¯¦ç»†ä»‹ç»ã€‘
data_loader_train = torch.utils.data.DataLoader(dataset=data_train, batch_size=64, shuffle=True)
data_loader_test = torch.utils.data.DataLoader(dataset=data_test, batch_size=64, shuffle=True)


# Step3. é€‰æ‹©ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œé¢„è§ˆ
# 3.1 ä½¿ç”¨nextã€iteræ¥è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡æ•°æ®å’Œå…¶å¯¹åº”çš„å›¾ç‰‡æ ‡ç­¾
images, labels = next(iter(data_loader_train))
# print(f'image.size() = {images.size()}')    # ç‰¹å¾å¼ é‡çš„å¤§å°æ˜¯(64, 1, 28, 28)
# print(f'labels.size() = {labels.size()}')   # æ ‡ç­¾å¼ é‡çš„å¤§å°æ˜¯(64, 1)

# æ˜¾å¼ç¬¬0æ¡è®­ç»ƒæ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ï¼š
# plt.imshow(images[0, 0], cmap='gray')
# plt.title(f'label = {labels[0]}')
# plt.show()

'''
3.2 å°†ä¸€ä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡æ„é€ æˆç½‘æ ¼æ¨¡å¼ï¼Œå‚æ•°å³ä¸ºä¸€ä¸ªæ‰¹æ¬¡æ‰€è£…è½½çš„æ•°æ®.
    è§£é‡Šï¼šæ¯ä¸ªæ‰¹æ¬¡çš„è£…è½½æ•°æ®éƒ½æ˜¯4ç»´çš„ï¼Œç»´åº¦çš„æ„æˆä»å‰å¾€ååˆ†åˆ«ä¸ºbatch_sizeã€channelã€heigthã€weight,
         åˆ†åˆ«å¯¹åº”è¿™ä¸ªæ‰¹æ¬¡ä¸­çš„æ•°æ®ä¸ªæ•°ã€æ¯å¼ å›¾ç‰‡çš„è‰²å½©é€šé“æ•°ã€æ¯å¼ å›¾ç‰‡çš„é«˜åº¦å’Œå®½åº¦ã€‚
         åœ¨é€šè¿‡torchvision.utils.make_gridä¹‹åï¼Œå›¾ç‰‡çš„ç»´åº¦å˜æˆäº†(channel, height, weight),
         å› ä¸ºè¿™ä¸ªæ‰¹æ¬¡çš„æ•°æ®å…¨éƒ¨è¢«æ•´åˆåˆ°äº†ä¸€èµ·ï¼ˆè‰²å½©é€šé“æ•°ä¸å˜ï¼‰ï¼Œæ‰€ä»¥æ²¡æœ‰batch_sizeäº†ã€‚
'''
img = torchvision.utils.make_grid(images)
# 3.3 ä¸Šé¢å¾—åˆ°çš„imgçš„ç»´åº¦æ˜¯(channel, height, weight)ï¼Œè¿™é‡Œå°†å…¶è½¬æ¢ä¸º(height, weight, channel)ï¼Œè¿™æ ·æ‰èƒ½æ­£ç¡®è°ƒç”¨imshow().
img = img.numpy().transpose(1, 2, 0)
# 3.4 å±•ç¤º
std = [0.5, 0.5, 0.5]
mean = [0.5, 0.5, 0.5]
img = img * std + mean
print([labels[i] for i in range(64)])  # æ‰“å°æ‰¹æ¬¡ä¸­å„æ•°æ®çš„å…¨éƒ¨æ ‡ç­¾
plt.imshow(img)
plt.show()
```
### éƒ¨åˆ†ä»£ç è§£é‡Š1(torchvision.transformsã€DataLoader)
- `torchvision.transforms`ä¸­æä¾›äº†ä¸°å¯Œçš„ç±»å¯¹è½½å…¥çš„æ•°æ®è¿›è¡Œå˜æ¢ã€‚æ¯”å¦‚ï¼Œå¯¹äºå›¾ç‰‡ç±»å‹çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¾—æƒ³åŠæ³•æŠŠå®ƒå˜æˆTensoræ•°æ®ç±»å‹çš„å˜é‡ï¼Œæ‰èƒ½å¯¹å…¶è¿›è¡Œè®¡ç®—ã€‚å¦å¤–ï¼Œå¦‚æœè·å–çš„æ•°æ®æ˜¯æ ¼å¼æˆ–è€…å¤§å°ä¸ä¸€çš„å›¾ç‰‡ï¼Œåˆ™è¿˜éœ€è¦è¿›è¡Œå½’ä¸€åŒ–å’Œå¤§å°ç¼©æ”¾ç­‰æ“ä½œã€‚
- `torchvision.transforms`ä¸­æœ‰å¤§é‡çš„æ•°æ®å˜æ¢ç±»ï¼Œå…¶ä¸­æœ‰å¾ˆå¤§ä¸€éƒ¨åˆ†å¯ä»¥ç”¨äºå®ç°**æ•°æ®å¢å¼ºï¼ˆData Argumentationï¼‰**ã€‚è‹¥åœ¨æˆ‘ä»¬éœ€è¦è§£å†³çš„é—®é¢˜ä¸Šèƒ½å¤Ÿå‚ä¸åˆ°æ¨¡å‹è®­ç»ƒä¸­çš„å›¾ç‰‡æ•°æ®éå¸¸æœ‰é™ï¼Œåˆ™å°±å¯ä»¥é€šè¿‡å¯¹æœ‰é™çš„å›¾ç‰‡æ•°æ®è¿›è¡Œå„ç§å˜æ¢ï¼Œæ¥ç”Ÿæˆæ–°çš„è®­ç»ƒé›†ï¼Œè¿™äº›å˜æ¢å¯ä»¥æ˜¯ç¼©å°æˆ–è€…æ”¾å¤§å›¾ç‰‡çš„å¤§å°ã€å¯¹å›¾ç‰‡è¿›è¡Œæ°´å¹³æˆ–è€…å‚ç›´ç¿»è½¬ç­‰ï¼Œéƒ½æ˜¯æ•°æ®å¢å¼ºçš„æ–¹æ³•ã€‚
- `transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])]) `ç”¨æ¥å¯¹æ•°æ®è¿›è¡Œå˜æ¢ï¼šå¯ä»¥å°†`torchvision.transforms.Compose`ç±»çœ‹åšä¸€ç§å®¹å™¨ï¼Œå®ƒèƒ½å¤ŸåŒæ—¶å¯¹å¤šç§æ•°æ®å˜æ¢è¿›è¡Œç»„åˆã€‚ä¼ å…¥çš„å‚æ•°æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„å…ƒç´ æ˜¯å¯¹è½½å…¥çš„æ•°æ®è¿›è¡Œçš„å„ç§å˜æ¢æ“ä½œã€‚æ­¤å¤„ï¼Œ`transforms.ToTensor()`æ˜¯ä¸€ä¸ª*ç±»å‹å˜æ¢*ï¼›` transforms.Normalize`æ˜¯ä¸€ä¸ª*æ•°æ®æ ‡å‡†åŒ–å˜æ¢*ï¼ˆæ ‡å‡†å·®å˜æ¢æ³•ï¼‰ï¼Œéœ€ä½¿ç”¨åŸå§‹æ•°æ®çš„å‡å€¼(Mean)å’Œæ ‡å‡†å·®(Standard Deviation)æ¥è¿›è¡Œæ•°æ®çš„æ ‡å‡†åŒ–ï¼Œåœ¨ç»è¿‡æ ‡å‡†å˜æ¢ä¹‹åï¼Œæ•°æ®å…¨éƒ¨ç¬¦åˆå‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º1çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚ä¸è¿‡ä½œè€…å·æ‡’äº†ï¼Œå‡å€¼å’Œæ ‡å‡†å·®ä¸æ˜¯æ¥è‡ªåŸå§‹æ•°æ®ï¼Œè€Œæ˜¯è‡ªå®šä¹‰çš„ã€‚
- `torchvision.transforms`ä¸­çš„å¸¸ç”¨æ•°æ®å˜æ¢æ“ä½œï¼š
1. `torchvision.transforms.Resize`
2. `torchvision.transforms.Scale`
3. `torchvision.transforms.CenterCrop`
4. `torchvision.transforms.RandomCrop`
5. `torchvision.transforms.RandomHorizontalFlip`
6. `torchvision.transforms.RandomVerticalFlip`
7. `torchvision.transforms.ToTensor()`
8. `torchvision.transforms.ToPILImage`

-- --
- æ•°æ®è£…è½½ï¼šç”±äºæ•°æ®é›†é‡Œæœ‰ä¸Šä¸‡æ¡æ•°æ®ï¼Œæ‰€ä»¥å¾€å¾€è¦*åˆ†æ‰¹* ä»æ•°æ®é›†ä¸­è¯»å‡ºæ•°æ®ï¼›ä¸”è£…è½½å¥½äº†çš„æ•°æ®å°±å¯ä»¥è¿›è¡Œéå†äº†ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ¯æ¬¡è¯»1ä¸ªæ•°æ®æ¡ç›®ï¼Œæˆ–æ˜¯æ¯æ¬¡è¯»64ä¸ªæ•°æ®æ¡ç›®ã€‚å¯¹æ•°æ®çš„è£…è½½ä½¿ç”¨çš„æ˜¯`torch.utils.data.DataLoader`ç±»ï¼Œç±»ä¸­çš„`dataseet`å‚æ•°è¡¨ç¤ºè½½å…¥çš„æ•°æ®é›†åç§°ï¼Œ`batch_size`å‚æ•°è¡¨ç¤ºæ¯æ‰¹æ¬¡è¯»å‡ æ¡æ•°æ®ï¼ˆé»˜è®¤ä¸º1ï¼‰ï¼Œ`shuffle`å‚æ•°è¡¨ç¤ºæ˜¯å¦åœ¨è£…è½½çš„è¿‡ç¨‹ä¸­æ‰“ä¹±å›¾ç‰‡çš„é¡ºåºï¼ˆé»˜è®¤ä¸ºFalseï¼‰.
### è¿è¡Œç»“æœ1
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20210424123710323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20210424123726519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)

### ä»£ç 2
```python
import torch
from torch.autograd import Variable
from book_ex6 import data_loader_train, data_loader_test, data_train, data_test
import torchvision
import matplotlib.pyplot as plt


# Step4ã€æ¨¡å‹æ­å»º ã€åæœ‰è¯¦ç»†ä»‹ç»ã€‘
class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = torch.nn.Sequential(
            torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # å·ç§¯å±‚
            torch.nn.ReLU(),                                             # æ¿€æ´»å±‚
            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),# å·ç§¯å±‚
            torch.nn.ReLU(),                                             # æ¿€æ´»å±‚
            torch.nn.MaxPool2d(stride=2, kernel_size=2)                  # æœ€å¤§æ± åŒ–å±‚
        )
        self.dense = torch.nn.Sequential(      # å®šä¹‰å…¨è¿æ¥å±‚
            torch.nn.Linear(14*14*128, 1024),
            torch.nn.ReLU(),
            torch.nn.Dropout(p=0.5),           # é˜²æ­¢å·ç§¯ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­å‘ç”Ÿè¿‡æ‹Ÿåˆ
            torch.nn.Linear(1024, 10)
        )

    def forward(self, x):           # å‰å‘ä¼ æ’­
        x = self.conv1(x)           # è¿›è¡Œå·ç§¯å¤„ç†
        x = x.view(-1, 14*14*128)   # å¯¹å‚æ•°å®ç°æ‰å¹³åŒ–(å¦‚æœä¸è¿›è¡Œæ‰å¹³åŒ–ï¼Œåˆ™ä¹‹åçš„å…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»æ—¶ä¼šä½¿å¾—å®é™…è¾“å‡ºçš„å‚æ•°ç»´åº¦å’Œå…¶å®šä¹‰è¾“å…¥çš„ç»´åº¦ä¸åŒ¹é…)
        x = self.dense(x)           # è°ƒç”¨å…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»
        return x


# Step5. è®­ç»ƒæ¨¡å‹ã€ä¼˜åŒ–å‚æ•°
model = Model()
# print(model)  # æŸ¥çœ‹æ­å»ºå¥½çš„æ¨¡å‹çš„å®Œæ•´ç»“æ„

cost = torch.nn.CrossEntropyLoss()  # è®¡ç®—æŸå¤±å€¼çš„æŸå¤±å‡½æ•°ä½¿ç”¨çš„æ˜¯äº¤å‰ç†µ
optimizer = torch.optim.Adam(model.parameters())  # ä¼˜åŒ–å‡½æ•°ä½¿ç”¨çš„æ˜¯Adamè‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•

n_epochs = 5
for epoch in range(n_epochs):
    running_loss = 0.0   # è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ¯æ¬¡çš„æŸå¤±å€¼ä¹‹å’Œ
    running_correct = 0  # è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¾—åˆ°çš„é¢„æµ‹å€¼ä¸å®é™…å€¼ç›¸åŒçš„æ¬¡æ•°
    print("Epoch {}/{}".format(epoch+1, n_epochs))
    print("-" * 10)

    for data in data_loader_train:
        X_train, y_train = data    # X_train, y_trainåˆ†åˆ«è¡¨ç¤ºè®­ç»ƒé›†çš„æ•°æ®ä»¥åŠå…¶æ‰€å¯¹åº”çš„åˆ†ç±»
        X_train, y_train = Variable(X_train), Variable(y_train)
        
        outputs = model(X_train)   # æ‰§è¡Œforwad()å‡½æ•°â€”â€”æ­£å‘ä¼ æ’­
        _, pred = torch.max(outputs.data, 1)
        optimizer.zero_grad()  		   # æ¸…ç©ºä¼˜åŒ–å™¨
        loss = cost(outputs, y_train)  # è®¡ç®—æŸå¤±å€¼
        loss.backward()   			   # åå‘ä¼ æ’­
        optimizer.step()  			   # ä¼˜åŒ–ï¼ˆæƒé‡æ›´æ–°ï¼‰
        
        running_loss += loss.data.item()
        running_correct += torch.sum(pred == y_train.data)

    testing_correct = 0
    for data in data_loader_test:
        X_test, y_test = data
        X_test, y_test = Variable(X_test), Variable(y_test)
        outputs = model(X_test)
        _, pred = torch.max(outputs, 1)
        testing_correct += torch.sum(pred == y_test.data)

    print("Loss is {:.4f}, Train Accuracy is {:.4f}%, Test Accuracy is {:.4f}".format(running_loss/len(data_train), 100*running_correct/len(data_train), 100*testing_correct/len(data_test)))


# Step6. éªŒè¯ï¼šéšæœºé€‰å–ä¸€éƒ¨åˆ†æµ‹è¯•é›†ä¸­çš„å›¾ç‰‡ï¼Œç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å¯¹ç»“æœè¿›è¡Œå¯è§†åŒ–
data_loader_test = torch.utils.data.DataLoader(dataset=data_test, batch_size=4, shuffle=True)
X_test, y_test = next(iter(data_loader_test))
inputs = Variable(X_test)
pred = model(inputs)
_, pred = torch.max(pred, 1)

print("Predict Label is: ", [i for i in pred.data])
print("Real Label is: ", [i for i in y_test])

img = torchvision.utils.make_grid(X_test)
img = img.numpy().transpose(1, 2, 0)

std = [0.5, 0.5, 0.5]
mean = [0.5, 0.5, 0.5]
img = img * std + mean
plt.imshow(img)
plt.show()
```

### éƒ¨åˆ†ä»£ç è§£é‡Š2
Step4å¼€å§‹ç¼–å†™å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ­å»ºå’Œå‚æ•°ä¼˜åŒ–çš„ä»£ç ã€‚å·ç§¯å±‚ä½¿ç”¨`torch.nn.Conv2d`ç±»æ–¹æ³•æ¥æ­å»ºï¼›æ¿€æ´»å±‚ä½¿ç”¨`torch.nn.ReLU`ç±»æ–¹æ³•æ¥æ­å»ºï¼›æ± åŒ–å±‚ç”¨`torch.nn.MaxPool2d`ç±»æ–¹æ³•æ¥æ­å»ºï¼›å…¨è¿æ¥å±‚ä½¿ç”¨`torch.nn.Linear`ç±»æ–¹æ³•æ¥æ­å»ºã€‚
### è¿è¡Œç»“æœ2
> æ•°æ®é‡å¤ªå¤§ï¼Œæˆ‘çš„ç”µè„‘å¥½åƒè¿è¡Œä¸å‡ºæ¥ã€‚ã€‚ã€‚

# è‡ªåŠ¨ç¼–ç å™¨
### ç†è®ºåŸºç¡€
è‡ªåŠ¨ç¼–ç å™¨ï¼ˆAutoEncoderï¼‰æ˜¯ä¸€ç§å¯ä»¥è¿›è¡Œæ— ç›‘ç£å­¦ä¹ çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œä¸€ä¸ªå®Œæ•´çš„è‡ªåŠ¨ç¼–ç å™¨ä¸»è¦ç”±ä¸¤éƒ¨åˆ†ç»„æˆâ€”â€”ç”¨äºæ ¸å¿ƒç‰¹å¾æå–çš„ç¼–ç éƒ¨åˆ†å’Œå¯ä»¥å®ç°æ•°æ®é‡æ„çš„è§£ç éƒ¨åˆ†ã€‚

**ç¼–ç å™¨**ä¸»è¦è´Ÿè´£*å¯¹åŸå§‹çš„è¾“å…¥æ•°æ®è¿›è¡Œå‹ç¼©å¹¶æå–å‡ºæ•°æ®ä¸­çš„æ ¸å¿ƒç‰¹å¾*ï¼Œè€Œ**è§£ç å™¨**ä¸»è¦æ˜¯*å¯¹åœ¨ç¼–ç å™¨ä¸­æå–çš„æ ¸å¿ƒç‰¹å¾è¿›è¡Œå±•å¼€å¹¶é‡æ–°æ„é€ å‡ºä¹‹å‰çš„æ•°æ®*ã€‚

è‡ªåŠ¨ç¼–ç å™¨è¿™ç§å…ˆç¼–ç ã€åè§£ç çš„ç¥ç»ç½‘ç»œæ¨¡å‹çš„æœ€å¤§ç”¨é€”æ˜¯ï¼šå®ç°è¾“å…¥æ•°æ®çš„æ¸…æ´—ï¼Œæ¯”å¦‚å»é™¤è¾“å…¥æ•°æ®ä¸­çš„å™ªå£°æ•°æ®ã€å¯¹è¾“å…¥æ•°æ®çš„æŸäº›å…³é”®ç‰¹å¾è¿›è¡Œå¢å¼ºæˆ–æ”¾å¤§ã€‚å†æ¯”å¦‚ï¼Œè¿˜å¯ä»¥å¯¹æœ‰é©¬èµ›å…‹çš„å›¾ç‰‡è¿›è¡Œé™¤ç å¤„ç†ã€‚ç­‰ç­‰ã€‚

### ä»£ç 1
é€šè¿‡çº¿æ€§å˜æ¢å®ç°è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹ã€‚
```python
import torch
import torchvision
from torchvision import datasets, transforms
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt

# 1. ä¸‹è½½æ•°æ®
transform = transforms.Compose([transforms.ToTensor()])
dataset_train = datasets.MNIST(
    root="./dataMNIST/",
    transform=transform,
    train=True,
    download=True
)
dataset_test = datasets.MNIST(
    root="./dataMNIST/",
    transform=transform,
    train=False
)

# 2. è£…è½½æ•°æ®
train_load = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=4, shuffle=True)
test_load = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=4, shuffle=True)

# 3. å–å‡ºä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–
images, label = next(iter(train_load))
print(images.shape)
images_example = torchvision.utils.make_grid(images)
images_example = images_example.numpy().transpose(1, 2, 0)
mean = [0.5, 0.5, 0.5]
std = [0.5, 0.5, 0.5]
images_example = images_example * std + mean
plt.imshow(images_example)
plt.show()
noisy_images = images_example + 0.5*np.random.randn(*images_example.shape)
noisy_images = np.clip(noisy_images, 0., 1.)
plt.imshow(noisy_images)
plt.show()

# 4. æ­å»ºæ¨¡å‹ã€è®­ç»ƒæ¨¡å‹
class AutoEncoder(torch.nn.Module):
    def __init__(self):
        super(AutoEncoder, self).__init__()
        '''
            ç¼–ç ï¼š
            	è¾“å…¥æ•°æ®çš„æ•°æ®é‡ä»224ä¸ªåˆ°128ä¸ªå†åˆ°64ä¸ªæœ€ååˆ°32ä¸ªçš„å‹ç¼©è¿‡ç¨‹ï¼Œ
            	è¿™32ä¸ªæ•°æ®å°±æ˜¯æˆ‘ä»¬æå–åˆ°çš„æ ¸å¿ƒç‰¹å¾ã€‚
        '''
        self.encoder = torch.nn.Sequential(
            torch.nn.Linear(28*28, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU()
        )
        
        # è§£ç 
        self.decoder = torch.nn.Sequential(
            torch.nn.Linear(32, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 28*28)
        )

    def forward(self, input):
        output = self.encoder(input)
        output = self.decoder(output)
        return output

model = AutoEncoder()
print(model)

optimizer = torch.optim.Adam(model.parameters())
loss_f = torch.nn.MSELoss()  # å‡æ–¹è¯¯å·®

epoch_n = 10
for epoch in range(epoch_n):
    running_loss = 0.0

    print("Epoch {}/{}".format(epoch, epoch_n))
    print("-"*10)

    for data in train_load:  # å¾ªç¯æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        # å¯¹å›¾ç‰‡è¿›è¡Œæ‰“ç å¤„ç†å¹¶è£å‰ªåˆ°æŒ‡å®šçš„åƒç´ å€¼èŒƒå›´å†…
        X_train, _ = data
        noisy_X_train = X_train + 0.5*torch.randn(X_train.shape)
        noisy_X_train = torch.clamp(noisy_X_train, 0., 1.)
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œè¾“å‡ºä¸€ä¸ªé¢„æµ‹å›¾ç‰‡ï¼Œç”¨è¯¥é¢„æµ‹å›¾ç‰‡ä¸åŸå§‹å›¾ç‰‡è¿›è¡ŒæŸå¤±å€¼è®¡ç®—
        X_train, noisy_X_train = Variable(X_train.view(-1, 28*28)), Variable(noisy_X_train.view(-1, 28*28))
        train_pre = model(noisy_X_train)
        loss = loss_f(train_pre, X_train)

        optimizer.zero_grad()
        loss.backward()  # é€šè¿‡æŸå¤±å€¼å¯¹æ¨¡å‹è¿›è¡Œåå‘ä¼ æ’­ï¼Œæœ€åå°±èƒ½å¾—åˆ°å»é™¤å›¾ç‰‡é©¬èµ›å…‹æ•ˆæœçš„æ¨¡å‹äº†
        optimizer.step()

        running_loss += loss.data.item()

    print("Loss is:{:.4f}".format(running_loss/len(dataset_train)))
```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20210426100019939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)

### ä»£ç 2
é€šè¿‡å·ç§¯å˜æ¢å®ç°ã€‚
```python
import torch
import torchvision
from torchvision import datasets, transforms
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt

# 1. ä¸‹è½½æ•°æ®
transform = transforms.Compose([transforms.ToTensor()])
dataset_train = datasets.MNIST(
    root="./dataMNIST/",
    transform=transform,
    train=True,
    download=True
)
dataset_test = datasets.MNIST(
    root="./dataMNIST/",
    transform=transform,
    train=False
)

# 2. è£…è½½æ•°æ®
train_load = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=64, shuffle=True)
test_load = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=64, shuffle=True)

# 3. å–å‡ºä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–
images, label = next(iter(train_load))
print(images.shape)
images_example = torchvision.utils.make_grid(images)
images_example = images_example.numpy().transpose(1, 2, 0)
mean = [0.5, 0.5, 0.5]
std = [0.5, 0.5, 0.5]
images_example = images_example * std + mean
plt.imshow(images_example)
plt.show()
noisy_images = images_example + 0.5*np.random.randn(*images_example.shape)
noisy_images = np.clip(noisy_images, 0., 1.)
plt.imshow(noisy_images)
plt.show()

# 4. æ­å»ºæ¨¡å‹ã€è®­ç»ƒæ¨¡å‹
class AutoEncoder(torch.nn.Module):
    def __init__(self):
        super(AutoEncoder, self).__init__()
        # ç¼–ç 
        self.encoder = torch.nn.Sequential(
            torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=2, stride=2),
            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=2, stride=2)
        )
        # è§£ç 
        self.decoder = torch.nn.Sequential(
            torch.nn.Upsample(scale_factor=2, mode="nearest"),
            torch.nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),
            torch.nn.ReLU(),
            torch.nn.Upsample(scale_factor=2, mode="nearest"),
            torch.nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)
        )

    def forward(self, input):
        output = self.encoder(input)
        output = self.decoder(output)
        return output

model = AutoEncoder()
Use_gpu = torch.cuda.is_available()
if Use_gpu:
    model = model.cuda()
print(model)

optimizer = torch.optim.Adam(model.parameters())
loss_f = torch.nn.MSELoss()  # å‡æ–¹è¯¯å·®

epoch_n = 5
for epoch in range(epoch_n):
    running_loss = 0.0

    print("Epoch {}/{}".format(epoch, epoch_n))
    print("-"*10)

    for data in train_load:  # å¾ªç¯æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        # å¯¹å›¾ç‰‡è¿›è¡Œæ‰“ç å¤„ç†å¹¶è£å‰ªåˆ°æŒ‡å®šçš„åƒç´ å€¼èŒƒå›´å†…
        X_train, _ = data
        noisy_X_train = X_train + 0.5*torch.randn(X_train.shape)
        noisy_X_train = torch.clamp(noisy_X_train, 0., 1.)
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œè¾“å‡ºä¸€ä¸ªé¢„æµ‹å›¾ç‰‡ï¼Œç”¨è¯¥é¢„æµ‹å›¾ç‰‡ä¸åŸå§‹å›¾ç‰‡è¿›è¡ŒæŸå¤±å€¼è®¡ç®—
        X_train, noisy_X_train = Variable(X_train.cuda()), Variable(noisy_X_train.cuda())
        train_pre = model(noisy_X_train)
        loss = loss_f(train_pre, X_train)

        optimizer.zero_grad()
        loss.backward()  # é€šè¿‡æŸå¤±å€¼å¯¹æ¨¡å‹è¿›è¡Œåå‘ä¼ æ’­ï¼Œæœ€åå°±èƒ½å¾—åˆ°å»é™¤å›¾ç‰‡é©¬èµ›å…‹æ•ˆæœçš„æ¨¡å‹äº†
        optimizer.step()

        running_loss += loss.data.item()

    print("Loss is:{:.4f}".format(running_loss/len(dataset_train)))
```

# è¿ç§»å­¦ä¹ 
æœŸæœ›ä½¿ç”¨æµ·é‡æ•°æ®è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ä¸æ˜¯åªèƒ½è§£å†³æŸä¸€ä¸ªé—®é¢˜ï¼Œè€Œæ˜¯è§£å†³æŸä¸€ç±»é—®é¢˜ï¼Œäºæ˜¯å‡ºç°äº†è¿ç§»å­¦ä¹ â€”â€”ä½¿ç”¨è¿ç§»æ¨¡å‹**è§£å†³åŒä¸€ç±»é—®é¢˜**ã€‚è¿™æ ·ï¼Œå¯¹ä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œç»†å¾®è°ƒæ•´ï¼Œå°±èƒ½å°†å…¶åº”ç”¨åˆ°ç›¸ä¼¼çš„é—®é¢˜ä¸­ï¼Œä¸”å–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚å¦å¤–ï¼Œå¯¹äºåŸå§‹æ•°æ®è¾ƒå°‘çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½å¤Ÿé€šè¿‡é‡‡ç”¨è¿ç§»æ¨¡å‹è¿›è¡Œæœ‰æ•ˆè§£å†³ã€‚ä¸è¿‡ï¼Œåœ¨ä½¿ç”¨è¿ç§»å­¦ä¹ çš„è¿‡ç¨‹ä¸­æœ‰æ—¶ä¼šå¯¼è‡´è¿ç§»æ¨¡å‹å‡ºç°è´Ÿè¿ç§»ï¼Œå³æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¾ˆç³Ÿç³•ã€‚è‹¥å°†è¿ç§»å­¦ä¹ ç”¨äºè§£å†³ä¸¤ä¸ªæ¯«ä¸ç›¸å¹²çš„é—®é¢˜ï¼Œåˆ™ææœ‰å¯èƒ½ä½¿æœ€åè¿ç§»å¾—åˆ°çš„æ¨¡å‹å‡ºç°è´Ÿè¿ç§»ã€‚