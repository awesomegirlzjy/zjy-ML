> 学习资源来自==《深度学习之PyTorch实战计算机视觉》(唐进民)== . 一步步让你弄懂使用PyTorch如何搭建一个神经网络。

> 机器学习的工作流程：
> 1. 处理数据 working with data
> 2. 创建模型 creating models
> 3. 优化模型参数 optimizing model parameters
> 4. 保存训练好的模型 saving the trained models

# 深度神经网络基础
## 欠拟合、过拟合
欠拟合：对已有数据的匹配性很差，对数据中的噪声不敏感。
过拟合：对数据的匹配性太好，对数据中的噪声非常敏感。

## 正向传播、反向传播
正向传播沿着从输⼊层到输出层的顺序，依次计算并存储神经⽹络的中间变量。
反向传播沿着从输出层到输⼊层的顺序，依次计算并存储神经⽹络的中间变量和参数的梯度。

> - 在通过多次反向传播后，就可以得到模型的最优参数组合。深度神经网络中的参数进行反向传播的过程其实就是一个复合函数求导的过程。
> - 在训练深度学习模型时，正向传播和反向传播相互依赖。
## 损失和优化
==损失==
- 度量预测值和数据真实值之间的差距，用来衡量训练出来的模型泛化能力好坏。
- 模型预测值和真实值的差距越大，损失值就会越高，这时我们就需要通过不断地对模型中的参数进行优化来减少损失。
- 进行损失值计算的函数叫做*损失函数*，如：均方误差函数（Mean Square Error, MSE）、均方根误差函数（Root Mean Square Error, RMSE）、平均绝对误差函数（Mean Absolute Error, MAE）。

==优化==  

- 对模型进行优化的最终目的尽可能的在不过拟合的情况下**降低损失值**。
- **反向传播**是模型参数优化过程中的一部分，在实际的优化过程中，我们还面临在优化过程中相关参数的初始化、参数以何种形式进行微调、如何选取合适的**学习速率**（学习速率用于控制梯度更新的快慢。学习速率越快，参数的更新跨步就会越大，易出现局部最优和抖动）等问题。我们可以把优化函数看做上述问题的解决方案的集合。
- 对模型参数进行优化的函数叫做*优化函数*。
- **梯度**：梯度是将多元函数的各个参数求得的偏导数以向量的形式展现出来，也叫做多元函数的梯度。偏导数就是在反向传播过程中对每个参数求得的偏导数，所以我们在模型优化的过程中使用的参数微调值其实就是函数计算得到的梯度，这个过程又叫做参数的梯度更新。【对于只有单个参数的函数，我们选择使用计算得到的导数来完成参数的更新，如果在一个函数中需要处理的是多个参数的问题，就选择使用计算得到的梯度来完成参数的更新。 **===>**  梯度就是导数呀，是多个偏导数组合成的一个向量】
- 实际操作中最常用到的是一阶优化函数，如梯度下降（Gradient Descent, GD）、批量梯度下降（Batch Gradient Descent, BGD）、随机梯度下降（Stochastic Gradient Descent, SGD）、Momentum、Adagrad、自适应时刻估计方法（Adaptive Moment Estimation, Adam），等等。一阶优化函数在优化过程中求解的是参数的一阶导数，这些一阶导数的值就是模型中参数的微调值。
## 激活函数
**偏置（Bias）**：偏置可以让我们搭建的神经网络模型偏离原点（没有偏置的函数必定会经过原点）。

- 无偏置：f(x) = W * X
- 有偏置b：f(x) = W * X + b

**激活函数**的引入给我们搭建的模型带来了非线性因素，非线性的模型能够处理更复杂的问题，所以通过选取不同的激活函数便可以得到复杂多变的深度神经网络。常用的激活函数：Sigmoid、tanh、**ReLU**（Rectified Linear Unit，修正线性单元）。

# 🔑搭建一个简易神经网络
### 代码
```python
# Step1、导包
import torch


# Step2、定义变量
batch_n = 100        # 一个批次中输入数据的数量（每个数据包含的数据特征有input_data个）
hidden_layer = 100   # 经过隐藏层后保留的数据特征的个数（此程序仅考虑一层隐藏层，所以仅定义一个隐藏层的参数）
input_data = 1000    # 每个数据包含的数据特征的个数
output_data = 10     # 有output_data种分类结果


# Step3、定义维度 & 初始化权重
x = torch.randn(batch_n, input_data)   # 输入层维度(100, 1000)
y = torch.randn(batch_n, output_data)  # 输出层维度(100, 10)

w1 = torch.randn(input_data, hidden_layer)   # 从输入层到隐藏层的权重参数维度(1000, 100)
w2 = torch.randn(hidden_layer, output_data)  # 从隐藏层到输出层的权重参数维度(100, 10)


# Step4、对模型进行训练
epoch_n = 20          # 训练次数/反向传播的次数
learning_rate = 1e-6  # 梯度下降使用的学习速率

for epoch in range(epoch_n): 	 # 【下有详细介绍】
    # 4.1 正向传播
    h1 = x.mm(w1)         # 矩阵乘积：x(100, 1000) * w1(1000, 100) ==> h1(100, 100)
    h1 = h1.clamp(min=0)  # 对h1进行裁剪，将小于0的值全部重新赋值为0(这就像加上了一个ReLU激活函数的功能)
    y_pred = h1.mm(w2)    # 正向传播得到的预测结果：h1(100, 100) * w2(100, 10) ==> y_pred(100, 10)

    loss = (y_pred - y).pow(2).sum()  # 计算预测结果与真实值之间的误差（使用的是均方误差函数）
    print("Epoch: {}, Loss:{:.4f}".format(epoch, loss))

    # 4.2 反向传播（计算链式求导的结果）
    grad_y_pred = 2 * (y_pred - y)
    grad_w2 = h1.t().mm(grad_y_pred)  # w2对应的梯度

    grad_h = grad_y_pred.clone()
    grad_h = grad_h.mm(w2.t())
    grad_h.clamp_(min=0)
    grad_w1 = x.t().mm(grad_h)  	  # w1对应的梯度

    # 4.3 根据学习速率对w1、w2的权重参数进行更新
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
```
### 部分代码解释 
一个批次的数据从输入到输出的完整过程：
1. 先输入100个具有1000个特征的数据（输入层）；
2. 经过隐藏层后变成100个具有100个特征的数据（隐藏层）；
> 前两步对应`h1 = x.mm(w1)`
3. 再经过输出层后输出100个具有10个分类结果值的数据（输出层）；
> 第三步对应`y_pred = h1.mm(w2)`
4. 得到输出结果之后计算**损失值**并进行**反向传播**。

本程序中定义的输入层`x`、输出层`y`、`y_pred `，其实就分别对应真实的数据集里的`数据`、`数据所属分类`、`使用自定义模型(就是做了两次矩阵的乘积)预测出来的数据的分类`。应该着重于总体步骤。

### 打印结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/2021042220044381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
### 结论
随着训练、优化的次数不断增加，损失值（真实值与预测值之间的差值）在不断减少。

# 自动梯度(torch.autograd、Variable)
### 理论基础
1. `torch.autograd`包：PyTorch提供的`torch.autograd`包，实现了模型参数自动计算在优化过程中需要用到的梯度值（或者说它完成了神经网络反向传播中的**链式求导**）。
2. **自动梯度**的实现过程：先通过输入的Tensor数据类型的变量在神经网络的前向传播过程中生成一张**计算图**，然后根据这个计算图和输出结果准确计算出每个参数需要更新的**梯度**，并通过完成**后向传播**来完成对参数的梯度更新。
3. 使用`torch.autograd`包中的**Variable类**对Tensor实例对象进行封装，在封装后，计算图中的各个结点就是一个Variable对象，这样才能应用自动梯度的功能。
4.  如果已经按照如上方式完成了相关的操作，则在选中了计算图中的某个节点时，这个节点必定会是一个`Variable`对象。设`X`为一个节点，那么`X.data`代表Tensor数据类型的变量，`X.grad`也是一个Variable对象，不过它表示的是`X`的梯度，在想访问梯度值时需要使用`X.grad.data`。
### 代码
```python
# Step1、导包
import torch
from torch.autograd import Variable


# Step2、定义变量
batch_n = 100
hidden_layer = 100 
input_data = 1000 
output_data = 10 


# Step3、定义维度 & 初始化权重 【下有详细介绍——Variable】
x = Variable(torch.randn(batch_n, input_data), requires_grad=False)  # 输入层维度
y = Variable(torch.randn(batch_n, output_data), requires_grad=False)  # 输出层维度
w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad=True)  # 从输入层到隐藏层的权重参数维度
w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad=True)  # 从隐藏层到输出层的权重参数维度


# Step4、对模型进行训练
epoch_n = 20   
learning_rate = 1e-6

for epoch in range(epoch_n):
	# 4.1 正向传播
    y_pred = x.mm(w1).clamp(min=0).mm(w2)
    loss = (y_pred-y).pow(2).sum()
    print("Epoch:{}, Loss:{:.4f}".format(epoch, loss.data.item()))  # 只包含一个整型的Tensor型变量调用item()函数可以直接把Tensor转换成一个整型变量

	# 4.2 反向传播
    loss.backward()   # 【下有详细介绍——backward()】

	# 4.3 根据学习速率对w1、w2的权重参数进行更新
    w1.data -= learning_rate*w1.grad.data  # w1.grad.data是求得的梯度值
    w2.data -= learning_rate*w2.grad.data

    # 4.4 将本次计算得到的各个参数节点的梯度值通过grad.data.zero_()全部置零，否则，计算的梯度值会被一直累加，影响后续的计算。
    w1.grad.data.zero_()
    w2.grad.data.zero_()
```
### 部分代码解释
Step3中，使用`torch.autograd`包中的Variable类对Tensor实例对象进行封装。参数`requires_grad=False`表示：该变量在进行自动梯度计算的过程中不会保留梯度值。把`x`, `y`设置为`False`，是因为这两个变量并不是我们的模型需要优化的参数。

Step4.2中，`backward()`函数的功能是让模型根据计算图自动计算每个节点的梯度值并根据需求进行保留。有了这一步，权重参数`w1.data`、`w2.data`就可以直接使用在自动梯度过程中求得的梯度值`w1.grad.data`、`w2.grad.data`，并结合**学习速率**来对现有的参数进行更新、优化。
### 打印结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210422203036311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
### 结论
损失值也在逐渐减小，而且比上个程序的结果更好。

# 自定义传播函数(torch.nn.Module)
### 理论基础
除了可以采用自动梯度方法，还可以通过构建一个继承了`torch.nn.Module`的新类，来完成对前向传播函数和后向传播函数的自定义重写。重写`forward()`即表示重写前向传播函数，重写`backward()`即表示重写后向传播函数。
### 代码
```python
# Step1、导包
import torch
from torch.autograd import Variable


# Step2、定义变量
batch_n = 100
hidden_layer = 100
input_data = 1000
output_data = 10


# Step3、重写传播函数
class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, input, w1, w2):  # 实现模型的前向传播中的矩阵运算
        x = torch.mm(input, w1)
        x = torch.clamp(x, min=0)
        x = torch.mm(x, w2)
        return x

    def backward(self):  # 实现自动梯度计算（没有特别需求，一般不用进行调整）
        pass


# Step4、定义维度 & 初始化权重 
x = Variable(torch.randn(batch_n, input_data), requires_grad=False)  # 输入层维度
y = Variable(torch.randn(batch_n, output_data), requires_grad=False)  # 输出层维度
w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad=True)  # 从输入层到隐藏层的权重参数维度
w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad=True)  # 从隐藏层到输出层的权重参数维度


# Step5、对模型进行训练
epoch_n = 20  
learning_rate = 1e-6 
model = Model()  # 实例化类对象

for epoch in range(epoch_n):
	# Step5.1 正向传播
    y_pred = model(x, w1, w2)   # forward()会自动被执行
    loss = (y_pred - y).pow(2).sum()
    print("Epoch: {}, Loss: {:.4f}".format(epoch, loss.data.item()))
    
    # Step5.2 反向传播
    loss.backward()
	
	 # Step5.3 权重参数更新
    w1.data -= learning_rate * w1.grad.data
    w2.data -= learning_rate * w2.grad.data
	
	# Step5.2 重置梯度值为0
    w1.grad.data.zero_()
    w2.grad.data.zero_()
```
### 打印结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210422205912454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)

# 模型搭建(torch.nn.Sequential)
### 理论基础
- PyTorch中不仅有能实现自动梯度计算的类，它还定义了神经网络中线性变换、激活函数、卷积层、全连接层、池化层等常用神经网络结构的实现。
- 在完成模型的搭建之后，我们还可以使用PyTorch提供的类型丰富的**优化函数**来完成对模型参数的优化，除此之外，还有很多防止模型在模型训练过程中发生过拟合的类。
- PyTorch中的`torch.nn`包提供了很多与实现神经网络中的具体功能相关的类。比如，神经网络中的卷积层、池化层、全连接层这类层次构造的方法，以及防止过拟合的参数归一化方法、Dropout方法，还有激活函数部分的线性激活函数、非线性激活函数相关的方法，等等。
### 代码
```python
import torch
from torch.autograd import Variable

batch_n = 100
hidden_layer = 100
input_data = 1000
output_data = 10

x = Variable(torch.randn(batch_n, input_data), requires_grad=False) 
y = Variable(torch.randn(batch_n, output_data), requires_grad=False) 
# 【为什么无需定义w1、w2了？下有解释】


# 搭建模型【下有详细介绍】
models = torch.nn.Sequential(
    torch.nn.Linear(input_data, hidden_layer),  # 从输入层到隐藏层的线性变换
    torch.nn.ReLU(),    						# 激活函数
    torch.nn.Linear(hidden_layer, output_data)  # 从隐藏层到输出层的线性变换
)


# 对模型进行训练、对参数进行优化
epoch_n = 10000
learning_rate = 1e-4

loss_fn = torch.nn.MSELoss()    # 【下有详细介绍——torch.nn.MSELoss()】

for epoch in range(epoch_n):
    y_pred = models(x)
    loss = loss_fn(y_pred, y)
    if epoch%1000 == 0:  # 每完成1000次训练，才打印一次
        print("Epoch: {}, Loss: {:.4f}".format(epoch, loss.data.item()))
        
    models.zero_grad()

    loss.backward()

	# 参数梯度更新：对models.parameters()进行遍历以访问模型中的全部参数，然后对其更新
    for param in models.parameters():   
        param.data -= param.grad.data * learning_rate
```
### 部分代码解释
1. `torch.nn.Sequential`括号内的内容就是我们搭建的神经网络模型的具体结构，这里首先通过`torch.nn.Linear(input_data, hidden_layer)`完成从输入层到隐藏层的线性变换，然后经过激活函数和`torch.nn.Linear(hidden_layer, output_data)`完成从隐藏层到输出层的线性变换。

- **`torch.nn.Sequential`**
`torch.nn.Sequential`类是`torch.nn`中的一种序列容器，通过在容器中嵌套各种实现神经网络中具体功能相关的类，来完成对神经网络模型的搭建，最主要的是，参数会按照我们定义好的序列自动传递下去。我们可以将嵌套在容器中的各个部分看做各种不同的模块，这些模块可以自由组合。模块的加入一般有两种方式，一种是在以上代码中使用的直接嵌套，另一种是以`orderdict`有序字典的方式进行传入。区别是，使用后者搭建的模型的每个模块都有我们自定义的名字，而前者默认使用从零开始的数字序列作为每个模块的名字。

- **`torch.nn.Linear`**
`torch.nn.Linear`类用于定义模型的线性层，即完成前面提到的不同的层之间的线性变换。`torch.nn.Linear`类接收的参数有三个，分别是输入特征数、输出特征数和是否使用偏置（默认为True）。在实际使用的过程中，我们只需将输入的特征数和输出的特征数传递给`torch.nn.Linear`类，就会**自动生成对应维度的权重参数和偏置**。= => 根据我们搭建模型的输入、输出和层次结构的需求，它的输入是在一个批次中包含100个特征数为1000的数据，最后得到100个特征数为10的数据，中间需要经过两次线性变换：1）x(100, 1000) * w1(1000, 100) ==> h1(100, 100)，2）h1(100, 100) * w2(100, 10) ==> y_pred(100, 10)。所以要使用两个线性层。这代替了之前使用矩阵乘法方式的实现。

- **`torch.nn.ReLU`**
`torch.nn.ReLU`类属于非线性激活分类，在定义时默认不需要传入参数。

2. 计算损失函数的代码发生了变化，现在使用的是`torch.nn`包中已经定义好的均方误差函数类`torch.nn.MSELoss`来计算损失值。
- **`torch.nn.MSELoss`**
`torch.nn.MSELoss`类使用**均方误差函数**来计算损失值。在定义类的对象时不用传入任何参数，但在使用实例时需要输入两个维度一样的参数方可进行计算。

- **`torch.nn.L1Loss`**
`torch.nn.L1Loss`类使用**平均绝对误差函数**来计算损失值。

- **`torch.nn.CrossEntropyLoss`**
`torch.nn.CrossEntropyLoss`类使用**交叉熵**来计算损失值。

3. 由1、2知，此程序就已经不需要自己做矩阵乘法来进行线性变换（实际上就是用矩阵乘法做预测）了，因为这些转换工作已经交给搭建好的模型来完成了。
### 运行结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210424115957198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
### 结论
可以看到，使用该模型已经可以使误差很小了。
# 参数优化(torch.optim)
### 理论基础
- 到目前为止，代码中的神经网络权重的参数优化和更新还没有实现自动化，并且目前使用的优化方法都有固定的学习速率，所以优化函数相对简单。
- `torch.optim`包中提供了非常多的实现**参数自动优化**的类，比如SGD、AdaGrad、RMSProp、Adam等。

### 代码
```python
import torch
from torch.autograd import Variable
batch_n = 100
hidden_layer = 1000
input_data = 1000
output_data = 10

x = Variable(torch.randn(batch_n, input_data), requires_grad=False)
y = Variable(torch.randn(batch_n, output_data), requires_grad=False)

models = torch.nn.Sequential(
    torch.nn.Linear(input_data, hidden_layer),
    torch.nn.ReLU(),
    torch.nn.Linear(hidden_layer, output_data)
)


epoch_n = 100
learning_rate = 1e-4
loss_fn = torch.nn.MSELoss()

optimzer = torch.optim.Adam(models.parameters(), lr = learning_rate)  # 【后有详细介绍】

for epoch in range(epoch_n):
    y_pred = models(x)
    loss = loss_fn(y_pred, y)
    if epoch % 5 == 0:
        print("Epoc {}, Loss: {:.4f}".format(epoch, loss.data.item()))

    optimzer.zero_grad()  # 对模型参数梯度归零

    loss.backward()  	  # 反向传播

    optimzer.step()  	  # 使用计算得到的梯度值对各个结点的参数进行梯度更新（这是本节要学习的新轮子，又减轻了前面都是专门写代码来更新的工作）
```
### 部分代码解释
使用优化函数对参数进行优化：`torch.optim.Adam(需要优化的参数列表，学习速率的初始值[默认为0.001]，动量)`。Adam优化函数的一个强大之处：对梯度更新使用到的学习速率进行自适应调节。因为我们需要优化的是模型中的全部参数，所以传递给`torch.optim.Adam`类的参数是`models.parameters`。
### 运行结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/2021042412071426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
# 实战手写数字识别
### 理论基础
每条数据的特征都是28*28个[0,1]之间的实数，表示灰度图像；标签是{0,1,...,9}这10个数字中的一个。

具体过程：
1) 使用训练数据对搭建好的神经网络模型进行训练并完成参数优化。
2) 使用优化好的模型对测试数据进行预测，对比预测值和真实值之间的损失值，同时计算出结果预测的准确率。

### 代码1
```python
import torch
import torchvision
import matplotlib.pyplot as plt
from torchvision import datasets, transforms  # 用于下载数据集、数据变换
from torchvision.transforms import ToTensor


# Step1. 数据集下载
transform = transforms.Compose([
    transforms.ToTensor(), 
    transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])
])  # 【后有详细介绍】

data_train = datasets.MNIST(
    root="./dataMNIST/",  # 指定数据集下载之后的存放路径
    transform=transform,  # 导入数据集时需要对数据进行哪种变换操作 【后有详细介绍】
    train=True, 		  # 指明数据集下载完成后需要载入哪部分数据(True-->训练集, False-->测试集)
    download=True
)

data_test = datasets.MNIST(
    root='./dataMNIST/',
    train=False,
    transform=transform
)


# Step2. 数据装载 【后有详细介绍】
data_loader_train = torch.utils.data.DataLoader(dataset=data_train, batch_size=64, shuffle=True)
data_loader_test = torch.utils.data.DataLoader(dataset=data_test, batch_size=64, shuffle=True)


# Step3. 选择一个批次的数据进行预览
# 3.1 使用next、iter来获取一个批次的图片数据和其对应的图片标签
images, labels = next(iter(data_loader_train))
# print(f'image.size() = {images.size()}')    # 特征张量的大小是(64, 1, 28, 28)
# print(f'labels.size() = {labels.size()}')   # 标签张量的大小是(64, 1)

# 显式第0条训练数据的特征和标签：
# plt.imshow(images[0, 0], cmap='gray')
# plt.title(f'label = {labels[0]}')
# plt.show()

'''
3.2 将一个批次的图片构造成网格模式，参数即为一个批次所装载的数据.
    解释：每个批次的装载数据都是4维的，维度的构成从前往后分别为batch_size、channel、heigth、weight,
         分别对应这个批次中的数据个数、每张图片的色彩通道数、每张图片的高度和宽度。
         在通过torchvision.utils.make_grid之后，图片的维度变成了(channel, height, weight),
         因为这个批次的数据全部被整合到了一起（色彩通道数不变），所以没有batch_size了。
'''
img = torchvision.utils.make_grid(images)
# 3.3 上面得到的img的维度是(channel, height, weight)，这里将其转换为(height, weight, channel)，这样才能正确调用imshow().
img = img.numpy().transpose(1, 2, 0)
# 3.4 展示
std = [0.5, 0.5, 0.5]
mean = [0.5, 0.5, 0.5]
img = img * std + mean
print([labels[i] for i in range(64)])  # 打印批次中各数据的全部标签
plt.imshow(img)
plt.show()
```
### 部分代码解释1(torchvision.transforms、DataLoader)
- `torchvision.transforms`中提供了丰富的类对载入的数据进行变换。比如，对于图片类型的数据集，我们得想办法把它变成Tensor数据类型的变量，才能对其进行计算。另外，如果获取的数据是格式或者大小不一的图片，则还需要进行归一化和大小缩放等操作。
- `torchvision.transforms`中有大量的数据变换类，其中有很大一部分可以用于实现**数据增强（Data Argumentation）**。若在我们需要解决的问题上能够参与到模型训练中的图片数据非常有限，则就可以通过对有限的图片数据进行各种变换，来生成新的训练集，这些变换可以是缩小或者放大图片的大小、对图片进行水平或者垂直翻转等，都是数据增强的方法。
- `transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])]) `用来对数据进行变换：可以将`torchvision.transforms.Compose`类看做一种容器，它能够同时对多种数据变换进行组合。传入的参数是一个列表，列表中的元素是对载入的数据进行的各种变换操作。此处，`transforms.ToTensor()`是一个*类型变换*；` transforms.Normalize`是一个*数据标准化变换*（标准差变换法），需使用原始数据的均值(Mean)和标准差(Standard Deviation)来进行数据的标准化，在经过标准变换之后，数据全部符合均值为0、标准差为1的标准正态分布。不过作者偷懒了，均值和标准差不是来自原始数据，而是自定义的。
- `torchvision.transforms`中的常用数据变换操作：
1. `torchvision.transforms.Resize`
2. `torchvision.transforms.Scale`
3. `torchvision.transforms.CenterCrop`
4. `torchvision.transforms.RandomCrop`
5. `torchvision.transforms.RandomHorizontalFlip`
6. `torchvision.transforms.RandomVerticalFlip`
7. `torchvision.transforms.ToTensor()`
8. `torchvision.transforms.ToPILImage`

-- --
- 数据装载：由于数据集里有上万条数据，所以往往要*分批* 从数据集中读出数据；且装载好了的数据就可以进行遍历了。例如，可以每次读1个数据条目，或是每次读64个数据条目。对数据的装载使用的是`torch.utils.data.DataLoader`类，类中的`dataseet`参数表示载入的数据集名称，`batch_size`参数表示每批次读几条数据（默认为1），`shuffle`参数表示是否在装载的过程中打乱图片的顺序（默认为False）.
### 运行结果1
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210424123710323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210424123726519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)

### 代码2
```python
import torch
from torch.autograd import Variable
from book_ex6 import data_loader_train, data_loader_test, data_train, data_test
import torchvision
import matplotlib.pyplot as plt


# Step4、模型搭建 【后有详细介绍】
class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = torch.nn.Sequential(
            torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # 卷积层
            torch.nn.ReLU(),                                             # 激活层
            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),# 卷积层
            torch.nn.ReLU(),                                             # 激活层
            torch.nn.MaxPool2d(stride=2, kernel_size=2)                  # 最大池化层
        )
        self.dense = torch.nn.Sequential(      # 定义全连接层
            torch.nn.Linear(14*14*128, 1024),
            torch.nn.ReLU(),
            torch.nn.Dropout(p=0.5),           # 防止卷积神经网络在训练的过程中发生过拟合
            torch.nn.Linear(1024, 10)
        )

    def forward(self, x):           # 前向传播
        x = self.conv1(x)           # 进行卷积处理
        x = x.view(-1, 14*14*128)   # 对参数实现扁平化(如果不进行扁平化，则之后的全连接层进行分类时会使得实际输出的参数维度和其定义输入的维度不匹配)
        x = self.dense(x)           # 调用全连接层进行分类
        return x


# Step5. 训练模型、优化参数
model = Model()
# print(model)  # 查看搭建好的模型的完整结构

cost = torch.nn.CrossEntropyLoss()  # 计算损失值的损失函数使用的是交叉熵
optimizer = torch.optim.Adam(model.parameters())  # 优化函数使用的是Adam自适应优化算法

n_epochs = 5
for epoch in range(n_epochs):
    running_loss = 0.0   # 训练模型时，每次的损失值之和
    running_correct = 0  # 训练模型时，得到的预测值与实际值相同的次数
    print("Epoch {}/{}".format(epoch+1, n_epochs))
    print("-" * 10)

    for data in data_loader_train:
        X_train, y_train = data    # X_train, y_train分别表示训练集的数据以及其所对应的分类
        X_train, y_train = Variable(X_train), Variable(y_train)
        
        outputs = model(X_train)   # 执行forwad()函数——正向传播
        _, pred = torch.max(outputs.data, 1)
        optimizer.zero_grad()  		   # 清空优化器
        loss = cost(outputs, y_train)  # 计算损失值
        loss.backward()   			   # 反向传播
        optimizer.step()  			   # 优化（权重更新）
        
        running_loss += loss.data.item()
        running_correct += torch.sum(pred == y_train.data)

    testing_correct = 0
    for data in data_loader_test:
        X_test, y_test = data
        X_test, y_test = Variable(X_test), Variable(y_test)
        outputs = model(X_test)
        _, pred = torch.max(outputs, 1)
        testing_correct += torch.sum(pred == y_test.data)

    print("Loss is {:.4f}, Train Accuracy is {:.4f}%, Test Accuracy is {:.4f}".format(running_loss/len(data_train), 100*running_correct/len(data_train), 100*testing_correct/len(data_test)))


# Step6. 验证：随机选取一部分测试集中的图片，用训练好的模型进行预测，并对结果进行可视化
data_loader_test = torch.utils.data.DataLoader(dataset=data_test, batch_size=4, shuffle=True)
X_test, y_test = next(iter(data_loader_test))
inputs = Variable(X_test)
pred = model(inputs)
_, pred = torch.max(pred, 1)

print("Predict Label is: ", [i for i in pred.data])
print("Real Label is: ", [i for i in y_test])

img = torchvision.utils.make_grid(X_test)
img = img.numpy().transpose(1, 2, 0)

std = [0.5, 0.5, 0.5]
mean = [0.5, 0.5, 0.5]
img = img * std + mean
plt.imshow(img)
plt.show()
```

### 部分代码解释2
Step4开始编写卷积神经网络模型的搭建和参数优化的代码。卷积层使用`torch.nn.Conv2d`类方法来搭建；激活层使用`torch.nn.ReLU`类方法来搭建；池化层用`torch.nn.MaxPool2d`类方法来搭建；全连接层使用`torch.nn.Linear`类方法来搭建。
### 运行结果2
> 数据量太大，我的电脑好像运行不出来。。。

# 自动编码器
### 理论基础
自动编码器（AutoEncoder）是一种可以进行无监督学习的神经网络模型。一般而言，一个完整的自动编码器主要由两部分组成——用于核心特征提取的编码部分和可以实现数据重构的解码部分。

**编码器**主要负责*对原始的输入数据进行压缩并提取出数据中的核心特征*，而**解码器**主要是*对在编码器中提取的核心特征进行展开并重新构造出之前的数据*。

自动编码器这种先编码、后解码的神经网络模型的最大用途是：实现输入数据的清洗，比如去除输入数据中的噪声数据、对输入数据的某些关键特征进行增强或放大。再比如，还可以对有马赛克的图片进行除码处理。等等。

### 代码1
通过线性变换实现自动编码器模型。
```python
import torch
import torchvision
from torchvision import datasets, transforms
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt

# 1. 下载数据
transform = transforms.Compose([transforms.ToTensor()])
dataset_train = datasets.MNIST(
    root="./dataMNIST/",
    transform=transform,
    train=True,
    download=True
)
dataset_test = datasets.MNIST(
    root="./dataMNIST/",
    transform=transform,
    train=False
)

# 2. 装载数据
train_load = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=4, shuffle=True)
test_load = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=4, shuffle=True)

# 3. 取出一个批次的数据进行可视化
images, label = next(iter(train_load))
print(images.shape)
images_example = torchvision.utils.make_grid(images)
images_example = images_example.numpy().transpose(1, 2, 0)
mean = [0.5, 0.5, 0.5]
std = [0.5, 0.5, 0.5]
images_example = images_example * std + mean
plt.imshow(images_example)
plt.show()
noisy_images = images_example + 0.5*np.random.randn(*images_example.shape)
noisy_images = np.clip(noisy_images, 0., 1.)
plt.imshow(noisy_images)
plt.show()

# 4. 搭建模型、训练模型
class AutoEncoder(torch.nn.Module):
    def __init__(self):
        super(AutoEncoder, self).__init__()
        '''
            编码：
            	输入数据的数据量从224个到128个再到64个最后到32个的压缩过程，
            	这32个数据就是我们提取到的核心特征。
        '''
        self.encoder = torch.nn.Sequential(
            torch.nn.Linear(28*28, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU()
        )
        
        # 解码
        self.decoder = torch.nn.Sequential(
            torch.nn.Linear(32, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 28*28)
        )

    def forward(self, input):
        output = self.encoder(input)
        output = self.decoder(output)
        return output

model = AutoEncoder()
print(model)

optimizer = torch.optim.Adam(model.parameters())
loss_f = torch.nn.MSELoss()  # 均方误差

epoch_n = 10
for epoch in range(epoch_n):
    running_loss = 0.0

    print("Epoch {}/{}".format(epoch, epoch_n))
    print("-"*10)

    for data in train_load:  # 循环每个批次的数据
        # 对图片进行打码处理并裁剪到指定的像素值范围内
        X_train, _ = data
        noisy_X_train = X_train + 0.5*torch.randn(X_train.shape)
        noisy_X_train = torch.clamp(noisy_X_train, 0., 1.)
        # 使用模型进行处理，输出一个预测图片，用该预测图片与原始图片进行损失值计算
        X_train, noisy_X_train = Variable(X_train.view(-1, 28*28)), Variable(noisy_X_train.view(-1, 28*28))
        train_pre = model(noisy_X_train)
        loss = loss_f(train_pre, X_train)

        optimizer.zero_grad()
        loss.backward()  # 通过损失值对模型进行后向传播，最后就能得到去除图片马赛克效果的模型了
        optimizer.step()

        running_loss += loss.data.item()

    print("Loss is:{:.4f}".format(running_loss/len(dataset_train)))
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210426100019939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70)

### 代码2
通过卷积变换实现。
```python
import torch
import torchvision
from torchvision import datasets, transforms
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt

# 1. 下载数据
transform = transforms.Compose([transforms.ToTensor()])
dataset_train = datasets.MNIST(
    root="./dataMNIST/",
    transform=transform,
    train=True,
    download=True
)
dataset_test = datasets.MNIST(
    root="./dataMNIST/",
    transform=transform,
    train=False
)

# 2. 装载数据
train_load = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=64, shuffle=True)
test_load = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=64, shuffle=True)

# 3. 取出一个批次的数据进行可视化
images, label = next(iter(train_load))
print(images.shape)
images_example = torchvision.utils.make_grid(images)
images_example = images_example.numpy().transpose(1, 2, 0)
mean = [0.5, 0.5, 0.5]
std = [0.5, 0.5, 0.5]
images_example = images_example * std + mean
plt.imshow(images_example)
plt.show()
noisy_images = images_example + 0.5*np.random.randn(*images_example.shape)
noisy_images = np.clip(noisy_images, 0., 1.)
plt.imshow(noisy_images)
plt.show()

# 4. 搭建模型、训练模型
class AutoEncoder(torch.nn.Module):
    def __init__(self):
        super(AutoEncoder, self).__init__()
        # 编码
        self.encoder = torch.nn.Sequential(
            torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=2, stride=2),
            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=2, stride=2)
        )
        # 解码
        self.decoder = torch.nn.Sequential(
            torch.nn.Upsample(scale_factor=2, mode="nearest"),
            torch.nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),
            torch.nn.ReLU(),
            torch.nn.Upsample(scale_factor=2, mode="nearest"),
            torch.nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)
        )

    def forward(self, input):
        output = self.encoder(input)
        output = self.decoder(output)
        return output

model = AutoEncoder()
Use_gpu = torch.cuda.is_available()
if Use_gpu:
    model = model.cuda()
print(model)

optimizer = torch.optim.Adam(model.parameters())
loss_f = torch.nn.MSELoss()  # 均方误差

epoch_n = 5
for epoch in range(epoch_n):
    running_loss = 0.0

    print("Epoch {}/{}".format(epoch, epoch_n))
    print("-"*10)

    for data in train_load:  # 循环每个批次的数据
        # 对图片进行打码处理并裁剪到指定的像素值范围内
        X_train, _ = data
        noisy_X_train = X_train + 0.5*torch.randn(X_train.shape)
        noisy_X_train = torch.clamp(noisy_X_train, 0., 1.)
        # 使用模型进行处理，输出一个预测图片，用该预测图片与原始图片进行损失值计算
        X_train, noisy_X_train = Variable(X_train.cuda()), Variable(noisy_X_train.cuda())
        train_pre = model(noisy_X_train)
        loss = loss_f(train_pre, X_train)

        optimizer.zero_grad()
        loss.backward()  # 通过损失值对模型进行后向传播，最后就能得到去除图片马赛克效果的模型了
        optimizer.step()

        running_loss += loss.data.item()

    print("Loss is:{:.4f}".format(running_loss/len(dataset_train)))
```

# 迁移学习
期望使用海量数据训练出来的模型不是只能解决某一个问题，而是解决某一类问题，于是出现了迁移学习——使用迁移模型**解决同一类问题**。这样，对一个训练好的模型进行细微调整，就能将其应用到相似的问题中，且取得很好的效果。另外，对于原始数据较少的问题，我们也能够通过采用迁移模型进行有效解决。不过，在使用迁移学习的过程中有时会导致迁移模型出现负迁移，即模型的泛化能力很糟糕。若将迁移学习用于解决两个毫不相干的问题，则极有可能使最后迁移得到的模型出现负迁移。