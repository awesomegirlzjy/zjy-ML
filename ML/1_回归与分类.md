> $m$ 代表训练集中实例的数量
>
> $n$ 代表特征数
>
> $x$ 代表特征/输入变量
>
> $y$ 代表目标变量/输出变量
>
> $\left( x,y \right)$ 代表训练集中的实例
>
> $({{x}^{(i)}},{{y}^{(i)}})$ 代表第$i$ 个观察实例/样本
>
> $h$ 代表学习算法的解决方案或函数, 也称为假设（**hypothesis**）

# 1 线性回归

## 1.1 代价函数

单变量线性回归问题(只含有一个特征/输入变量)：

![image-20210919231439890](E:\md笔记\images\image-20210919231439890.png)

代价函数 $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$也被称作**平方误差函数**，有时也被称为**平方误差代价函数**。我们之所以要求出误差的平方和，是因为误差平方代价函数对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

假设代价函数 $J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}$ 其中：${h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$

则其Python代码实现：

```python
def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))
```

## 1.2 梯度下降

### 1.2.1 简介

在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小$a$。

梯度下降算法和线性回归算法比较如图：

![image-20210924100839069](E:\md笔记\images\image-20210924100839069.png)

对线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：

$\frac{\partial }{\partial {{\theta }_{j}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{\partial }{\partial {{\theta }_{j}}}\frac{1}{2m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}^{2}}$

$j=0$ 时：$\frac{\partial }{\partial {{\theta }_{0}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}{{\sum_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}}$

$j=1$ 时：$\frac{\partial }{\partial {{\theta }_{1}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

则算法改写成：

**Repeat {**

 ${\theta_{0}}:={\theta_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{ \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}$

 ${\theta_{1}}:={\theta_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

 **}**

我们刚刚使用的算法，有时也称为**批量梯度下降**。”**批量梯度下降**”指的是在梯度下降的每一步中，我们都需要考虑所有一"批"训练样本；有时也有其他类型的梯度下降法，不是这种"批量"型的，即不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。在后面的课程中，我们也将介绍这些方法。

### 1.2.2 梯度下降运算中的实用技巧

#### 特征缩放（Feature Scaling）

特征缩放：确保不同特征的取值在相近的范围内，这样梯度下降法就能更快地收敛。

以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。

![image-20210920145600311](E:\md笔记\PyTorch\images\image-20210920145600311.png)

解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。如图：

![image-20210920145709984](E:\md笔记\PyTorch\images\image-20210920145709984.png)

最简单的方法是令：${{x}_{n}}=\frac{{{x}_{n}}-{{\mu}_{n}}}{{{s}_{n}}}$，其中 ${\mu_{n}}$是训练集中特征${x_{n}}$的平均值，${s_{n}}$是该特征值的取值范围（最大值-最小值）或标准差。

#### 学习率

绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛：

![image-20210920153126480](E:\md笔记\PyTorch\images\image-20210920153126480.png)

另外，也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较，但是要选择一个合适的阈值是相当困难，所以通常直接利用图表更好。这一图像还可以告诉你算法有没有正常工作。

通常可以考虑尝试些学习率：

$\alpha=0.01，0.03，0.1，0.3，1，3，10$

## 1.3 正规方程

### 1.3.1 介绍

到目前为止，我们都是在使用梯度下降算法来求模型的参数，但是对于某些线性回归问题，正规方程方法是更好的解决方案。正规方程提供了一种求得参数的解析解法（直接一次性求得最优值），而不是梯度下降算法这种迭代的方式。

正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0$ 【即微积分里另导数为0，求得极值点】。 假设我们的训练集特征矩阵为 $X$（包含了 ${{x}_{0}}=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量 $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ 。 上标**T**代表矩阵转置，上标 -1 代表矩阵的逆。设矩阵$A={X^{T}}X$，则：${{\left( {X^T}X \right)}^{-1}}={A^{-1}}$ 。

示例：

![image-20210921131825629](E:\md笔记\PyTorch\images\image-20210921131825629.png)

运用正规方程方法求解参数 $\theta$：

![image-20210921131645232](E:\md笔记\PyTorch\images\image-20210921131645232.png)

梯度下降与正规方程的比较：

| 梯度下降                      | 正规方程                                                     |
| :---------------------------- | ------------------------------------------------------------ |
| 需要选择学习率$\alpha$        | 不需要                                                       |
| 需要多次迭代                  | 一次运算得出                                                 |
| 当特征数量$n$大时也能较好适用 | 需要计算${{\left( {{X}^{T}}X \right)}^{-1}}$ 。如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O\left( {{n}^{3}} \right)$，通常来说当$n$小于10000 时还是可以接受的。 |
| 适用于各种类型的模型          | 只适用于线性模型，不适合逻辑回归模型等其他模型               |

总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，通常使用标准方程法，而不使用梯度下降法。

随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。

用Python实现正规方程：

```python
import numpy as np
    
def normalEqn(X, y):
    
   theta = np.linalg.inv(X.T@X)@X.T@y  # X.T@X等价于X.T.dot(X)
    
   return theta
```

在 **Octave** 中，正规方程写作：

```octave
pinv(X'*X)*X'*y
```

### 1.3.2 正规方程与不可逆

当计算 $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$时，发现${X^T}X$不可逆时，怎么办？

我们称那些不可逆矩阵为**奇异**或**退化矩阵**。

 ${X^T}X$的不可逆的问题其实很少发生。导致${X^T}X$不可逆的常见原因：

1. 包含多余的特征。例如，在预测住房价格时，如果${x_{1}}$是以平方英尺为单位的房子面积，${x_{2}}$是以平方米为单位的房子面积，由于1米等于3.28英尺，所以：${x_{1}}={x_{2}}*{{\left( 3.28 \right)}^{2}}$。包含了这两个特征的$X$做运算${X^T}X$将得到的是不可逆的矩阵（两行成比例了呀, 线性相关）。

2. 学习算法的特征过多（m ≤ n）。例如，m=10，n=100（即m个训练样本，每个样本有100个特征），要找到适合的$(n +1)$ 维参数矢量$\theta$。尝试从10个训练样本中找到满足101个参数的值，这项工作并不一定能成功，因为数据太少。

   > 当$m$比$n$小时，我们可以选择删除某些特征，或者使用一种叫做**正则化**的方法（即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数）

> 在**Octave**里，如果使用`pinv(X'*X)*X'*y`来实现$\theta$的计算，你将会得到一个正解。
>
> 在**Octave**里，有两个函数可以求解矩阵的逆，一个被称为`pinv()`，另一个是`inv()`，这两者之间的差异是些许计算过程上的，一个是所谓的伪逆，另一个被称为逆。使用`pinv()`可以展现数学上的过程，这将计算出$\theta$的值，即便矩阵$X'X$是不可逆的。

### 1.3.3 选学

$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$ 的推导过程：

$J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}$ 其中：${h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$

将向量表达形式转为矩阵表达形式，则有$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{2}}$ ，其中$X$为$m$行$n$列的矩阵（$m$为样本个数，$n$为特征个数），$\theta$为$n$行1列的矩阵，$y$为$m$行1列的矩阵，对$J(\theta )$进行如下变换

$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{T}}\left( X\theta -y \right)$

 $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}-{{y}^{T}} \right)\left(X\theta -y \right)$

 $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}X\theta -{{\theta}^{T}}{{X}^{T}}y-{{y}^{T}}X\theta -{{y}^{T}}y \right)$

接下来对$J(\theta )$偏导，需要用到以下几个矩阵的求导法则:

$\frac{dAB}{dB}={{A}^{T}}$

$\frac{d{{X}^{T}}AX}{dX}=2AX$

所以有:

$\frac{\partial J\left( \theta \right)}{\partial \theta }=\frac{1}{2}\left(2{{X}^{T}}X\theta -{{X}^{T}}y -{}({{y}^{T}}X )^{T}-0 \right)$

$=\frac{1}{2}\left(2{{X}^{T}}X\theta -{{X}^{T}}y -{{X}^{T}}y -0 \right)$

 $={{X}^{T}}X\theta -{{X}^{T}}y$

令$\frac{\partial J\left( \theta \right)}{\partial \theta }=0$,

则有$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$.

# 2 逻辑回归（Logistic Regression）

## 2.1 logistic function

我们从二元的分类问题开始讨论。假设分类结果只有0、1。那么我们可以称0为**负类**（**negative class**），1为**正类**（**positive class**）。

> 通常，我们把0表示成“没有...”，1表示成“有...”。

我们如何开发一个分类算法？

对于分类问题，不适用线性回归的方法来解决，因为“线性”代表连续的很多值，而“分类”表明分类结果数是有限的。因此，人们开发出了一个被称为**“逻辑回归”**的算法来解决分类问题，这个算法的性质是：*它的输出值永远在 0 到 1 之间*。

> 注意：你可能因为这个算法的名字中出现了“回归”而感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 $y$ 取值离散的情况。

**sigmoid function / logistic function**：$g(z)=\frac{1}{1+{e}^{-z}}$.

> 函数g的图像：![image-20210923195100159](E:\md笔记\images\image-20210923195100159.png)

则令，${{h}_{\theta}}(x)=g(\theta^{T}x)=\frac{1}{1+{e}^{-\theta^{T}x}}$. 这样，假设函数h的取值就在0-1之间了。有了这个假设函数，我们要做的和之前一样，就是用参数 $\theta$ 拟合我们的数据。所以，当我们拿到一个训练集，我们需要给参数 $ \theta$选定一个值，然后假设函数就会帮我们做出预测。

${{h}_{\theta}}(x)$ 实际上表示：**当输入 x 时，y=1 的概率被估计为${{h}_{\theta}}(x)$这个值**。

> **${{h}_{\theta}}(x)$ = estimated probability that y = 1 on input x.**

## 2.2 决策边界 Decision Boundary

既然我们知道了${{h}_{\theta}}(x)$表示y=1的概率，那么就可以直接令：

- 当${h_\theta}\left( x \right)≥0.5$时，预测 $y=1$；

- 当${h_\theta}\left( x \right)<0.5$时，预测 $y=0$ 。

根据上面绘制出的 **Sigmoid** 函数图像，我们知道当

- $z≥0$ 时 $g(z)≥0.5$；

- $z<0$ 时 $g(z)<0.5$。

又 $z={\theta^{T}}x$ ，$g(z)=\frac{1}{1+{e}^{-z}}$. 故有：

- ${\theta^{T}}x≥0$ 时，预测 $y=1$ ；
- ${\theta^{T}}x<0$ 时，预测 $y=0$。

现在假设我们有一个数据集如下图所示，且已经拟合好了参数$\theta$ = [-3 1 1]：

![image-20210923231303854](E:\md笔记\images\image-20210923231303854.png)

 则当$-3+{x_1}+{x_2} \geq 0$，即${x_1}+{x_2} \geq 3$时，模型将预测 $y=1$。

我们可以绘制直线${x_1}+{x_2} = 3$，这条线便是我们模型的分界线，被称为**决策边界**，它将预测为1的区域和预测为 0的区域分隔开。如下图：

![image-20210923231614722](E:\md笔记\images\image-20210923231614722.png)

一个更复杂的例子——假使我们的数据呈现下图这样的分布情况，怎样的模型才能适合呢？

![image-20210923232120023](E:\md笔记\PyTorch\images\image-20210923232120023.png)

显然，线性模型是不正确的；所以，我们在特征中添加额外的高阶多项式项。我们添加两个额外的高阶多项式项，得到假设函数如下：

$${h_\theta}\left( x \right)=g\left( {\theta_0}+{\theta_1}{x_1}+{\theta_{2}}{x_{2}}+{\theta_{3}}x_{1}^{2}+{\theta_{4}}x_{2}^{2} \right)$$

假设参数$\theta=[-1,0,0,1,1]$，则我们得到的**判定边界**恰好是圆点在原点且半径为1的圆形。对于该模型，有：

- $-1 + x_1^2 + x_2^2 ≥ 0$时，预测$y=1$；
- $-1 + x_1^2 + x_2^2 < 0$时，预测$y=0$。

总之，对于任何复杂的数据集，我们都能找到一个模型来确定这个判定边界。（这是有关于如何确定参数$\theta$的问题，下节将对此进行讲述）

## 2.3 代价函数

![image-20210924082307873](E:\md笔记\PyTorch\images\image-20210924082307873.png)

对于**线性回归**模型，我们定义的代价函数是所有模型误差的平方和：

$$J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^i),y^i)$$, 其中$Cost(h_\theta(x^i),y^i)=\frac{1}{2}(h_\theta(x^i)-y^i)^2$

理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将${h_\theta}\left( x \right)=\frac{1}{1+{e^{-\theta^{T}x}}}$带入到这样定义了的代价函数中时，我们得到的$J(\theta)$将是一个非凸函数（**non-convexfunction**）。这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

![image-20210924083551859](E:\md笔记\PyTorch\images\image-20210924083551859.png)

因此，我们重新定义**逻辑回归**的代价函数为：

![image-20210926125910233](E:\md笔记\PyTorch\images\image-20210926125910233.png)

${h_\theta}\left( x \right)$与 $Cost\left( {h_\theta}\left( x \right),y \right)$之间的关系如下图所示：

![image-20210924084343407](E:\md笔记\PyTorch\images\image-20210924084343407.png)

![image-20210924090216113](E:\md笔记\PyTorch\images\image-20210924090216113.png)

这样构建的$Cost\left( {h_\theta}\left( x \right),y \right)$函数的特点是：

当 $y=1$ 且${h_\theta}\left( x \right)$也为 1 时，代价为 0；

当 $y=1$ 但${h_\theta}\left( x \right)$不为1时，代价随着${h_\theta}\left( x \right)$变小而变大。

当 $y=0$ 且${h_\theta}\left( x \right)$也为 0 时，代价为 0；

当 $y=0$ 但${h_\theta}\left( x \right)$不为 0时，代价随着 ${h_\theta}\left( x \right)$的变大而变大。

> $y$是x的实际所属类别，$h_\theta(x)$是预测得到的类别。

我们所选的这样一个$Cost$函数会给我们一个凸优化问题，即整体的代价函数$J(\theta)$将会是一个凸函数，从而只有一个全局最优值，而不会出现多个局部最优值。

## 2.4 简化代价函数与梯度下降

将上面构建的 $Cost\left( {h_\theta}\left( x \right),y \right)$简化成下式： $Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$ .

> 解释：我们已经知道，y的取值只有0、1。则：
>
> - if $y=1$, then $Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-0$
> - if $y=0$, then $Cost\left( {h_\theta}\left( x \right),y \right)=0-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$

带入到总的代价函数中得到： $J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$ 

> Python代码实现：
>
> ```python
> import numpy as np
>     
> def cost(theta, X, y):
>     
>   theta = np.matrix(theta)
>   X = np.matrix(X)
>   y = np.matrix(y)
>   first = np.multiply(-y, np.log(sigmoid(X* theta.T)))
>   second = np.multiply((1 - y), np.log(1 - sigmoid(X* theta.T)))
>   return np.sum(first - second) / (len(X))
> ```

即：

$J\left( \theta \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)+\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$

> 我们为什么可以把代价函数写成这种形式？似乎我们也可以选择别的代价函数。在本课程中，我们不介绍有关这个问题的细节。这个式子是从统计学的**极大似然法**中得来的，它是统计学中为不同模型快速寻找参数的方法，同时它还有一个很好的性质——它是一个凸函数。因此，这就是大部分人用来拟合logistic回归模型的代价函数。

根据这个代价函数，我们要做的就是找出让$J(\theta)$取得最小值时的参数$\theta$。**最小化代价函数**的方法是使用**梯度下降法**：

![image-20210924101825871](E:\md笔记\PyTorch\images\image-20210924101825871.png)

即：

**Repeat { **

$\theta_j := \theta_j - \alpha \frac{1}{m}\sum\limits_{i=1}^{m}{{\left( {h_\theta}\left( \mathop{x}^{\left( i \right)} \right)-\mathop{y}^{\left( i \right)} \right)}}\mathop{x}_{j}^{(i)}$        **(simultaneously update all $\theta_j$**  ) 

**}**

> **【求导过程】**
>
> 已知 $J\left( \theta \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)+\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$ ，
>
> 又考虑到： ${h_\theta}\left( {{x}^{(i)}} \right)=\frac{1}{1+{{e}^{-{\theta^T}{{x}^{(i)}}}}}$ 
>
> 则： ${{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)+\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)$ $={{y}^{(i)}}\log \left( \frac{1}{1+{{e}^{-{\theta^T}{{x}^{(i)}}}}} \right)+\left( 1-{{y}^{(i)}} \right)\log \left( 1-\frac{1}{1+{{e}^{-{\theta^T}{{x}^{(i)}}}}} \right)$ $=-{{y}^{(i)}}\log \left( 1+{{e}^{-{\theta^T}{{x}^{(i)}}}} \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1+{{e}^{{\theta^T}{{x}^{(i)}}}} \right)$
>
> 所以： $\frac{\partial }{\partial {\theta_{j}}}J\left( \theta \right)=\frac{\partial }{\partial {\theta_{j}}}[-\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( 1+{{e}^{-{\theta^{T}}{{x}^{(i)}}}} \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1+{{e}^{{\theta^{T}}{{x}^{(i)}}}} \right)]}]$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\frac{-x_{j}^{(i)}{{e}^{-{\theta^{T}}{{x}^{(i)}}}}}{1+{{e}^{-{\theta^{T}}{{x}^{(i)}}}}}-\left( 1-{{y}^{(i)}} \right)\frac{x_j^{(i)}{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}}]$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}[{{y}^{(i)}}\frac{x_j^{(i)}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}-\left( 1-{{y}^{(i)}} \right)\frac{x_j^{(i)}{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}]$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{{{y}^{(i)}}x_j^{(i)}-x_j^{(i)}{{e}^{{\theta^T}{{x}^{(i)}}}}+{{y}^{(i)}}x_j^{(i)}{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}}$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{{{y}^{(i)}}\left( 1\text{+}{{e}^{{\theta^T}{{x}^{(i)}}}} \right)-{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}x_j^{(i)}}$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{({{y}^{(i)}}-\frac{{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}})x_j^{(i)}}$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{({{y}^{(i)}}-\frac{1}{1+{{e}^{-{\theta^T}{{x}^{(i)}}}}})x_j^{(i)}}$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{{y}^{(i)}}-{h_\theta}\left( {{x}^{(i)}} \right)]x_j^{(i)}}$ 
>
> $=\frac{1}{m}\sum\limits_{i=1}^{m}{[{h_\theta}\left( {{x}^{(i)}} \right)-{{y}^{(i)}}]x_j^{(i)}}$

逻辑回归的梯度下降算法与线性回归的梯度下降算法的区别在于${h_\theta}\left( x \right)$不同。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。

现在你知道如何实现逻辑回归算法了，这是一种非常强大、甚至可能世界上使用最广泛的一种分类算法。

## 2.5 高级优化

本节，我们将学习一些高级优化算法和一些高级的优化概念，利用这些方法，我们能够使通过梯度下降进行逻辑回归的速度大大提高，而这也将使算法更加适合解决大型的机器学习问题，比如，我们有数目庞大的特征量。 

现在我们换个角度来看什么是梯度下降。我们有个代价函数$J\left( \theta \right)$，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数 $\theta$ 时，它们会计算出两样东西：$J\left( \theta \right)$ 以及$J$ 等于 0、1直到 $n$ 时的偏导数项。那么梯度下降所做的是：反复执行$\theta$的更新代码。

![image-20210924124744793](E:\md笔记\PyTorch\images\image-20210924124744793.png)

> 另一种考虑梯度下降的思路是：我们需要写出代码来计算$J\left( \theta \right)$ 和这些偏导数，然后把这些插入到梯度下降中，然后它就可以为我们最小化这个函数。 对于梯度下降来说，我认为从技术上讲，你实际并不需要编写代码来计算代价函数$J\left( \theta \right)$。你只需要编写代码来计算导数项，但是，如果你希望代码还要能够监控这些$J\left( \theta \right)$ 的收敛性，那么我们就需要自己编写代码来计算代价函数$J(\theta)$和偏导数项$\frac{\partial }{\partial {\theta_j}}J\left( \theta \right)$。

梯度下降并不是唯一的可以优化代价函数的算法，还有更高级、更复杂、更高效的算法，如**共轭梯度法 BFGS** (**变尺度法**) 和**L-BFGS** (**限制变尺度法**) 等，这里不做详细介绍，仅介绍一点：它们都不需要手动设置学习率 $\alpha$。这些算法的实现使用了一个叫**线性搜索**(**line search**)的算法，它可以自动尝试不同的学习速率 $\alpha$，并自动选择一个好的学习速率 $a$，它甚至可以为每次迭代选择不同的学习速率。当然，这些算法的优点远不止于此。

> 吴恩达：“我过去使用这些算法已经很长一段时间了，也许超过十年了，使用得相当频繁，而直到几年前我才真正搞清楚**共轭梯度法 BFGS** 和 **L-BFGS**的细节。事实上，完全可以不真正理解这些算法的内部做了些什么，而成功使用这些算法，并将其应用于许多不同的学习问题。如果说这些算法有缺点的话，那么我想说主要缺点是它们比梯度下降法复杂多了，特别是你最好不要使用 **L-BGFS**、**BFGS**这些算法，除非你是数值计算方面的专家。实际上，我不会建议你们编写自己的代码来计算数据的平方根，或者计算逆矩阵，因为对于这些算法，我还是会建议你直接使用一个软件库，比如说，要求一个平方根，我们所能做的就是调用一些别人已经写好用来计算数字平方根的函数。幸运的是现在我们有**Octave** 和与它密切相关的 **MATLAB** 语言可以使用。”

**Octave** 有一个非常理想的库用于实现这些先进的优化算法，所以，如果你直接调用它自带的库，你就能得到不错的结果。我必须指出这些算法实现得好或不好是有区别的，如果你使用的是其他语言（如**C**、**C++**、**Java**等）的库，你可能需要多试几个库才能找到一个能很好实现这些算法的库。

现在让我们来说明如何使用这些算法。下图是一个含两个参数${\theta_{1}}$和${\theta_{2}}$的示例：

![image-20210924154831933](E:\md笔记\PyTorch\images\image-20210924154831933.png)

如果你将$J\left( \theta \right)$ 最小化的话，那么它的最小值将是${\theta_{1}}=5$ ，${\theta_{2}}=5$。下面我们用比梯度下降算法更高级的算法来最小化$J\left( \theta \right)$ ，并找到对应的最佳的${\theta}$值。

首先定义如下函数：

```matlab
function [jVal, gradient]=costFunction(theta)
    
　　jVal=(theta(1)-5)^2+(theta(2)-5)^2;
    
　　gradient=zeros(2,1);
    
　　gradient(1)=2*(theta(1)-5);  % Octave里的下标从1开始
    
　　gradient(2)=2*(theta(2)-5);
    
end
```

该函数的第一个返回值是$J(\theta)$，第二个返回值是一个2×1的梯度向量，该梯度向量的两个元素对应这里的两个偏导数项。

然后，就可以调用**Octave**里的一个高级优化函数**fminunc**（**Octave** 里的无约束最小化函数）了。在调用这个函数之前，需要先设置几个**options**： 这里**GradObj** 和**On**表示设置梯度目标参数为打开(**on**)，这意味着你现在确实要给这个算法提供一个梯度，然后设置最大迭代次数**MaxIter**，比方说100。然后我们给出一个$\theta$ 的猜测初始值，它是一个2×1的向量。最后，调用**fminunc**。

```matlab
>> options=optimset('GradObj','on','MaxIter',100);

>> initialTheta=zeros(2,1);
    
>> [optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options);  % @符号表示指向我们刚刚定义的costFunction函数的指针
```

总结——一个返回$J(\theta)$、梯度向量的函数：

```matlab
function [jVal, gradient]=costFunction(theta)
    
　　jVal=[计算J(\theta)的代码];
    
　　gradient(1)=[计算J(\theta)对(\theta)_0的偏导数];  % Octave里的下标从1开始
    
　　gradient(2)=[计算J(\theta)对(\theta)_1的偏导数];
　　.
　　.
　　.
　　gradient(n+1)=[计算J(\theta)对(\theta)_n的偏导数];
    
end
```

这个函数既可以应用到逻辑回归，也可以应用到线性回归，只需要编写合适的代码即可。

现在你已经知道如何使用这些高级的优化算法，有了这些算法，你就可以使用一个复杂的优化库，虽然它让算法使用起来更模糊一点，也稍微有点难调试，不过由于这些算法的运行速度通常远远超过梯度下降，所以当我有一个很大的机器学习问题时，我会选择这些高级算法，而不是梯度下降。有了这些概念，你就应该能将逻辑回归和线性回归应用于更大的问题中，这就是**高级优化**的概念。

在下一个视频，将讲解如何修改你已经知道的逻辑回归算法，使它在多类别分类问题中也能正常运行。

## 2.6 多元分类：一对多

> 逻辑回归算法解决多类别分类问题。

我们现在已经知道如何进行二元分类，可以使用逻辑回归。用一对多的分类思想，我们可以将其用在多类分类问题上。下面将介绍如何进行一对多的分类工作，有时这个方法也被称为**"一对余"方法**。

现在我们有一个训练集，有3个分类类别，我们用三角形表示 $y=1$，方框表示$y=2$，叉叉表示 $y=3$：

![image-20210924165423722](E:\md笔记\PyTorch\images\image-20210924165423722.png)

我们下面要做的就是使用一个训练集，将其分成3个二元分类问题。

实际上我们可以创建一个新的"伪"训练集。我们先设定**Class 1**为正类，**Class 2**和**Class 3**为负类，如下右图所示。然后，我们要拟合出一个适合这个“新的训练集”的分类器。

![image-20210924165651606](E:\md笔记\PyTorch\images\image-20210924165651606.png)

为了能实现这样的转变，我们将多个类中的一个类标记为正向类（$y=1$），然后将其他所有类都标记为负向类，这个模型记作$h_\theta^{\left( 1 \right)}\left( x \right)$。接着，我们选择另一个类标记为正向类（$y=2$），再将其它类都标记为负向类，将这个模型记作 $h_\theta^{\left( 2 \right)}\left( x \right)$，依此类推。 最后我们得到一系列的模型简记为： $h_\theta^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta \right)$，其中：$i=\left( 1,2,3....k \right)$。最后，对这几种情况分别进行训练。

为了做出预测，我们需要对这些模型分别输入一个测试值$x$，然后选择一个让 $h_\theta^{\left( i \right)}\left( x \right)$ 最大的$ i$，从而$x$就属于$i$对应的这个正向类。

# 3 正则化(Regularization）

## 3.1 过拟合

回归问题：

![image-20210924172129477](E:\md笔记\images\image-20210924172129477.png)

分类问题：

![image-20210924172144782](E:\md笔记\images\image-20210924172144782.png)

以上的第三种情况都属于过拟合。如何解决过拟合问题？

1. 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如**PCA**）。
2. 正则化。 保留所有的特征，但是减少参数的大小（**magnitude**）。

> 当数据集小，而拥有的特征又很多时，就容易出现过拟合。

## 3.2 代价函数

> 正则化是怎样运行的？当我们进行正则化的时候，我们还将写出相应的代价函数。

如下图，当模型为 ${h_\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}+{\theta_{4}}{x_{4}^4}$ 时，产生了过拟合。

![image-20210924173734437](E:\md笔记\images\image-20210924173734437.png)

容易理解，是那些高次项导致了过拟合问题。所以我们希望能让高次项的系数接近于0。我们决定要减少${\theta_{3}}$和${\theta_{4}}$的大小，我们要做的便是修改代价函数——加入**惩罚项**，使得${\theta_{3}}$和${\theta_{4}}$都非常小。【注意，其实只要提到“惩罚”，就是使它变小】

我们要最小化均方误差代价函数：

${min}_{\theta}\frac{1}{2m}[\sum\limits_{i=1}^{m}{{{\left( {{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}} \right)}^{2}}]}$

修改后的代价函数如下：${min}_{\theta}\frac{1}{2m}[\sum\limits_{i=1}^{m}{{{\left( {{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}} \right)}^{2}}+1000\theta _{3}^{2}+1000\theta _{4}^{2}]}$

> 式子中1000只是随便一个比较大的数。

假如我们要使这个修改后的函数尽可能小，那么就要使${\theta_{3}}$和${\theta_{4}}$都尽可能小（应接近于0），这是因为它们的系数都设置的很大了。这样，${\theta_{3}}$和${\theta_{4}}$都得接近于0，即我们拟合数据的函数${h_\theta}\left( x \right)$实际上就是一个二次函数加上了一些非常小的项，也就还是相当于还是一个二次函数。

正则化的思想：给参数加惩罚项，从而使的该参数所在的这一项尽可能小，从而简化了模型。

> 结果表明，参数越小，我们得到的函数就会越平滑，也越简单，因此也更不容易出现过拟合的问题。

假如我们有非常多的特征，我们并不知道其中哪些特征是相关度较低的（我们希望对相关性较低的特征进行惩罚，即使它的权重变小），那么我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择惩罚程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：

$J\left( \theta \right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}{{{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}+\lambda \sum\limits_{j=1}^{n}{\theta_{j}^{2}}]}$

其中，$\lambda \sum\limits_{j=1}^{n}{\theta_{j}^{2}}$就是**正则化项**，$\lambda$被称为**正则化参数**，其作用是控制两个不同目标之间的取舍【注：根据惯例，我们不对${\theta_{0}}$ 进行惩罚】：第一个目标是更好地拟合数据，这与$\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$项有关；第二个目标是保持参数尽量的小，这与正则化项有关。**$\lambda$可以用来控制这两个目标之间的平衡关系，从而避免过拟合。**

当我们用一个高阶多项式拟合数据时出现了过拟合现象时，如果你依然想保留所有特征的高阶项，你只要确保利用上面的正则化目标（在$J(\theta)$上添加正则化项），就能得到一个拟合的比较好的曲线。经过正则化处理的模型与原模型的可能对比如下图所示：

![image-20210924200001420](E:\md笔记\images\image-20210924200001420.png)

> 可以发现，正则化后的曲线更平滑、更简单。

为什么缩小参数就能达到这样的效果？这确实不太好理解。但如果你亲自用正则化去实现算法，你就能直观的看到这种效果。

不过，如果设置的正则化参数$\lambda$ 过大，则表明对这些参数的惩罚程度太大，那么这些参数都将接近于0，导致模型变成 ${h_\theta}\left( x \right)={\theta_{0}}$（我们通常不对${\theta}_0$进行惩罚），上图中红色直线表示这种情况，导致欠拟合。 

> - 为什么增加正则化项$\lambda\sum\limits_{j=1}^{n}{\theta_j^{2}}$ 可以使$\theta $的值减小？ 
>
> - <font color="red">因为如果我们令 $\lambda$ 的值很大的话，为了使 **Cost Function** 尽可能的小，所有的 $\theta $ 的值（不包括 ${\theta_{0}}$）都会自动的在一定程度上减小。 当 $\lambda$ 的值设置的很大很大，那么 $\theta $（不包括${\theta_{0}}$）就会趋近于0。 </font> 

下面，我们将把这些概念应用到到线性回归和逻辑回归中来避免过拟合。

## 3.3 线性回归的正则化

对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。下面我们会将这两种算法推广到正则化线性回归中。

正则化线性回归的代价函数为：

$J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{[({{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2}})]}$

下面我们想使用梯度下降法为这个增加了正则化项的代价函数找到合适的$\theta$使其最小化。

> 使用梯度下降法在没有正则化的情况下最小化代价函数时使用的算法：
>
> $Repeat$ {
>
>  ${\theta_j}:={\theta_j}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}$           $(j=0,1,2,...n)$
>
>  }
>
> 注意，$\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}$项是$J(\theta)$对$\theta$的导数。

首先，因为我们不对$\theta_0$进行正则化，所以增加了正则化项的梯度下降算法将分两种情形：

$Repeat$ $until$ $convergence${

 ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$

 ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]={\theta_j}(1-a\frac{\lambda }{m})-a\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}$   ( $j=1,2,...n$)

 }

> 注意，$[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]$是加入了正则化项的$J(\theta)$对$\theta$的导数。

<font color="red">可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令 $\theta $ 值减少了一个额外的值：$a\frac{\lambda }{m}$。【注意：学习率 $a$ 很小且为正数，$m$ 很大，因此 $a\frac{\lambda }{m}$ 很小】</font>  

我们同样也可以利用正规方程（令偏导数都为0，求得极值）来求解正则化线性回归模型，方法如下所示：

![image-20210924221938758](E:\md笔记\images\image-20210924221938758.png)

<img src="E:\md笔记\images\image-20210924221718302.png" alt="image-20210924221718302" style="zoom:67%;" />

这样得到的 $\theta$ 就能使代价函数最小。图中的矩阵尺寸为 $(n+1)*(n+1)$。【注意，m为样本数，n为特征数】

## 3.4 逻辑回归的正则化

针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：1. 使用梯度下降法来优化代价函数$J\left( \theta \right)$，2. 更高级的优化算法，这些高级优化算法需要你自己设计代价函数$J\left( \theta \right)$。本节将会对这两种优化算法进行改进，使它们能够用于正则化逻辑回归中。

逻辑回归也会出现过拟合的问题。当使用下图右侧的式子拟合数据时，就会出现左侧过拟合的结果：

![image-20210924222903941](E:\md笔记\PyTorch\images\image-20210924222903941.png)

通常情况下，如果你的逻辑回归有很多特征，有无关紧要的多项式，这些大量的特征最终会导致过拟合的现象。

逻辑回归的代价函数：

$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$

增加正则化项：

$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}+\frac{\lambda }{2m}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$

这样做的效果是，**即使当你拟合的$h_\theta(x)$的阶数很高且参数很多，只要添加了这个正则化项，就可以保持参数较小，从而得到一条合适的判断边界**。

我们如何实现它？

要想最小化这个增加了正则化项的代价函数，对应的梯度下降算法应为：

$Repeat$ $until$ $convergence${

 ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$

 ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]$   ( $j=1,2,...n$)

 }

看上去同线性回归一样，但是知道 ${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)=\frac{1}{1+e^{-e^Tx}}$这里是与线性回归不同的。

下面我们讲解如何在更高级的优化算法中使用正则化。提醒一下，对于这些高级算法，我们需要自己定义一个`costFunction`函数（见上面），这个函数以参数向量$\theta$作为输入；然后将`costFunction`函数赋给`fminuc`函数（或其他高级优化算法），这将最小化新的正则化代价函数$J(\theta)$，而函数返回的参数代表的就是正则化逻辑回归的解。

目前大家对机器学习算法可能还只是略懂，但是一旦你精通了线性回归、逻辑回归、高级优化算法和正则化技术，坦率地说，你对机器学习的理解就已经超过大多数人了。你已经学了很多知识，并可以运用它们来解决实际问题。

