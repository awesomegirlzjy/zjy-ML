# *10* 如何改进你的学习算法

## 10.1 改进算法的几种方法

我们已经学了一些不同的学习算法，但还无法高效的运用它们。下面介绍一些使用的建议和指导，以设计出更好的机器学习系统。

仍然以“预测房价”作为例子。假如你已经完成了正则化线性回归，也就是计算得到最小化代价函数 $J{(\theta)}$ 的值：

$J\left( \theta \right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}{{{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}+\lambda \sum\limits_{j=1}^{n}{\theta_{j}^{2}}]}$

得到学习参数后，当将这个假设函数放到一组新的房屋样本上进行预测时，发现预测结果产生了巨大的误差。那么接下来怎么办呢？

改进算法的方法有以下几种：

- **使用更多的训练样本**；

> 但有时候更多的训练数据实际上并没有作用。在接下来的几段视频中，我们将解释原因。另外，我们非常希望在花费大量时间完成诸如收集数据的任务之前，我们就能知道那样做的效果如何。

- 尝试**选用更少/多的特征**
- 尝试**增加多项式特征**（如$x_1^2$，$x_2^2$，$x_1*x_2$，等等）
- **减小/增大正则化参数 $\lambda$ 的值**

上面的很多方法都可以扩展成一个六个月或更长时间的项目。遗憾的是，大多数人的选择方法是随便从这些方法中选择一种，比如他们会说“噢，我们来多找点数据吧”，然后花上六个月的时间收集了一大堆数据；然后也许另一个人说：“好吧，让我们来从这些房子的数据中多找点特征吧”。我很遗憾不止一次地看到很多人花了至少六个月时间来完成他们随便选择的一种方法，而在六个月或者更长时间后，他们很遗憾地发现结果并不理想。幸运的是，有一系列简单的方法能让你事半功倍，排除掉单子上的至少一半不太有效的方法，从而为你节省大量不必要花费的时间。

在接下来的两段视频中，我首先介绍**怎样评估机器学习算法的性能**；再之后的几段视频，我们将讨论**机器学习诊断法**。“诊断法”是一种测试法，通过执行这种测试，能够了解算法在哪里出了问题，深入了解某种算法到底是否有用。不过，理解诊断法的执行和实现，需要花很多时间，但这样做的确是把时间用在了刀刃上，因为诊断法能让你在开发学习算法时，节省几个月的时间，提早发现某些方式是无效的。

## 10.2 评估你的假设模型

本节讲解：如何评价你的学习算法学习到的假设。

我们知道，仅仅因为假设函数具有很小的训练误差（在训练集上的误差），并不能说明它就一定是一个好的假设函数。

如何判断一个假设函数是否过拟合？对于特征少的例子，我们可以画出假设函数$h(x)$的图像，然后观察图形趋势；但对于特征变量有很多的情况，想要通过画出假设函数来进行观察，就会变得很难甚至不可能实现。因此，我们需要其他评估方法。

为了检验算法是否过拟合，我们将数据分成**训练集**和**测试集**，通常用**70%**的数据作为训练集，用剩下**30%**的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，使数据随机分布，然后再分成训练集和测试集。

1. 训练和测试线性回归问题的学习算法：

- 首先，对**训练集**进行学习以得到最小化 $J({\theta})$ 对应的参数 ${\theta}$ ；

- 然后计算出**测试误差** $J_{test}({\theta})=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_{\theta}(x_{test}^{(i)})-y_{test}^{(i)})^2$

2. 训练和测试逻辑回归问题的学习算法：

- 首先，对**训练集**进行学习以得到最小化 $J({\theta})$ 对应的参数 ${\theta}$ ；
- 然后计算出**测试误差** $J_{test}({\theta})=-\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}y_{test}^{(i)}logh_{\theta}(x_{test}^{(i)})+(1-y_{test}^{(i)})logh_{\theta}(x_{test}^{(i)})$

还有另一种可能更易于理解的形式的测试度量，叫做**错误分类**，也被称为**0/1分类错误**（0、1表示预测得到的分类是正确还是错误）【就是**错误率**呀】：

![image-20210928153130853](E:\md笔记\PyTorch\images\image-20210928153130853.png)

## 10.3 模型选择和交叉验证集

如何确定学习模型的多项式次数？如何选择学习算法中的正则化参数 ？这类问题叫做**模型选择问题**。对于这类问题，所涉及的不仅仅是把数据分为训练集和测试集，而是应该将数据分为三个数据组：**训练集**、**验证集**和**测试集**。

### 10.3.1 确定多项式次数 $d$

现在，我们想要确定模型的多项式次数：

![image-20210928160018462](E:\md笔记\images\image-20210928160018462.png)

<font color="red">Step1.</font> 将数据集分成**训练集(Train set)**、**交叉验证集(Cross validation set**)和**测试集(Test set)**，通常按照 $6:2:2$ 的比例进行分配。

<font color="red">Step2.</font> 选择第一个模型，最小化**训练误差**得到一个参数向量 ${\theta}^{(1)}$；接着再选择第二个模型来拟合训练集，得到另一个参数向量${\theta}^{(2)}$... 以此类推，直到得到${\theta}^{(10)}$。

<font color="red">Step3.</font> 对所有这些模型分别求出**验证集误差**：$J_{cv}({\theta}^{(1)})$, $J_{cv}({\theta}^{(2)})$, ..., $J_{cv}({\theta}^{(10)})$.

> $J_{train}(\theta) = \frac{1}{2m}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$
>
> $J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_\limits{i=1}^{m_{cv}}(h_{\theta}(x^{(i)}_{cv})-y^{(i)}_{cv})^2$
>
> $J_{test}(\theta)=\frac{1}{2m_{test}}\sum_\limits{i=1}^{m_{test}}(h_{\theta}(x^{(i)}_{test})-y^{(i)}_{test})^2$

<font color="red">Step4.</font> 验证误差最小的那个多项式就是我们要选择的。在**测试集**上评估它的泛化能力。

> - 有些人仍然只把数据分成训练集和测试集，这样就是用测试集选择模型，同时又直接用测试集得出误差，也许这样做不会很糟，但大多数的机器学习的实践者还是建议不要这样做！也就是说，还是要把数据分成训练集、验证集、测试集。

### 10.3.2 确定正则化参数 ${\lambda}$

假设我们想要从 $0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$ 里选出一个合适的${\lambda}$ 值。步骤如下：

> ${\lambda}$ 的待选值通常设置为 0-10之间的呈现 2 倍关系的值。

![image-20210930100940536](E:\md笔记\images\image-20210930100940536.png)

<font color="red">Step1.</font> 把数据分为训练集、交叉验证集和测试集。

<font color="red">Step2.</font> 使用**训练集**分别在这几种情况下最小化代价函数 $J({\theta})$, 得到对应的参数向量 ${\Theta}$. 

<font color="red">Step3.</font> 使用**交叉验证集**对这几个模型进行评价，即计算验证集误差。

<font color="red">Step4.</font> 选择使交叉验证集误差最小的那个 ${\lambda}$，并使用**测试集**对最终模型进行评价（即计算测试集误差）。

![image-20210930101659431](E:\md笔记\images\image-20210930101659431.png)

### 10.3.3 确定神经网络的隐藏层数目

在9.6节中，我们提到了隐藏层数目的设置，这里再补充一点：如果想尝试让隐藏层的数目不为1，那么就把数据分割为**训练集**、**验证集**、**测试集**后，训练含1个隐藏层的神经网络，然后再试试训练含2个隐藏层的神经网络... 多试几种。然后看看哪一种在交叉验证集上表现得最理想（选择验证误差最小的）。最后使用测试集查看选中的模型的泛化能力。

## 10.4 偏差、方差与多项式次数

> **偏差：**描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。
>
> **方差：**描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。
>
> ![img](https://pic2.zhimg.com/80/162bbe3ae6c8f46da4f4e05edea2d9fc_1440w.jpg?source=1940ef5c)
>
> - 方差，形容数据分散程度，算是“无监督的”，客观的指标；偏差，形容数据跟我们期望的中心差得有多远，算是“有监督的”，有人的知识参与的指标。
> - 当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是**偏差比较大**——**欠拟合**，要么是方差比较大——**过拟合**。

**Training error:** $J_{train}(\theta) = \frac{1}{2m}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$

**Cross Validation error:** $J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)}_{cv})-y^{(i)}_{cv})^2$

训练误差、验证误差与**多项式模型的次数 $d$ **呈现的关系：

![image-20210930091324843](E:\md笔记\PyTorch\images\image-20210930091324843.png)

![image-20210930091211527](E:\md笔记\PyTorch\images\image-20210930091211527.png)

> - 对于训练集，当 $d$ 较小时，模型拟合程度更低，误差较大；随着 $d$ 的增长，拟合程度提高，误差减小。
>
> - 对于交叉验证集，当 $d$ 较小时，模型拟合程度低，误差较大；但是随着 $d$ 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。

由上图可发现，当交叉验证集误差较大时，可能是因为 $d$ 过小，也可能是因为 $d$ 过大，那么我们如何确定问题到底是因为 $d$ 过小我们而导致的，还是因为 $d$ 过大而导致的？事实上， $d$ 过小意味着偏差过高， $d$ 过大意味着方差过高。

再由上图可以发现：

- $d$ 过小 ---> 训练集误差很大，且验证集误差 ≈ 训练集误差   ---> 高偏差/欠拟合
- $d$ 过大 ---> 训练集误差很小，且验证集误差 >> 训练集误差 ---> 高方差/过拟合

## 10.5 偏差、方差与正则化

> 正则化是如何影响偏差、方差的？

假设我们要对这个高阶多项式进行拟合：

$$h_{\theta}(x)={\theta}_0+{\theta}_1x+{\theta}_2x^2+{\theta}_3x^3+{\theta}_4x^4$$

为了防止过拟合，我们使用正则化（通过一个正则化项来让参数的值尽可能小）：

$$J({\theta})=\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^m{\theta}_j^2$$

-- --

正则化参数 ${\lambda}$ 与拟合结果 $h_{\theta}(x)$ 之间的关系：

![image-20210930095946597](E:\md笔记\images\image-20210930095946597.png)

- 当 ${\lambda}$ 很大时，所有参数/权重 ${\theta}_0$, ${\theta}_1$, ${\theta}_2$, ${\theta}_3$, ${\theta}_4$ 都将被惩罚很重，从而使得这些参数都很接近于0，从而假设函数 $h_{\theta}(x)$ 接近于0，将产生**欠拟合/高偏差**的问题。

- 当 ${\lambda}$ 很小时，就相当于没有进行正则化，将产生**过拟合/高方差**的问题。

-- --

正则化参数 ${\lambda}$ 与训练误差、验证误差的关系：

![image-20210930103337059](E:\md笔记\images\image-20210930103337059.png)

- ${\lambda}$ 过小 ---> 相当于未正则化处理 ---> 参数/权重未被惩罚 ---> 容易导致过拟合/高方差（如本节第1幅图所示）---> 训练集误差小，验证集误差大
- ${\lambda}$ 过大 ---> 参数/权重被惩罚很重 ---> 参数/权重会很小 ---> 容易导致欠拟合/高偏差（如本节第1幅图所示）---> 训练集误差大，验证集误差大

## 10.6 学习曲线

学习曲线是将**训练集误差**和**交叉验证集**误差作为**训练集样本数量 $m$** 的函数绘制的曲线，通常呈现出下图所示的趋势：

![image-20210930105110767](images/image-20210930105110767.png)

> **【规律】**随着训练集容量的增大，训练误差将增大（因为数据越多，越不容易把这些数据拟合在一起），而验证集误差和测试集误差将减小（因为使用的数据越多，泛化能力会越好）。

学习曲线能够诊断学习算法是在偏差上出现问题还是在方差上出现问题还是两者都有问题。

-- --

### 10.6.1 高偏差情况下的学习曲线

> 模型 $h_{\theta}(x)$ 出现高偏差问题

设假设函数 $h_{\theta}(x)={\theta}_0 + {\theta}_1x$， 样本少、样本多的情况下拟合出来的函数的图像如下图右列所示。则高偏差情况下的学习曲线为下图左列所示：

![image-20210930112510034](E:\md笔记\images\image-20210930112510034.png)

学习曲线表明：训练集样本容量很小时，验证集误差将很大；训练集样本数增大到某个值时，可以找到最有可能拟合数据的那条直线；继续增大样本数量，拟合结果将几乎不再改变。训练误差一开始很小，然后会逐渐增大，最后接近交叉验证误差（之所以“接近”，是因为数据很多但我们的这个假设函数的参数很少）。

高偏差的问题可以由很高的交叉验证误差和训练误差反映出来，也就是说，最终会得到一个值比较大的训练误差和验证误差。进一步，如果一个学习算法有高偏差，随着我们增加训练样本，交叉验证误差会基本不变，这说明，**在高偏差问题下，增加训练集样本数并不会改善算法**。

### 10.6.2 高方差情况下的学习曲线

> 模型 $h_{\theta}(x)$ 出现高方差问题

设假设函数 $h_{\theta}(x)={\theta}_0 + {\theta}_1x+...+{\theta}_{100}x^{100}$(且正则化参数 ${\lambda}$ 很小)，样本少、样本多的情况下拟合出来的函数的图像如下图右列所示。则高方差情况下的学习曲线为下图左列所示：

![image-20210930115213510](E:\md笔记\images\image-20210930115213510.png)

学习曲线表明：训练样本越多，越难以把训练集数据拟合的很好，因此训练误差逐渐增大，但总的来说训练集误差还是很小。在高方差情形中，假设函数对数据过拟合，因此交叉验证误差将会一直很大。

高方差情形下，训练误差和验证误差之间差距很大。这表明，如果我们**考虑增大训练集的样本数**，那么这两条曲线实际上会逐渐靠近，进一步，**验证误差和测试误差都是会减小的，所以这有助于我们优化算法**。

-- --

### 10.6.3 学习曲线的指导作用

回顾10.1节中提到的集中改进方法：

1. 获得更多的训练样本——解决高方差
2. 尝试减少特征的数量——解决高方差
3. 尝试获得更多的特征——解决高偏差
4. 尝试增加多项式特征($x_1^2, x_2^2, x_1x_2, etc$)——解决高偏差
5. 尝试减少正则化程度λ——解决高偏差
6. 尝试增加正则化程度λ——解决高方差

> 对于神经网络来说，隐藏层很少、每一层中含有的隐藏单元很少，这表明要学习的权重少，容易出现欠拟合问题；隐藏层很多、每一层中含有的隐藏单元很多，这表明要学习的权重多，容易出现过拟合问题。通常，越大型的网络，其性能就会越好。

# 11 设计一个机器学习系统

本章，将学习机器学习系统的设计——在设计复杂的机器学习系统时所面临的主要问题和建议。这部分内容在数学方面涉及的不多，但却非常有用，能使你在构建大型的机器学习系统时节约大量时间。

## 11.1 确定执行的优先级

首要问题：在实际工作过程中，我们应该优先处理哪些事情？

以垃圾邮件分类为例。设垃圾邮件的分类标签 $y=1$，非垃圾邮件的分类标签为 $y=2$, 那么如何通过监督学习的方法来构造一个分类器呢？

为了应用监督学习，<font color="red">首先</font>要想的是如何来表示邮件的特征向量 $x$, 通过特征向量 $x$ 和分类标签 $y$, 我们就能训练出一个分类器，比如使用逻辑回归的方法。

这里有一个选择邮件的特征向量的方法，我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），那么这个列表的尺寸为100×1。

如何在有限的时间内让你的垃圾邮件分类器具有高精准度和低错误率，以下是一些方法：

- 收集大量的数据（垃圾邮件和非垃圾邮件的样本）
- 用更复杂的特征变量来描述邮件标题/头部（比如将发件人的信息包括其中）
- 用更复杂的特征变量来描述邮件主体/内容（比如关注大小写、标点符号）
- 开发识别刻意拼写错误的算法（如把**watch** 写成**w4tch**）

如何做出明智的选择？后面将讲解**误差分析**的思想，告诉你怎样用更加系统性的方法，做出最明智的选择。

## 11.2 误差分析

本节，将讲述**误差分析**（**Error Analysis**）的概念。这会帮助你更系统地做出决定。

每当我开始一个机器学习问题时，我最多只会花一天的时间，简单粗暴的把它做出来（即便效果不好），然后通过<font color="blue">交叉验证集</font>来检验数据，再然后通过画出**学习曲线**以及计算**测试误差**，来找出算法是否存在高偏差和高方差的问题，或者别的问题。在做出这些分析后，再来决定是否使用更多的数据或特征。【这样做逻辑才会清楚，学习的时候也是先从简单的一段内容开始学起，然后再慢慢向其中添加更复杂的内容】

> - 我们应当用证据来指导我们的决策，使我们决定把时间花在哪里，而不是仅凭直觉。

另一个帮助我们发现问题所在的方法是做**误差分析**：当实现比如一个垃圾邮件分类器时，我经常会观察<font color="blue">交叉验证集</font>的情况，看一看其中被算法错误分类的邮件，然后为它们手工分类，目的是找到这些被错误分类的邮件的共同特征和规律。这个过程能启发你构造怎样的新的特征变量，或者告诉你现在这个系统的缺陷，然后指导你想出办法来改进它。

> “我强烈推荐在**交叉验证集**上来做误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。这从数学上讲是不合适的。”

（举例略-视频）

有时，我们希望能够用直白的数据来进行评估——如果你的算法能够返回一个**数值评价指标**来估计算法执行的效果，这将会很有帮助。假如我们正在决定是否应该将**discount/discounts/discounted/discounting**处理成同一个词，那么如果使用误差分析的方法，可能也无法帮助你来决定。最好的方法应该是分别尝试这两种情况——①将**discount/discounts/discounted/discounting**处理成同一个词；②将**discount/discounts/discounted/discounting**处理成不同的词，然后根据数值检验的结果来直观的判断出哪一种更好。具体来说，是计算<font color="blue">交叉验证误差</font>。

## 11.3 偏斜类的误差度量

训练集中有非常多的同一种类的样本，只有很少或没有其他类的样本时，就被称为**偏斜类**（**skewed classes**）。这种情况不适合使用测试误差或者错误率等来评估一个模型的好坏。

### 11.3.1 问题所在

例如我们希望用算法来预测癌症是否是恶性的，并令 $y=0$ 表示没有得癌症，$y=1$ 表示得了癌症，且训练集中只有0.5%的分类标签为 $y=1$。

假设我们用测试集检验出某个分类模型的错误率为1%，即我们有99%的可能性是可以正确预测出患者是否得了癌症。

与此同时，我们编写下面这样一个被写“死”的程序：

```octave
function y = predictCancer(x)
	y = 0; % 不论输入是怎样的，都直接输出y=0
return
```

尽管这个程序没有考虑输入，但实际上它只有0.5%的错误率，因为数据集中刚好有99.5%的分类标签为 $y=0$。

这个“死”程序甚至比上面那个错误率为1%的分类模型还要好！！！这种正类数量和负类数量的比例非常接近一个极端的情况，就叫做**偏斜类**。这种情况下，用分类精确度并不能很好地衡量算法。

### 11.3.2 问题解决：查准率和查全率

那么如何评估偏斜类情况下，学习算法的好坏呢？答案是使用**查准率**(Precision)和**召回率**/**查全率**(Recall)。

对于一个二分类问题来说，我们将预测出的结果可以分成四种情况（“预测结果”是指在验证集上进行预测）：

![image-20210930171204122](E:\md笔记\images\image-20210930171204122.png)

**查准率**=**TP/(TP+FP)**。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比。越高越好。

**查全率**=**TP/(TP+FN)**。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比。越高越好。

这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。这样就判断出那个"死"程序其实是很差的，尽管它在测试集上的准确度是高的。

通过计算查准率和查全率，我们能更好地知道分类模型到底好不好，尤其适合作为偏斜类问题中的评估度量值。【我们总是习惯性的用 $y=1$ 来表示出现的次数较少的那一类】

### 11.3.3 查准率和查全率之间的权衡：$F_1$值

在很多应用中，我们希望能够保证查准率和召回率的相对平衡。在这节课中，我将告诉你应该怎么做，同时也向你展示一些查准率和召回率作为算法评估度量值的更有效的方式。

继续沿用刚才预测肿瘤性质的例子。假设我们用逻辑回归模型训练了数据，输出概率在0-1之间，我们使用阀值0.5来预测真和假。

![image-20210930190330734](E:\md笔记\images\image-20210930190330734.png)

如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，那么我们可以使用比0.5更大的阀值，如0.7，0.9。但是这样的回归模型会有较低的查全率，因为当我们做预测的时候，我们只给一小部分的病人预测 $y=1$. 

如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查，那么我们可以使用比0.5更小的阀值，如0.3。

> 通常，我们会在 $h_{\theta}(x) ≥ 临界值$ 的情况下，预测 $y=1$.

我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表（下图中有三条曲线，含义相同，都是**从左到右临界值/阈值减小的情况下，查准率减小[纵坐标]、查全率增大[横坐标]**，只是实际曲线的形状根据数据的不同而不同）：

![image-20210930192317807](E:\md笔记\images\image-20210930192317807.png)

这又产生了另一个有趣的问题：有没有办法自动选取一个合适的临界值？或者，更广泛地说，如果我们有不同的算法或者不同的想法，我们如何比较不同的查准率和查全率呢，如何决定哪一个算法是最好的？我们希望能存在一个对其进行评估的度量值。你可能会尝试使用 $\frac{P+R}{2}$ (查准率和查全率的平均值) 进行“该值越高越好”的评估。但是这并非好的方法。具体见下表中第三种情况：如果我们的回归模型总是预测 $y=1$, 那么将得到很高的查全率，却很低的查准率，这并不是一个好的模型。另一种结合查准率和查全率的度量值被称为**$F_1$值**：$2\frac{PR}{P+R}$. 它的定义会考虑一部分查准率和查全率的平均值，同时会给查准率和查全率中较低的那个值更高的权重。其中的乘积项就确保了P、R必须都较大才能使**$F_1$值**更大。

![image-20210930194455438](E:\md笔记\images\image-20210930194455438.png)

有了**$F_1$值**后，我们就能直接通过编写程序得到效果最好（**$F_1$值**越大，效果越好，即选择的临界值越适合）的那个算法了 ---> 帮助我们自动选择临界值。

## 11.4 多少数据

上面，我们讨论了一些评价指标。现在，我们将讨论机器学习系统设计中另一个重要的方面——用来训练的数据有多少。

在之前的一些视频中，我曾告诫大家不要盲目地花大量的时间来收集大量的数据，因为数据只在一些情况下起作用。但事实证明，在一定条件下（后面会讲到这些条件是什么），使用大量的数据在某种类型的学习算法中进行训练，是一种获得具有良好性能的学习算法的有效方法。

通过确保有一个具有**很多参数**的学习算法来保证“低偏差”+通过确保使用**非常大的训练集**来保证“低方差” ---> 成功得到一个好的学习算法。

