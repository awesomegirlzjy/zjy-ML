# 13 聚类

> "聚类"是一种无监督学习。

## 13.1 K-均值算法

**K-均值（K-Means）**是最普及的一种聚类算法，该算法接受一个未标记的数据集，然后将数据聚类成不同的组。

-- --

**K-均值**是一个迭代算法。假设我们想要将数据聚类成n个组，其方法为:

Step1. 首先选择 $K$ 个随机的点，称为**聚类中心**（**cluster centroids**）。

Step2. 遍历数据集中的**每一个数据**，按照距离 $K$ 个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。

Step3. 计算**每一个组**的平均值，将该组所关联的中心点移动到平均值的位置。

Step4. 重复步骤2-4直至中心点不再变化。

-- -

**K-均值**算法的伪代码如下：

```octave
随机初始化 K 个中心点 μ_1, μ_2, ..., μ_K (它们都是一个n维向量，n为特征数，这里通常不把偏置算进去，所以才是n维，而不是n+1维) % Step 1

Repeat {
	for i = 1 to m % Step 2
		c(i) := index (from 1 to K) of cluster centroid 			closest to x(i)

	for k = 1 to K % Step 3
		μ_k := average (mean) of points assigned to cluster k
}
```

## 13.2 优化目标

在大多数我们已经学习到的监督学习算法中，如线性回归、逻辑回归等，所有的这些算法都有一个目标函数，或者用于最小化的代价函数。K-均值也有一个优化目标函数，或者叫一个用于最小化的代价函数。

K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的代价函数（又称**失真代价函数** **Distortion cost function**）为：

$$J(c^{(1)},...,c^{(m)},μ_1,...,μ_K)=\dfrac {1}{m}\sum^{m}_{i=1}\left|| x^{\left( i\right) }-\mu_{c^{(i)}}\right|| ^{2}$$

其中 ${\mu }_{{c}^{(i)}}$ 代表与 ${{x}^{(i)}}$ 最近的聚类中心点。 我们的的优化目标便是找出使得代价函数最小的 $c^{(1)}$, $c^{(2)}$, ..., $c^{(m)}$ 和 $μ^1$, $μ^2$, ..., $μ^k$。

回顾刚才给出的**K-均值**迭代算法可知，第一个for循环是用于减小 $c^{(i)}$ 引起的代价，第二个for循环则是用于减小 ${{\mu }_{i}}$ 引起的代价。迭代的过程一定是使代价函数在减小，不然便是出现了错误。

## 13.3 随机初始化中心点

在运行K-均值算法之前，我们首先要随机初始化所有的聚类中心点。

1. 应使 $K<m$，即聚类中心点的个数要小于训练集样本数量
2. 随机选择$K$个训练实例作为$K$个聚类中心

**K-均值**的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。随机初始化的状态不同，K均值最后就可能得到不同的结果。

![image-20211005224335515](E:\md笔记\PyTorch\images\image-20211005224335515.png)

为了尽量避免取得局部最优，我们通常需要多次运行**K-均值**算法，每一次都重新进行随机初始化，最后再比较多次运行**K-均值**的结果，选择代价函数最小的结果。这种方法在 $K$ 较小的时候（2~10之间）还是可行的，但是如果$K$较大，这么做也可能不会有明显地改善。

## 13.4 选取聚类数量

没有能自动选取聚类数量（即参数 $K$）的方法。目前常用的方法仍然是通过观察可视化的图或观察聚类算法的输出后，人工选择出来。

当人们在讨论选择聚类数目的方法时，可能会谈及“肘部法则”。关于“肘部法则”，我们所需要做的是在不同的 $K$ 值下运行算法，然后画出一个 $K$ 与 代价函数之间的关系图像，这个图像期望呈现下图这样的趋势：

<img src="E:\md笔记\PyTorch\images\image-20211005231041034.png" alt="image-20211005231041034" style="zoom: 80%;" />

那么我们将选择转折点处对应的 $K$ 值。

然后，实际情况可能会是：图像平缓，不易于找出转折点。如下图所示：

<img src="E:\md笔记\PyTorch\images\image-20211005231155928.png" alt="image-20211005231155928" style="zoom:50%;" />

在这种情况下，就不容易确定 $K$ 值了。这里再提供另一种方法：如果后续目的（如市场分割）能给出一个评估标准，那么决定聚类的数量的更好方式是看哪个聚类数量能更好的应用于后续目的。

# 14 降维

> “降维”是又一种无监督学习问题。

## 14.1 降维动机：数据压缩

**数据压缩**——压缩数据可以使得占用更少的计算机内存或磁盘空间，另外也可以加快学习算法的运行。

假设某个数据集有许多特征，下面我们只绘制其中两个：

<img src="E:\md笔记\PyTorch\images\image-20211006124743165.png" alt="image-20211006124743165" style="zoom:80%;" />

假设这两个特征为：以厘米为单位的长度 $x_1$、以英寸为单位的长度 $x_2$。

这两个特征高度相关，是一种高度冗余的情况，所以我们想要减少特征到一维：投影原来的数据到一条直线上，如下图所示。

![image-20211006124914266](E:\md笔记\PyTorch\images\image-20211006124914266.png)

再看一个将数据从三维特征压缩到二维特征的例子【实际情况可能是将10000D压缩到100D】。压缩过程与上面类似，我们将三维向量投射到一个二维的平面上。

![image-20211006125321071](E:\md笔记\PyTorch\images\image-20211006125321071.png)

## 14.2 降维动机：可视化数据

在许多机器学习的应用中，数据可视化能很好地帮助我们对学习算法进行优化，让我们更好的了解我们的数据。此时，降维给我们提供了另一个有用的工具可以做到这些。让我们先看一个例子。

下面是有关不同国家的一些数据，这个数据集设置了50个特征。

![image-20211006131936467](E:\md笔记\PyTorch\images\image-20211006131936467.png)

此时，如何进行可视化？我们希望只用两个特征来概述这50个特征，从而可以将其表示在二维平面上，更好的观察数据。假设降维后，可视化效果如下：

![image-20211006131951597](E:\md笔记\PyTorch\images\image-20211006131951597.png)

但是对于新的两维特征，它们表示的含义可能不易明白。假设横坐标表示“国家的总体规模”或者“国家的总体经济活跃度”或“GDP”，纵轴表示“人均GDP”或“每个人的幸福程度”或“个人经济活跃度”，那么美国、新加坡所对应的位置可能如上图所示。【**降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。**】

## 14.3 主成分分析

**主成分分析(Principal Components Analysis, 简称PCA)**是最常见的降维算法。

### 14.3.1 实例

假设我们现在要将二维特征压缩到一维。**PCA**要做的是找到一个方向向量（**Vector direction**），然后把所有的数据都投射到该向量上，我们希望投射出来的（如下图蓝色线段）垂直长度的平方尽可能小【目标】。这些蓝色的线段有时也称**投影误差**。

![image-20211006134255901](E:\md笔记\PyTorch\images\image-20211006134255901.png)

> 在应用PCA之前，通常先进行均值归一化和特征规范化，使得特征 $x_1, x_2$ 的均值为0，并且其数值在可比较的范围之内。

再画一条能够进行数据投影的直线，如下图粉色线段。如果在这条线段上进行投影，那么会使得投影误差很大。

![image-20211006134740655](E:\md笔记\PyTorch\images\image-20211006134740655.png)

从而，主成成分分析的结果会选择上面那条红线，而不是这条粉色线段。

> 总结，PCA所进行的操作：如果想将数据从二维降到一维，我们要试着找一个向量，假设找出的是n维向量 $u^{(i)}$；我们要找一个数据投影后能够最小化投影误差的一个向量。

更一般的情况是，我们想将**n**维特征的数据降到**k**维，这样就不是找单个向量来对数据进行投影，而是寻找**k**个方向来对数据进行投影，达到最小化投影误差的效果。例如，有下图所示的三维点云，若想将其降到二维，就要找到两个向量 $u^{(1)}, u^{(2)}$，这两个向量一起定义了一个二维平面，然后将原来的三维数据都投影到这个平面上。

![image-20211006141031258](E:\md笔记\PyTorch\images\image-20211006141031258.png)

### 14.3.2 PCA和线性回归之间的关系

相似：

- 线性回归是找到一条拟合的线段；PCA也需要找出一条向量，这个向量满足投影误差尽可能小。

区别：

- 主成分分析最小化的是投射误差（**Projected Error**），而线性回归尝试的是最小化预测误差。体现在图中，它们需要最小化的距离分别可表示成下图蓝色线段所示。前者线性回归最小化的是高度差，后者PCA最小化的是映射距离。

![image-20211006142641967](E:\md笔记\PyTorch\images\image-20211006142641967.png)

- 线性回归的目的是预测结果，而主成分分析不作任何预测。

### 14.3.3 主成分分析算法

总的来说，**PCA试图找到一个低维的平面来对数据进行投影，目标是最小化投影误差的平方**。

下面使用主成分分析算法对数据进行降维。

在使用PCA之前，首先要做的是数据预处理，如均值标准化、特征缩放。

- 对于均值标准化，首先计算每个特征的平均值：${\mu}_j = \frac{1}{m}\sum_{i=1}^mx_j^{(i)}$. 然后用每个特征的值 $x_j^{(i)}$ 减去这个特征对应的均值 $\mu_j$。这将使每个特征的均值正好为0。

- 如果不同的特征有非常不同的缩放(eg. 房子的尺寸 $x_1$、卧室数量 $x_2$)，那么我们就把它们缩放到一个相同的数值范围。

当我们进行特征缩放时，要想办法计算：用来进行投影的向量 $\mu$ （可能不止一个）以及用来表示新特征的向量 $z$ 。

![image-20211006151118549](E:\md笔记\PyTorch\images\image-20211006151118549.png)

-- --

使用**PCA**找出正确的 $\mu, z$ 值：

<font color="red">Step1.</font> 计算协方差（**covariance matrix**）： $\Sigma=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$

> 这里 $\Sigma$ 是大写字母Sigma，不是求和符号。$\Sigma$ 常用来表示一个矩阵。$x^{(i)}$ 为 $n×1$ 向量，$(x^{(i)})^T$ 是 $1×n$ 向量，所以 $\Sigma$ 是 $n×n$ 矩阵。

<font color="red">Step2.</font> 计算协方差矩阵 $\Sigma$ 的特征向量。对于Octave，应使用这个语句：`[U,S,V] = svd(Sigma)`。`svd`表示奇异值分解，它返回三个矩阵 $U,S,V$，我们真正需要的是矩阵 $U$：

![image-20211006153945818](E:\md笔记\PyTorch\images\image-20211006153945818.png)

如果我们想减少数据的维数从 **n** 维到 **k** 维，我们需要做的是提取这个 $U$ 矩阵的前 **k** 个向量作为我们的方向向量，并将它们组合成一个名叫 $U_{reduce}$ 的矩阵：

<img src="E:\md笔记\PyTorch\images\image-20211006154958114.png" alt="image-20211006154958114" style="zoom:67%;" />

<font color="red">Step3.</font> 计算 $z$ ，即将原始数据集中的所有数据 $x$ 表示到低维特征向量中。使用如下公式计算 $z$ ：$z={U_{reduce}^T}x$ .

![image-20211006155324619](E:\md笔记\PyTorch\images\image-20211006155324619.png)

## 14.4 主成分数量选择

> 我们要压缩到 **k** 维，问题在于如何确定这个 **k** 值。

投射误差：$\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2$

训练集的方差：$\dfrac {1}{m}\sum^{m}_{i=1}\left|| x^{\left( i\right) }\right|| ^{2}$

我们选择能使如下式子成立的最小k值：$\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\dfrac {1}{m}\sum^{m}_{i=1}\left|| x^{\left( i\right) }\right|| ^{2}} ≤ 0.01$。

如果我们希望这个比例小于0.01或1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。

> 表达这个式子的一种方式是：“99% of variance is retained.”。这里的0.01只是常用值，也可以用其他的数值。

我们可以先令 $k=1$，然后进行主成分分析，获得 $U_{reduce}$ 和 $z$，然后计算比例是否小于1%。如果不是的话再令 $k=2$，如此类推，直到找到可以使得比例小于1%的最小 $k$ 值（原因是各个特征之间通常情况存在某种相关性）。这个方法的效率并不高。

还有一些更好的方式来选择 $k$​，当我们在**Octave**中调用`svd()`时，我们获得三个参数：`[U, S, V] = svd(sigma)`。其中的 $S$ 是一个 $n×n$ 的矩阵，只有对角线上有值，而其它单元都是0：

![image-20211006164911572](E:\md笔记\PyTorch\images\image-20211006164911572.png)

我们可以用这个矩阵来计算平均均方误差与训练集方差的比例： $$\dfrac {\dfrac {1}{m}\sum^{m}_{i=1}\left| x^{\left( i\right) }-x^{\left( i\right) }_{approx}\right| ^{2}}{\dfrac {1}{m}\sum^{m}_{i=1}\left| x^{(i)}\right| ^{2}}=1-\dfrac {\Sigma^{k}_{i=1}S_{ii}}{\Sigma^{m}_{i=1}S_{ii}}\leq 0.01$$

也就是：$$\frac {\Sigma^{k}_{i=1}s_{ii}}{\Sigma^{n}_{i=1}s_{ii}}\geq0.99$$

这样就可以直接计算出 $k$ 了，而不用一次次去试。

## 14.5 压缩重现

压缩后的低维特征如何回到原来的高维？

如图，压缩后的一维特征表示在 $z$ 坐标上。给定一个点 $z^{(1)}$，我们怎么将它表示在原来的高维空间中？

![image-20211006171333071](E:\md笔记\PyTorch\images\image-20211006171333071.png)

已知 $z=U^{T}_{reduce}x$，变形得：$x_{appox}=U_{reduce}z$, 在平方投影误差不大的情况下，有 $x_{appox}\approx x$。最后结果如下图：

![image-20211006171428654](E:\md笔记\PyTorch\images\image-20211006171428654.png)

都投影在了绿线上，有些点投影到线上并不是准确的，但与原来是近似的。

## 14.6 应用PCA的建议

**【使用PCA算法来加速监督学习算法】**

假设你有一个监督学习问题，且这个问题有很多特征。例如针对一张 100×100 像素的图片进行某个计算机视觉的机器学习，即总共有10000 个特征。对于像这种的高维特征向量，运行学习算法时将变得非常慢。所以我们希望通过PCA来减少数据的维度。

你可以这样做：

1. 检查已经被标记的训练集，并抽取出 x 得到一个无标签的训练集；

2. 使用PCA算法压缩数据得到 z 。

3. 得到新的训练集；

4. 对新的训练集运行学习算法

![image-20211006175025928](E:\md笔记\PyTorch\images\image-20211006175025928.png)

> Note：PCA所做的是定义一个 $x→z$ 的映射，这个过程是对训练集进行操作。 

**【PCA不适合用来防止过拟合】**

PCA不适合用来防止过拟合，因为PCA只是近似地丢弃掉一些特征，它并不考虑任何与标签变量 $y$ 有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到标签变量 $y$，不会丢掉重要的数据。要使用正则化来防止过拟合。

**【PCA操作是非必需的】**

一个常见的错误是，默认地将主成分分析作为学习过程中的一部分：

![image-20211006180152075](E:\md笔记\PyTorch\images\image-20211006180152075.png)

建议先不使用PCA。只有在结果不理想的情况下，比如运行速度太慢、占用内存太大而需要压缩数据时，才考虑使用PCA。也就是说，只有你证明不使用PCA时效果不好的情况下，才应该去使用PCA。